---
title: e35da518e543d331abf0b57fa939d682d39f5a88.md (chunk 008)
description: Preserved review artifacts and rationale.
audience: [contributors]
domain: [quality]
tags: [review]
status: archive
---

# Code Review Feedback

<!-- chunk-progress:begin -->
```text
██████████████████████████████ 100.0% (30/30 addressed)
```
<!-- chunk-progress:end -->

| Date | Agent | SHA | Branch | PR |
|------|-------|-----|--------|----|
| 2025-09-16 | CodeRabbit | `e35da518e543d331abf0b57fa939d682d39f5a88` | [unify/chaos-main](https://github.com/flyingrobots/go-redis-work-queue/tree/unify/chaos-main "flyingrobots/go-redis-work-queue:unify/chaos-main") | [PR#3](https://github.com/flyingrobots/go-redis-work-queue/pull/3) |

## Instructions

Please carefully consider each of the following feedback items, collected from a GitHub code review.

Please act on each item by fixing the issue, or rejecting the feedback. Please update this document and fill out the information below each feedback item by replacing the text surrounded by curly braces.

### create_postmortem_tasks.py:117

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In create_postmortem_tasks.py around lines 114 to 117, the two os.makedirs calls
use exist_ok=True but lack error handling; wrap each directory creation in a
try/except that catches OSError, detect and handle permission/readonly errors
(e.g. errno.EACCES, errno.EROFS) and any other OS-level failures, and on error
either log a clear message with the path and errno/details and exit with a
non-zero status or re-raise a new exception with contextual information so the
script fails loudly rather than silently.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Added explicit error handling so directory creation fails fast with actionable context. |
>
> ## Lesson Learned
>
> Even simple scripts benefit from surfacing permission/readonly issues explicitly instead of relying on `exist_ok`.
>
> ## What did you do to address this feedback?
>
> Wrapped the `os.makedirs` calls in a helper that catches `OSError`, maps EACCES/EROFS to descriptive messages, and exits non-zero on failure.
>
> ## Regression Avoidance Strategy
>
> The helper raises a `RuntimeError` that propagates through the script and is printed to stderr before exiting.
>
> ## Notes
>
> None.

### demos/lipgloss-transformation.tape:278

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In demos/lipgloss-transformation.tape lines 1–278, the demo invokes a
nonexistent ./bin/queue-tui --enhanced; update the tape to run the actual
entrypoint instead (replace the "./bin/queue-tui --enhanced" line with either a
build+run sequence "go build -o bin/queue-tui ./cmd/tui && ./bin/queue-tui" or a
direct run "go run ./cmd/tui" and remove the unsupported "--enhanced" flag), or
alternatively implement and document a matching --enhanced CLI flag in
cmd/tui/main.go that toggles the enhanced view; pick one approach and make the
corresponding change so the demo invocation runs successfully.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Updated the VHS tape to invoke the actual TUI binary and removed the unsupported flag. |
>
> ## Lesson Learned
>
> Demos should mirror real entrypoints; placeholder flags quickly go stale.
>
> ## What did you do to address this feedback?
>
> Swapped the scripted command to check for `./bin/tui` and run it with the sample config, dropping the nonexistent `--enhanced` flag.
>
> ## Regression Avoidance Strategy
>
> Keeping the demo tied to the canonical binary ensures future flag changes show up in one place.
>
> ## Notes
>
> None.

### deploy/deploy/data/test.txt:1

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deploy/deploy/data/test.txt lines 1-1 the test data is incorrectly placed
under deploy/deploy; move the file into a Go-style testdata directory such as
testdata/producer/input.ndjson, update any source references, tests, and
CI/deploy manifests to point to the new path, and perform the move as a tracked
rename in git so history is preserved.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Moved the sample queue input under `testdata/producer/` so tests can consume it via `go test` conventions. |
>
> ## Lesson Learned
>
> Test fixtures belong under `testdata/` to keep tooling and go:embed consumers happy.
>
> ## What did you do to address this feedback?
>
> Relocated the legacy `deploy/data/test.txt` into `testdata/producer/input.ndjson` to follow Go’s testdata layout.
>
> ## Regression Avoidance Strategy
>
> Future fixtures can expand within the new directory without polluting deployment assets.
>
> ## Notes
>
> None.

### deploy/deploy/data/test.txt:1

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deploy/deploy/data/test.txt around lines 1 to 1, the file contains only the
useless line "test file for producer"; replace it with a deterministic NDJSON
fixture representing a real job payload (one JSON object per line) used by the
producer tests. Construct a minimal, valid payload including required fields
(e.g., id, type, payload data, timestamps, and any flags the consumer expects),
ensure values are deterministic (static IDs/timestamps), and save as NDJSON so
each test run consumes identical input for reproducible assertions.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Replaced the placeholder text with deterministic NDJSON representing real jobs. |
>
> ## Lesson Learned
>
> High-fidelity fixtures make producer tests meaningful and reproducible.
>
> ## What did you do to address this feedback?
>
> Authored two canonical queue jobs (email + report) with fixed IDs, priorities, and ISO-8601 timestamps inside the NDJSON fixture.
>
> ## Regression Avoidance Strategy
>
> Using static values ensures future assertions and snapshot tests see stable input.
>
> ## Notes
>
> None.

### deploy/grafana/dashboards/work-queue.json:37

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deploy/grafana/dashboards/work-queue.json lines 1-37, the dashboard currently
only defines five panels and lacks operational essentials: add a top-level
"time" range object (e.g., from/to defaults such as now-1h/now) so users can
change window; add a "templating" block with query variables for queue, worker,
and environment (using label_values on relevant metrics) so panels can be
filtered; add an "annotations" block to surface deployments and incidents
(Prometheus expressions such as changes(build_info[...])); convert key panels
(queue length, job failure rate, circuit breaker state) to include alerting
rules with sensible thresholds, evaluation window and for/conditions and add
panel threshold visualization and reduceOptions as needed; and add SLO-focused
panels (error budget burn rate, availability over time, SLO target lines) and an
availability/alerts panel tied to SLO thresholds. Ensure all added fields follow
Grafana dashboard JSON schema and reference the Prometheus datasource/metric
labels used elsewhere in this file.
```

> [!CAUTION]- **Rejected**
> | Confidence | Remarks |
> |------------|---------|
> | 6 | The requested dashboard overhaul reaches beyond current metrics coverage and would be speculative right now. |
>
> ## Rejection Rationale
>
> The Grafana JSON shipped today is a placeholder for design review; implementing templating, annotations, SLO panels, and alert definitions requires agreement on metric names, labels, and alert policies that we have not finalized. Shipping speculative queries would mislead operators.
>
> ## What you did instead
>
> Documented the gap and kept the existing barebones dashboard in place until the observability team lands the finalized metric schema.
>
> ## Tradeoffs considered
>
> A quick edit could add scaffolding, but without validated metrics it would fail at runtime; conversely, delaying avoids churn and misconfigured alerts.
>
> ## What would make you change your mind
>
> Once the SLO/error-budget metrics and alert thresholds are finalized, we can implement the richer dashboard with real queries and add regression tests for the JSON.
>
> ## Future Plans
>
> Track the observability roadmap (P1: metrics parity + alert definitions). When those land, replace the placeholder dashboard with the production-ready version requested here.

### deployments/admin-api/Dockerfile:18

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/admin-api/Dockerfile around lines 16 to 18, the go build
invocation doesn't strip debug symbols, set a version variable, or trim paths
for reproducible, smaller binaries; change the build step to accept a VERSION
build-arg (ARG VERSION), add -trimpath and -ldflags '-s -w -X
main.version=${VERSION}' to the go build command (keeping CGO_ENABLED=0
GOOS=linux), so the produced admin-api binary is stripped, has embedded version
metadata, and uses reproducible paths.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Docker build now trims paths, strips symbols, and honors a VERSION build arg. |
>
> ## Lesson Learned
>
> Embedding build metadata while producing reproducible binaries is worth the extra flags.
>
> ## What did you do to address this feedback?
>
> Added a `VERSION` build arg and built the admin API binary with `-trimpath` plus `-ldflags "-s -w -X main.version=${VERSION}"` in the builder stage.
>
> ## Regression Avoidance Strategy
>
> The Dockerfile defaults VERSION to `dev`, so CI can override it and still produce reproducible artefacts.
>
> ## Notes
>
> Requires the new `cmd/admin-api` entrypoint introduced alongside this change.

### deployments/admin-api/Dockerfile:23

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/admin-api/Dockerfile around lines 20 to 23, the image currently
uses unpinned alpine:latest and runs as root; change to a specific, pinned
Alpine version (e.g., alpine:3.18 or a project-approved tag) and create a
non-root user/group before switching to it: install packages as root, create a
dedicated user and group, create and set ownership of any workdir/home, drop
privileges with USER <username>, and ensure file permissions are set so the
container does not run as root at runtime.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Runtime image now pins Alpine, drops root, and avoids baking configs. |
>
> ## Lesson Learned
>
> Container images should be minimal, pinned, and defer env-specific config to runtime mounts.
>
> ## What did you do to address this feedback?
>
> Switched to `alpine:3.19`, created a dedicated `adminapi` user, removed the config COPY, and ensured runtime directories are created with correct ownership.
>
> ## Regression Avoidance Strategy
>
> Comments in the Dockerfile note that configs must be mounted, helping future edits honor the pattern.
>
> ## Notes
>
> None.

### deployments/admin-api/Dockerfile:31

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/admin-api/Dockerfile around lines 26 to 31, the Dockerfile
currently copies an environment-specific config into the image (COPY
--from=builder /app/configs/admin-api.yaml ./configs/); remove that COPY so
environment-specific configs are not baked into the image and instead rely on
runtime mounting (volume/ConfigMap/Secret). Update the Dockerfile to stop
copying configs, ensure the image expects configs at a runtime path (e.g.,
./configs/) and add a brief comment indicating configs must be mounted at
runtime via your deployment manifests.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 10 | Fixed the table of contents slug so it matches the section heading. |
>
> ## Lesson Learned
>
> Anchor typos break navigation; small doc fixes matter for usability.
>
> ## What did you do to address this feedback?
>
> Corrected the link to use `#go-no-go-decision-gates` so the TOC points to the intended section.
>
> ## Regression Avoidance Strategy
>
> Quick grep for `gono-go` verified no other instances remain.
>
> ## Notes
>
> None.

### deployments/docker/Dockerfile.admin-api:21

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/docker/Dockerfile.admin-api around lines 20-21, the Go build
command should strip debug info, embed a version, and make builds reproducible;
change the build to add -trimpath and -ldflags (e.g. -ldflags "-s -w -buildid=
-X main.version=${VERSION}") to bake in version and remove symbol tables, then
run strip on the resulting binary (or use 'go build' with -ldflags "-s -w" and
follow with 'strip admin-api') so the image is smaller and builds are
reproducible; ensure VERSION is provided via build-arg or environment.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Documented the baseline environment and introduced the new bench payload flag. |
>
> ## Lesson Learned
>
> Benchmarks need reproducible hardware + config context; payload size is part of that story.
>
> ## What did you do to address this feedback?
>
> Added a "Test Environment" section (hardware, Redis config, payload size) and updated the sample command to use the new `--bench-payload-size` flag alongside the existing tuning knobs.
>
> ## Regression Avoidance Strategy
>
> The doc now explicitly asks readers to record their own environment, reducing ambiguous comparisons.
>
> ## Notes
>
> None.

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Docker compose build now strips symbols, trims paths, and embeds the VERSION arg. |
>
> ## Lesson Learned
>
> Dev and prod builds should share the same reproducibility/hardening flags.
>
> ## What did you do to address this feedback?
>
> Added a VERSION build arg and compiled with `-trimpath` plus `-ldflags "-s -w -X main.version=${VERSION}"`, then ensured runtime directories are created with correct ownership for the non-root user.
>
> ## Regression Avoidance Strategy
>
> Builder outputs to `/app/bin/admin-api`, making the artifact path consistent across Dockerfiles.
>
> ## Notes
>
> None.

### deployments/docker/Dockerfile.admin-api:40

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/docker/Dockerfile.admin-api around lines 38-40, the COPY line
assumes /app/configs/admin-api.yaml exists in the builder stage which makes the
build fail when it’s absent; fix by guaranteeing the file always exists in the
builder stage (add a step in the builder stage to mkdir -p /app/configs and
create a default admin-api.yaml if missing, or copy a repository default config
into that path) so the final-stage COPY is deterministic and never errors.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Switched the documentation to the interface-based collector example. |
>
> ## Lesson Learned
>
> Highlighting the interface encourages implementers to build real collectors instead of relying on helper closures.
>
> ## What did you do to address this feedback?
>
> Removed the closure helper snippet, introduced the `MetricsCollector` interface, and referenced the concrete `QueueMetricsCollector` example.
>
> ## Regression Avoidance Strategy
>
> Future examples will build on the interface, keeping guidance aligned with the package contract.
>
> ## Notes
>
> None.

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Updated examples to alias the import as `ars` for readability. |
>
> ## Lesson Learned
>
> Consistent aliases make lengthy module paths easier to digest in docs.
>
> ## What did you do to address this feedback?
>
> Replaced uses of `anomalyradarslobudget` with the shorter `ars` alias across all code samples.
>
> ## Regression Avoidance Strategy
>
> Future snippets can reuse the alias without reintroducing long identifiers.
>
> ## Notes
>
> None.

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Added versioning/deprecation policy to the API doc and central index. |
>
> ## Lesson Learned
>
> Reliability APIs need explicit lifecycle guidance so consumers can plan migrations.
>
> ## What did you do to address this feedback?
>
> Introduced a "Versioning & Deprecation" section in the Anomaly Radar doc and created `docs/api/_index.md` with the shared policy.
>
> ## Regression Avoidance Strategy
>
> Centralising the policy in `_index.md` keeps future APIs aligned and discoverable.
>
> ## Notes
>
> None.

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Removed the config COPY; runtime images now expect mounts and create required dirs. |
>
> ## Lesson Learned
>
> Shipping environment-specific configs inside the image causes drift; mounts are safer.
>
> ## What did you do to address this feedback?
>
> Dropped the `COPY` instruction, created `/app/configs` during build, and clarified that configs must be provided at runtime.
>
> ## Regression Avoidance Strategy
>
> Runtime command keeps pointing at `/app/configs/admin-api.yaml`, so deployments just mount the desired file.
>
> ## Notes
>
> None.

### docs/03_milestones.md:13

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/03_milestones.md around lines 11-13 (and similarly at lines 46-51), the
table-of-contents uses the incorrect anchor `#gono-go-decision-gates`; update
those links to the correct slug `#go-no-go-decision-gates` (and search the file
for any other occurrences of `gono-go` to replace) so the ToC links point to the
actual section anchor.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Added explicit `BurnRateThresholds` type with documented fields. |
>
> ## Lesson Learned
>
> Burn rate inputs deserve first-class documentation so operators know the fast vs slow windows.
>
> ## What did you do to address this feedback?
>
> Inserted a code block defining `BurnRateThresholds` (fast/slow rate + window) immediately after the SLO config.
>
> ## Regression Avoidance Strategy
>
> The snippet sits near the SLO config, keeping the relationship obvious for future edits.
>
> ## Notes
>
> None.

### docs/12_performance_baseline.md:45

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/12_performance_baseline.md around lines 41 to 45, the benchmark example
lacks test environment details making the numbers meaningless; update the doc to
specify the hardware (CPU model, core count, RAM), Redis version and config used
(persistence, maxmemory, eviction, TCP settings), and payload characteristics
(size, serialization) and show a revised example command including a
--bench-payload-size flag (e.g., 1024) plus an accompanying short note listing
the exact hardware and Redis configuration used for the reported results.
```


> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Documented the benchmark environment and added the new payload-size flag example. |
>
> ## Lesson Learned
>
> Benchmarks aren’t meaningful without hardware + payload context.
>
> ## What did you do to address this feedback?
>
> Added a "Test Environment" section detailing CPU/RAM/Redis config and updated the sample command to include `--bench-payload-size=1024`.
>
> ## Regression Avoidance Strategy
>
> Future edits now have a prominent checklist of knobs to keep in sync (hardware + payload).
>
> ## Notes
>
> None.
>
> ## What did you do to address this feedback?
>
> Added follow-up sentences beneath the budget responses describing `budget_utilization` as a 0–1 fraction, `current_burn_rate` as budget/hour, and `time_to_exhaustion` as a duration string.
>
> ## Regression Avoidance Strategy
>
> Keeping the notes directly under the JSON ensures future edits will keep them in sync.
>
> ## Notes
>
> None.

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Documented units/ranges via inline comments and a table under `AnomalyThresholds`. |
>
> ## Lesson Learned
>
> Explicit units prevent misconfiguration, especially for latency and error rate thresholds.
>
> ## What did you do to address this feedback?
>
> Updated struct comments to mention units and added a table summarising each field’s units and valid ranges.
>
> ## Regression Avoidance Strategy
>
> The table lives alongside the struct definition, making it easy to keep in sync when fields change.
>
> ## Notes
>
> None.

### docs/api/anomaly-radar-slo-budget.md:15

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 1–15, add a "Versioning &
Deprecation" section that declares supported API versions (e.g., v1), the
compatibility guarantees (minor/patch compatibility, no silent breaking
changes), the breaking-change policy (how breaking changes are evaluated and
approved), the deprecation timeline (minimum 90 days notice before removal), the
changelog/release process (where changes are recorded and how releases are
communicated), and concise migration guidance for clients (examples of typical
migration steps and links to relevant types like SLOConfig, BurnRateThresholds,
AnomalyThresholds, Alert, MetricSnapshot); also add the same section to the
central API docs file (docs/api/_index.md or the repository’s central API docs
entry) so the policy is discoverable project-wide, and ensure any references to
routes (internal/anomaly-radar-slo-budget/handlers.go RegisterRoutes) and types
are linked or cross-referenced for implementer guidance.
```


### docs/api/anomaly-radar-slo-budget.md:38

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 36-38 (and also apply the
same change at 52-75 and 475-483), the Go import uses the long package path
"github.com/flyingrobots/go-redis-work-queue/internal/anomaly-radar-slo-budget";
update the examples to alias this import to a short, readable identifier (e.g.,
ars or slo) and update all references in the examples accordingly so the code is
concise and consistent across the noted ranges.
```


### docs/api/anomaly-radar-slo-budget.md:38

- [ ] Fixed
- [ ] Test Written
- [x] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 36 to 38, the docs instruct
users to import an internal package path which is not accessible to consumers;
update the documentation to point to the public exported package path (the
module's published import path for the anomaly-radar-slo-budget package), ensure
the package is exported (move/rename from internal if necessary or add a public
wrapper package), and replace the internal import line with the correct public
import path that consumers can use.
```

> [!CAUTION]- **Rejected**
> | Confidence | Remarks |
> |------------|---------|
> | 6 | The anomaly radar library remains internal-only until its API is stabilised. |
>
> ## Rejection Rationale
>
> The package isn’t ready for external consumption; exporting it now would lock us into an unstable surface area.
>
> ## What you did instead
>
> Documented usage with the existing internal path and noted the alias for clarity, keeping the scope limited to internal consumers.
>
> ## Tradeoffs considered
>
> Publishing a wrapper prematurely risks breaking public guarantees; keeping it internal preserves flexibility while we iterate.
>
> ## What would make you change your mind
>
> Once the team signs off on a public API contract (types, lifecycle, support expectations) we can promote the package and update docs.
>
> ## Future Plans
>
> Track the observability roadmap; when the API is hardened we’ll move it under `pkg/` and update the import instructions accordingly.

### docs/api/anomaly-radar-slo-budget.md:58

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 51 to 58, the docs present
two conflicting collector APIs (closures-based SimpleMetricsCollector and an
interface-based QueueMetricsCollector); remove the closures-based
SimpleMetricsCollector snippet and keep the interface-based approach, add an
explicit MetricCollector interface signature description immediately before the
QueueMetricsCollector example so readers see the expected methods and types;
repeat the same cleanup for the other occurrence around lines 446–470 by
deleting the closure example and ensuring the interface signature is documented
prior to the QueueMetricsCollector sample.
```


### docs/api/anomaly-radar-slo-budget.md:89

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 81 to 89, the SLOConfig
struct references BurnRateThresholds but that type is not defined; add a new
BurnRateThresholds type immediately after the SLOConfig block with four fields:
FastBurnRate (float64) and FastBurnWindow (time.Duration) for the fast alert
threshold and its evaluation window, and SlowBurnRate (float64) and
SlowBurnWindow (time.Duration) for the slow alert threshold and its evaluation
window, each with brief inline comments explaining units (budget/hour for rates,
time.Duration for windows).
```


### docs/api/anomaly-radar-slo-budget.md:102

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 94-102 (and also apply same
change at lines 193-199), the struct fields lack explicit units and valid
ranges; update the struct comments to include units and ranges (e.g.,
BacklogGrowthWarning/BacklogGrowthCritical: "items/second";
ErrorRateWarning/ErrorRateCritical: "0–1"; LatencyP95Warning/LatencyP95Critical:
"ms"), and add a concise explanatory table or short paragraph immediately
beneath the struct and its JSON examples that lists each field, its unit, and
valid range so readers and JSON consumers have clear expectations.
```


### docs/api/anomaly-radar-slo-budget.md:124

- [ ] Fixed
- [ ] Test Written
- [x] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around line 124, there is no
machine-readable API spec; create an OpenAPI 3.1 YAML file that models all
endpoints, request/response schemas, parameters, auth, and example payloads
described in this doc, add it to the repo (e.g., docs/api/openapi.yaml), update
this markdown to link to that file and include a brief note on versioning, and
add a CI job (using OpenAPI Generator CLI or similar) that validates the spec
and generates client SDKs (specify targets, output directory, and caching) on
merge so clients are produced automatically.
```

> [!CAUTION]- **Rejected**
> | Confidence | Remarks |
> |------------|---------|
> | 5 | Producing a full OpenAPI 3.1 spec + CI pipeline is out-of-scope for this review chunk. |
>
> ## Rejection Rationale
>
> We lack consensus on the final endpoint surface and don’t have automated validation wired up; generating an incomplete spec would be misleading.
>
> ## What you did instead
>
> Documented the endpoints comprehensively in markdown and continued referencing the existing admin API OpenAPI documents for guidance.
>
> ## Tradeoffs considered
>
> Building the spec now would take significant time and risk drift; deferring keeps focus on stabilising the API first.
>
> ## What would make you change your mind
>
> Once the anomaly radar API is stabilised and we can commit to long-term support, we can add the OpenAPI spec and CI job.
>
> ## Future Plans
>
> Track the “API formalisation” epic; when prioritised we’ll emit the spec and wire the generator pipeline requested here.

### docs/api/anomaly-radar-slo-budget.md:176

- [ ] Fixed
- [ ] Test Written
- [x] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 124-176 (and apply same
changes to ranges 179-218, 220-238, 240-270, 272-296, 298-327, 329-350,
352-369): the API endpoint docs lack an auth model, standard error schema, and
explicit Content-Type and status codes; add a short "Authentication &
Authorization" subsection stating the auth scheme (e.g., Bearer token) and
required RBAC roles for the endpoint, add a "Errors" subsection with the
standard JSON error shape (fields: error.code, error.message, error.details,
request_id) and list applicable HTTP error responses (401, 403, 422, 429 where
relevant) and finally specify response Content-Type (application/json) for
request and response examples so every endpoint doc includes auth, error schema,
relevant status codes, and Content-Type.
```

> [!CAUTION]- **Rejected**
> | Confidence | Remarks |
> |------------|---------|
> | 5 | Comprehensive auth/error rewrites require product + security alignment beyond this chunk. |
>
> ## Rejection Rationale
>
> The anomaly radar API is pre-release; documenting a finalised auth/error contract now would be speculative and risk diverging from the upcoming security review.
>
> ## What you did instead
>
> Focused on targeted clarifications (e.g., start/stop idempotency) and left the broader auth/error policy for the planned API hardening effort.
>
> ## Tradeoffs considered
>
> Partial coverage would produce inconsistent docs; deferring keeps the current pages truthful until the contract is nailed down.
>
> ## What would make you change your mind
>
> Once the security review publishes the standard error envelope and RBAC mapping we can update every endpoint accordingly.
>
> ## Future Plans
>
> Track the "AR SLO auth hardening" backlog item; when scheduled we will add the global auth/error sections and Content-Type annotations requested here.

### docs/api/anomaly-radar-slo-budget.md:176

- [ ] Fixed
- [ ] Test Written
- [x] Suggestion Ignored

```text
docs/api/anomaly-radar-slo-budget.md around lines 128 to 176: the response
example lacks an explicit timezone policy for timestamps; add a short sentence
directly beneath each response example block stating that all timestamps are
formatted as RFC3339 in UTC and include the trailing "Z" (e.g., "All timestamps
are RFC3339 in UTC and use the trailing 'Z'"), ensuring the note appears under
every response example in the file.
```

> [!CAUTION]- **Rejected**
> | Confidence | Remarks |
> |------------|---------|
> | 6 | Added centralized guidance instead of repeating the same note under every block. |
>
> ## Rejection Rationale
>
> Replicating identical text under each response makes the doc verbose and harder to maintain.
>
> ## What you did instead
>
> Added prominent UTC/RFC3339 callouts under the primary response sections so readers see the policy without duplication.
>
> ## Tradeoffs considered
>
> Per-response notes maximise visibility; a single callout is leaner but risks being overlooked.
>
> ## What would make you change your mind
>
> If feedback shows readers still miss the timestamp policy we can adopt reusable callouts for each block.
>
> ## Future Plans
>
> Consider a documentation shortcode for timestamp conventions during the next docs tooling pass.

### docs/api/anomaly-radar-slo-budget.md:135

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 132-135 (and also apply
same change at 247-249 and 335-337), the "duration" type for query parameters is
not defined; update the docs to state that durations use Go's time.ParseDuration
format and give a short example (e.g., "duration (Go time.ParseDuration format,
e.g., 30m, 1h, 24h, 7h30m)"). Insert this one-line clarification immediately
after each query-parameter list mentioned so callers know the expected format.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Added explicit notes under each duration-based parameter list referencing `time.ParseDuration`. |
>
> ## Lesson Learned
>
> Duration formatting trips people up; pointing to the Go syntax prevents guesswork.
>
> ## What did you do to address this feedback?
>
> Added clarifying sentences beneath the relevant query parameter lists (status, metrics, budget history) that call out `time.ParseDuration` examples.
>
> ## Regression Avoidance Strategy
>
> Keeping the note adjacent to each parameter list makes it hard to miss when fields change.
>
> ## Notes
>
> None.

### docs/api/anomaly-radar-slo-budget.md:238

- [ ] Fixed
- [ ] Test Written
- [x] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 220 to 238, the PUT
/api/v1/anomaly-radar/config section lacks semantics for validation, unknown
fields, partial updates and immutability; update the doc to state whether PUT
performs a full replace or a deep PATCH-like merge (choose one and describe
behavior for nested objects), enumerate validation rules (e.g.,
availability_target must be between 0 and 1 inclusive, latency_threshold_ms
positive integer, threshold rates between 0 and 1, types), state that invalid
input returns HTTP 422 with a JSON body listing field errors, explain how
unknown fields are handled (reject with 400 or ignore with warning), and call
out any fields that are immutable at runtime and require a restart to change.
```

> [!CAUTION]- **Rejected**
> | Confidence | Remarks |
> |------------|---------|
> | 5 | PUT semantics are still under design review; documenting specifics now would be speculative. |
>
> ## Rejection Rationale
>
> We haven’t finalised whether the handler behaves as full replace vs merge, nor the validation contract—setting expectations prematurely could mislead integrators.
>
> ## What you did instead
>
> Left the section describing the high-level behaviour and noted validation will follow once the config API stabilises.
>
> ## Tradeoffs considered
>
> Documenting assumptions now increases risk of churn; waiting preserves accuracy until behaviour is locked down.
>
> ## What would make you change your mind
>
> When the config API specification is final (validation rules, immutability, error codes), we can document the details outlined here.
>
> ## Future Plans
>
> Track the “Config PUT semantics” task in the observability backlog to revisit these docs after the implementation is frozen.

### docs/api/anomaly-radar-slo-budget.md:249

- [ ] Fixed
- [ ] Test Written
- [x] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 246-249, the query param
`max_samples` is underspecified and can lead to huge responses; set and document
a sensible default (e.g., default=1000) and a hard upper bound (e.g.,
max=10_000), add an optional pagination token query param (e.g., `next_cursor`)
and show the paginated response structure including `metrics`, `count`, and
`next_cursor` (opaque token) so callers know how to request subsequent pages;
also mention that `max_samples` cannot exceed the hard limit and that server
will return `count` and `next_cursor=null` when no more data exists.
```

> [!CAUTION]- **Rejected**
> | Confidence | Remarks |
> |------------|---------|
> | 5 | Pagination and hard limits aren’t implemented yet; documenting them would be misleading. |
>
> ## Rejection Rationale
>
> The metrics endpoint currently streams bounded historical data without cursors; adding pagination requires backend changes beyond this doc update.
>
> ## What you did instead
>
> Retained the existing description and noted in backlog to design a pagination scheme alongside the API changes.
>
> ## Tradeoffs considered
>
> Documenting hypothetical parameters now could cause client breakage when the real implementation lands.
>
> ## What would make you change your mind
>
> Once pagination support is implemented (cursor format, limits, defaults) we can document the exact contract.
>
> ## Future Plans
>
> Track the "metrics pagination" feature request in the reliability roadmap; documentation will follow the implementation.

### docs/api/anomaly-radar-slo-budget.md:266

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 250-266 (and similarly for
lines 339-350), the example metrics response omits p90 while the percentiles
endpoint includes it; update the historical metrics payload to include a
p90_latency_ms field for each metric entry (matching the same format and units
as p50/p95/p99) so both endpoints use the same percentile set, and ensure the
surrounding documentation text reflects that the metrics include
p50/p90/p95/p99.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Added `p90_latency_ms` to the metrics response and updated the descriptive text. |
>
> ## Lesson Learned
>
> Percentile docs should reflect the full set exposed by the API to avoid confusion.
>
> ## What did you do to address this feedback?
>
> Updated the JSON examples to include `p90_latency_ms` and expanded the bullet list to mention P90 alongside the other percentiles.
>
> ## Regression Avoidance Strategy
>
> All percentile mentions now share the same list, so future additions will require a single edit.
>
> ## Notes
>
> None.

### docs/api/anomaly-radar-slo-budget.md:327

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 299 to 327, the response
fields lack units/definitions; update the documentation by adding explicit unit
notes for budget_utilization, current_burn_rate, and time_to_exhaustion —
specify "budget_utilization: fraction [0,1]", "current_burn_rate: budget/hour
(fraction of total budget consumed per hour)", and "time_to_exhaustion: RFC3339
duration string" under the response example so readers know the domains and
units.
```


### docs/api/anomaly-radar-slo-budget.md:374

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 371 to 374, the health
endpoint HTTP status codes are too limited; add entries for 500, 429, and 206
with brief conditions: 500 Internal Server Error for unexpected/internal
failures, 429 Too Many Requests when collectors or clients are being throttled,
and 206 Partial Content when the endpoint returns partial or degraded data
(include a short parenthetical or one-line condition for each). Ensure
formatting matches the existing bullet list and keep descriptions concise.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Documented additional status codes (500/429/206) for the health endpoint. |
>
> ## Lesson Learned
>
> Operators need to know what degraded/ throttled states look like.
>
> ## What did you do to address this feedback?
>
> Extended the status code list with explanations for 500, 429, and 206 outcomes.
>
> ## Regression Avoidance Strategy
>
> Keeping the list in one place makes future additions straightforward.
>
> ## Notes
>
> None.

### docs/api/anomaly-radar-slo-budget.md:383

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 375 to 383, the Start/Stop
section lacks idempotency and authorization details; update the docs to
explicitly define semantics for repeated calls (POST /api/v1/anomaly-radar/start
returns 200 OK with a message "already started" if radar is running, otherwise
202 Accepted or 200 OK when started; POST /api/v1/anomaly-radar/stop returns 200
OK with "already stopped" if not running, otherwise 202/200 when stopping), list
required authentication/authorization (e.g., requires bearer token with role
"slo_admin" or permission "anomaly_radar:manage"), document possible status
codes and example request/response bodies for both success, idempotent-no-op,
and unauthorized (401/403) cases, and add note about concurrency handling
(server guarantees idempotent behavior and returns current state) so callers
know double start/stop are safe.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Documented auth requirements, idempotent responses, and concurrency guarantees for start/stop. |
>
> ## Lesson Learned
>
> Operational endpoints should spell out idempotency and required privileges.
>
> ## What did you do to address this feedback?
>
> Added bearer-token scope requirements, sample "already_started/ already_stopped" responses, and described status codes plus concurrency behaviour.
>
> ## Regression Avoidance Strategy
>
> The doc now acts as a checklist for behaviour; future changes must update these expectations.
>
> ## Notes
>
> None.

### docs/api/anomaly-radar-slo-budget.md:441

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 435 to 441, the "Batch
Operations: Use batch endpoints for efficient data retrieval" bullet references
endpoints that are not documented; remove this bullet or add a new "Batch
endpoints" section detailing the routes and request/response shapes. If
removing, delete bullet 4 and renumber/adjust wording to keep the list coherent;
if adding, create a new subsection immediately after the Performance list that
documents each batch route (path, method), expected request payload, response
schema, and example use-cases so the docs are accurate and not misleading.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Removed the stray batch-operations bullet until matching endpoints exist. |
>
> ## Lesson Learned
>
> Documentation should not promise features we haven’t built.
>
> ## What did you do to address this feedback?
>
> Deleted the bullet under the performance section so the list only reflects current capabilities.
>
> ## Regression Avoidance Strategy
>
> We’ll add a dedicated section once batch endpoints are implemented.
>
> ## Notes
>
> None.

### docs/api/anomaly-radar-slo-budget.md:503

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/anomaly-radar-slo-budget.md around lines 488 to 503, the Prometheus
exporter incorrectly calls Inc() during scrape which mutates metrics on each
scrape; instead compute counts and set gauges idempotently. Change the loop to
tally active alert counts by severity, call
alertCountVec.WithLabelValues(sev).Set(count) for each severity, and ensure any
previously-exposed severity labels not present are either Set(0) or removed
(e.g., DeleteLabelValues) so the exporter is fully idempotent.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Updated the exporter example to tally counts and use `Set` for idempotent scrapes. |
>
> ## Lesson Learned
>
> Metric examples should promote scrape-safe patterns.
>
> ## What did you do to address this feedback?
>
> Replaced the `Inc()` loop with a map accumulation plus explicit `Set` calls and zeroing for missing severities.
>
> ## Regression Avoidance Strategy
>
> The snippet now mirrors best practices; future updates can copy the pattern.
>
> ## Notes
>
> None.
