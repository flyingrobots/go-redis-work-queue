---
title: e35da518e543d331abf0b57fa939d682d39f5a88.md (chunk 007)
description: Preserved review artifacts and rationale.
audience: [contributors]
domain: [quality]
tags: [review]
status: archive
---

# Code Review Feedback

<!-- chunk-progress:begin -->
```text
███████████████████░░░░░░░░░░░ 63.3% (19/30 addressed)
```
<!-- chunk-progress:end -->

| Date | Agent | SHA | Branch | PR |
|------|-------|-----|--------|----|
| 2025-09-16 | CodeRabbit | `e35da518e543d331abf0b57fa939d682d39f5a88` | [unify/chaos-main](https://github.com/flyingrobots/go-redis-work-queue/tree/unify/chaos-main "flyingrobots/go-redis-work-queue:unify/chaos-main") | [PR#3](https://github.com/flyingrobots/go-redis-work-queue/pull/3) |

## Instructions

Please carefully consider each of the following feedback items, collected from a GitHub code review.

Please act on each item by fixing the issue, or rejecting the feedback. Please update this document and fill out the information below each feedback item by replacing the text surrounded by curly braces.

### docs/TUI/README.md:21

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/TUI/README.md around lines 15 to 21, the README shows verified images
but is missing a short "How to build/run TUI" snippet; add a brief section
titled "How to build/run TUI" that states the command to run the TUI (make
run-tui) and a one-line note that referenced SVGs are in docs/TUI/images and the
Makefile target is defined (Makefile:19–20), so users can build and run the
interface easily.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Added a concise "How to build/run TUI" section that points to the supported Makefile target. |
>
> ## Lesson Learned
>
> Design docs benefit from including the one command that gets contributors into the experience quickly.
>
> ## What did you do to address this feedback?
>
> Documented the `make run-tui` target, explained what it builds, and pointed readers to the SVG assets directory for mockups.
>
> ## Regression Avoidance Strategy
>
> I cross-checked the instructions with the Makefile target so future edits stay aligned with the canonical entrypoint.
>
> ## Notes
>
> None.

### Makefile:13

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In Makefile around line 13, remove the unnecessary GO111MODULE=on prefix from
the go build command and ensure the linker variable used in LDFLAGS exists:
either add a package-level variable in package main (e.g. in cmd/$(APP)/main.go
declare var version string) so -X main.version=$(VERSION) will succeed, or
change the -X flag to point to the actual package and symbol that holds the
version; then run the build to confirm the "Missing main.version symbol for -X"
error is resolved.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Dropped the unused GO111MODULE env and confirmed the linker flag resolves against the existing `main.version` symbol. |
>
> ## Lesson Learned
>
> Keeping Makefile defaults aligned with the current Go toolchain avoids redundant env overrides and surprise warnings.
>
> ## What did you do to address this feedback?
>
> Removed the legacy `GO111MODULE=on` prefix from the primary build target and verified that the `cmd/job-queue-system` main package already exposes the `version` variable required by `-X main.version`.
>
> ## Regression Avoidance Strategy
>
> Build scripts rely on the module defaults now, so future Go upgrades won't inherit stale environment flags.
>
> ## Notes
>
> None.

### Makefile:18

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In Makefile around lines 16 to 18, the TUI build target uses a different go
build invocation than the other build targets; make it consistent by using the
same variables and flags as the other builds: ensure the target creates the bin
directory if needed and invoke go build with the shared GOFLAGS and LDFLAGS
(e.g. use GO111MODULE=on go build $(GOFLAGS) -ldflags "$(LDFLAGS)" -o bin/tui
./cmd/tui) so the TUI is built with the same environment and linker flags as the
rest of the project.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Unified the TUI build recipe with the primary binary and added the missing version symbol so LDFLAGS apply cleanly. |
>
> ## Lesson Learned
>
> Duplicated build recipes drift quickly; centralizing on shared variables keeps the binaries in lockstep.
>
> ## What did you do to address this feedback?
>
> Introduced reusable `BIN_DIR`/`GOFLAGS` helpers, ensured the target creates `bin/`, mirrored the linker flags, and added a `version` variable to `cmd/tui/main.go` so the `-X main.version` injection succeeds.
>
> ## Regression Avoidance Strategy
>
> Both binaries now rely on the same Makefile plumbing, so adding flags or changing linker settings will propagate automatically.
>
> ## Notes
>
> None.

### .githooks/pre-commit:15

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In .githooks/pre-commit around lines 7 to 15, the hook assumes python3 exists
and will fail if it's absent; update it to detect an available interpreter at
runtime by checking for "python3" first, then "python" (using quoted paths), and
if neither exists either exit with a clear error message or skip the progress
update gracefully; use the detected interpreter variable when invoking
scripts/update_progress.py and ensure git add only runs when the update
succeeded.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Hook now discovers python3/python dynamically and exits early if neither exists. |
>
> ## Lesson Learned
>
> Pre-commit tooling should degrade with clear messaging when platform dependencies are missing.
>
> ## What did you do to address this feedback?
>
> Added interpreter discovery logic that prefers `python3`, falls back to `python`, and surfaces an actionable error when both are absent before running the progress script.
>
> ## Regression Avoidance Strategy
>
> The hook keeps using `set -e`, so any failure in interpreter detection or script execution halts the commit safely.
>
> ## Notes
>
> None.

### .github/workflows/update-progress.yml:6

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In .github/workflows/update-progress.yml lines 1-6, YAMLLint flagged the
reserved key and bracket spacing: wrap the reserved key "on" in quotes (e.g.
"on":) and tighten the branch list brackets by removing inner spaces and quoting
the value (e.g. branches: ['main']) so the YAML parses cleanly.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Quoted the reserved `on` key and normalized the branches list to satisfy YAML linters. |
>
> ## Lesson Learned
>
> GitHub Actions still needs quoting for reserved words; keeping lint feedback in mind avoids CI churn.
>
> ## What did you do to address this feedback?
>
> Updated the workflow header to wrap `on` in quotes and tightened the push branches array to `['main']` as suggested.
>
> ## Regression Avoidance Strategy
>
> YAMLlint now passes on this section, so future formatting checks won't flag the reserved key.
>
> ## Notes
>
> None.

### .github/workflows/update-progress.yml:10

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In .github/workflows/update-progress.yml around lines 8 to 10, the workflow
lacks concurrency control which allows overlapping runs on rapid pushes; add a
top-level concurrency block (e.g., concurrency: { group: 'update-progress-${{
github.ref }}', cancel-in-progress: true }) to serialize runs per branch/ref and
cancel any in-progress run when a new one starts; place this block at the same
indentation level as permissions and jobs.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Added a concurrency block so only one update-progress run executes per ref. |
>
> ## Lesson Learned
>
> Serializing automation jobs prevents the GitHub Actions queue from thrashing on fast pushes.
>
> ## What did you do to address this feedback?
>
> Declared a top-level concurrency group keyed by `github.ref` and enabled `cancel-in-progress` to ensure newer pushes replace in-flight runs.
>
> ## Regression Avoidance Strategy
>
> Future tweaks to this workflow inherit the concurrency gate because it sits alongside `permissions` and `jobs`.
>
> ## Notes
>
> None.

### .github/workflows/update-progress.yml:41

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
.github/workflows/update-progress.yml lines 31-41: the workflow uses unguarded
git add and unquoted $GITHUB_OUTPUT redirections; update the script to first
check for the presence of the files (or use a safe add that won’t fail) before
running git add, and ensure all instances of >> $GITHUB_OUTPUT are changed to >>
"$GITHUB_OUTPUT" (quote the variable) so the redirection target is not
word-split; keep behavior the same otherwise (commit only when changes exist).
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Hardened the commit step to quote `$GITHUB_OUTPUT` and only add files that exist. |
>
> ## Lesson Learned
>
> Guarding automation scripts against missing files prevents noisy failures when outputs are unchanged.
>
> ## What did you do to address this feedback?
>
> Collected the files to add conditionally, skipped `git add` when none are present, and wrapped all output writes with quotes to avoid word splitting.
>
> ## Regression Avoidance Strategy
>
> The array approach keeps the add list explicit, so future additions can extend it without breaking empty runs.
>
> ## Notes
>
> None.

### .github/workflows/update-progress.yml:47

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
.github/workflows/update-progress.yml around lines 43-47 contains an extra
trailing blank line after the git push step; remove the empty line after the
final "git push" run block so the file ends immediately after the command (no
extra newline line), keeping YAML lint happy.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Simplified the push step to a single-line run so no stray blank line remains. |
>
> ## Lesson Learned
>
> Trimming cosmetic whitespace keeps workflow linters quiet and diff noise low.
>
> ## What did you do to address this feedback?
>
> Replaced the heredoc block with `run: git push`, ensuring the file ends immediately after the command.
>
> ## Regression Avoidance Strategy
>
> Using a single-line `run` leaves no trailing block to collect accidental whitespace.
>
> ## Notes
>
> None.

### cmd/job-queue-system/main.go:92

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In cmd/job-queue-system/main.go around lines 85 to 92, the code always starts
the observability HTTP server (metrics/healthz/readyz) even for role=admin; this
can cause unnecessary port conflicts for one-shot admin commands. Modify the
logic to only call obs.StartHTTPServer and defer its Shutdown when the
configured role is not "admin" (e.g., if cfg.Role != "admin" or a bool IsAdmin
check is false); otherwise skip starting the HTTP server entirely. Ensure
readyCheck remains defined for non-admin paths and that no server is started or
shutdown deferred when running as admin.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Admin executions now skip starting the HTTP metrics server to avoid port conflicts. |
>
> ## Lesson Learned
>
> CLI-style roles should avoid background services that can collide with other processes.
>
> ## What did you do to address this feedback?
>
> Wrapped the HTTP server startup in a `role != "admin"` guard so the metrics/health endpoints only run for long-lived roles.
>
> ## Regression Avoidance Strategy
>
> The guard ensures future admin subcommands inherit the same non-server behavior without additional checks.
>
> ## Notes
>
> None.

### cmd/job-queue-system/main.go:114

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In cmd/job-queue-system/main.go around lines 112–114, the background metrics
updater is started unconditionally for CLI/admin paths; guard the
obs.StartQueueLengthUpdater call so it is skipped when running the admin/CLI
mode. Change the code to check an existing config flag (e.g. cfg.Admin or
cfg.IsAdmin) or add a new flag like cfg.DisableBackgroundUpdaters/DisableMetrics
and only call obs.StartQueueLengthUpdater when that flag is false; ensure the
updater is not launched for admin CLI invocations.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Admin runs no longer spawn the background queue length updater. |
>
> ## Lesson Learned
>
> Background telemetry loops should respect CLI/one-shot modes to avoid dangling goroutines.
>
> ## What did you do to address this feedback?
>
> Wrapped the queue length updater in the same `role != "admin"` guard used for the HTTP server so admin commands finish cleanly.
>
> ## Regression Avoidance Strategy
>
> Tying both behaviors to the role check ensures future admin-only features inherit the non-daemon behavior.
>
> ## Notes
>
> None.

### cmd/job-queue-system/main.go:149

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In cmd/job-queue-system/main.go around lines 142 to 149, the admin handling is
nested inside the switch's default branch which obscures flow and risks
accidental execution when new roles are added; refactor by branching early or
adding an explicit case for "admin" instead of placing it in default: move the
admin check before the switch or add a dedicated case "admin" that calls
runAdmin(...) and returns, and keep the default branch solely to log/exit on
unknown roles so control flow is clear and future role additions won't
accidentally run admin logic.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Refactored the role switch to give admin its own case and leave default for true unknowns. |
>
> ## Lesson Learned
>
> Explicit switch branches prevent future roles from accidentally falling into admin execution.
>
> ## What did you do to address this feedback?
>
> Added a `case "admin"` branch that delegates to `runAdmin` and returns, leaving the default case to log an error for unsupported roles.
>
> ## Regression Avoidance Strategy
>
> Future role additions now require an explicit case, making the control flow self-documenting.
>
> ## Notes
>
> None.

### cmd/job-queue-system/main.go:188

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In cmd/job-queue-system/main.go around lines 187-188, the purge-all branch
prints a human-readable string with fmt.Printf which is inconsistent with other
admin commands that emit machine-readable JSON; replace the fmt.Printf call with
code that outputs a JSON object (e.g. {"purged": n}) to stdout using the
encoding/json package (or fmt.Fprintf with a properly escaped JSON string) and
ensure it ends with a newline and returns the same exit path as other admin
outputs.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Purge-all now emits JSON matching other admin outputs. |
>
> ## Lesson Learned
>
> Keeping admin command outputs machine-friendly simplifies scripting and automation.
>
> ## What did you do to address this feedback?
>
> Replaced the plain printf with a JSON payload (`{"purged": <count>}`) so the output format aligns with the other admin commands.
>
> ## Regression Avoidance Strategy
>
> Using `json.Marshal` gives us a consistent encoder; future fields can be added without manual string building.
>
> ## Notes
>
> None.

### deployments/kubernetes/rbac-monitoring.yaml:35

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/kubernetes/rbac-monitoring.yaml around line 35 (and also review
lines 78, 88, 99, 109, 120, 130, 140, 151, 161, 172, 182), the Prometheus alert
expressions reference non-existent rbac_* metrics and job="rbac-token-service";
either register the missing rbac_* metrics in the RBAC token service (create and
MustRegister prometheus metrics, instrument code, and expose them on /metrics)
or update the YAML alerts to reference actual metric names we already export
(e.g., jobs_*, backpressure_*, exactly_once_*) and/or ensure Prometheus scrape
configs include a target with job="rbac-token-service" so
up{job="rbac-token-service"} is valid; choose one approach and make consistent
changes across all listed lines.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Updated the RBAC monitoring stack to use existing http/app metrics and dropped unreachable token counters. |
>
> ## Lesson Learned
>
> Alerting should reference metrics we actually export; otherwise dashboards and rules silently fail.
>
> ## What did you do to address this feedback?
>
> Swapped `job=` selectors for the deployed `app=` label, removed alerts/panels that referenced nonexistent `rbac_*` metrics, and based security alerts on HTTP 401/429 response rates.
>
> ## Regression Avoidance Strategy
>
> Keeping the Grafana targets and Prometheus rules aligned with observable metrics prevents broken dashboards from slipping into CI.
>
> ## Notes
>
> Token-specific alerts can be reinstated once the service exports dedicated counters.

### Dockerfile:3

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In Dockerfile around line 3, the build stage uses golang:1.23 while CI/docs
require Go 1.25; update the builder image tag to golang:1.25 to align
toolchains. Edit the FROM line to use golang:1.25 (or the exact patch variant
your CI/doc specify) so the compile environment matches CI/docs, then rebuild
and run CI to verify no compatibility issues.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 10 | Bumped the builder image to Go 1.25 to match CI/tooling requirements. |
>
> ## Lesson Learned
>
> Keeping Docker builds on the same Go version as CI avoids subtle stdlib mismatches.
>
> ## What did you do to address this feedback?
>
> Updated the build stage to use `golang:1.25`, in line with the documented toolchain baseline.
>
> ## Regression Avoidance Strategy
>
> The image tag now matches the documented minimum; future upgrades will follow the same pattern.
>
> ## Notes
>
> None.

### docs/15_promotion_checklists.md:33

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/15_promotion_checklists.md around lines 21–33, the promotion checklist
table is missing OWNER and EXIT CRITERIA columns which prevents enforceable
gates; add two new columns "OWNER" and "EXIT CRITERIA" to the table header and
for each row populate a responsible owner (GitHub handle or team) and a clear,
testable exit criterion (e.g., "E2E pass 5× in CI", ">=80% coverage on admin &
obs", "alert rules merged + helm docs") and update any status cells accordingly
so CI can validate these criteria; also add a short note under the table
describing how CI will read these columns (OWNER format and exact phrasing
required for automated checks).
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Added Owner/Exit Criteria columns across promotion tables and documented CI expectations. |
>
> ## Lesson Learned
>
> Checklist tables need explicit ownership and measurable gates so automation can enforce them.
>
> ## What did you do to address this feedback?
>
> Extended each promotion table with `Owner` and `Exit Criteria` columns, populated GitHub-handle owners plus testable gates, and added a CI note clarifying the required formats.
>
> ## Regression Avoidance Strategy
>
> Future edits must keep owner handles prefixed with `@` and exit criteria machine-verifiable, otherwise CI will flag them.
>
> ## Notes
>
> None.

### docs/api/admin-api.md:44

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/admin-api.md around lines 41 to 44, the configuration shows a single
confirmation_phrase while the endpoints require different phrases (e.g.,
CONFIRM_DELETE_ALL) causing inconsistency; choose one approach and make docs and
code consistent: either document separate keys (e.g., dlq_confirmation_phrase
and purge_all_confirmation_phrase) and update the sample config and README to
list both keys, or change the endpoints to validate against the single
configured confirmation_phrase and update any endpoint docs/samples to reference
that single key; apply the chosen change across the docs and codebase so the
config keys and endpoint expectations match.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Split destructive confirmations into DLQ/purge-all phrases and documented the fallback. |
>
> ## Lesson Learned
>
> Matching docs and config structs prevents endpoint confusion around dangerous operations.
>
> ## What did you do to address this feedback?
>
> Added dedicated `dlq_confirmation_phrase` and `purge_all_confirmation_phrase` fields (with legacy fallbacks), updated handlers to consume them, and refreshed the config docs to describe the new keys and compatibility behavior.
>
> ## Regression Avoidance Strategy
>
> Helper methods centralize the fallback logic, so future handlers can reuse the same guardrails without duplicating string math.
>
> ## Notes
>
> Legacy configs using only `confirmation_phrase` continue to work; new deployments should switch to the explicit fields.

### docs/api/admin-api.md:132

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/admin-api.md around lines 106 to 132, the queue parameter
description is ambiguous about alias-to-Redis-key mappings; update the docs to
explicitly list accepted aliases (high, low, completed, dead_letter) and show
the exact Redis key each alias resolves to (or state that a full Redis key may
be provided), and reference the configuration fields that control those mappings
by name (worker.queues.* for priority queues, completed_list for completed jobs,
dead_letter_list for dead-letter queue). Mention accepted value formats (alias
or full Redis key) and provide a short example mapping table or inline examples
referencing the config keys so readers know where to change the mappings.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Clarified queue alias mapping with a table tied to the worker config fields. |
>
> ## Lesson Learned
>
> Documenting alias-to-key resolution saves operators from reading source to understand peek behavior.
>
> ## What did you do to address this feedback?
>
> Expanded the parameter docs with an alias table, highlighted the `worker.queues`, `worker.completed_list`, and `worker.dead_letter_list` config fields, and noted the full-key override.
>
> ## Regression Avoidance Strategy
>
> The table gives us a single place to update when queue defaults change, keeping docs and config in sync.
>
> ## Notes
>
> None.

### docs/api/admin-api.md:268

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/admin-api.md around lines 260 to 268, the CORS guidance currently
implies using cors_allow_origins: ["*"]; update the text to recommend an empty
list as the safe default and explicitly warn that using "*" is dangerous when
require_auth: true. Replace the current bullet with instructions to set
cors_allow_origins to an explicit, environment-specific list of allowed origins
(or leave empty to block cross-origin requests), add a short note discouraging
"*" for authenticated endpoints, and include a brief recommendation to use
specific subdomains or environment variables for allowed origins and to test
CORS in staging before production.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Updated CORS guidance to promote explicit allow-lists and warn against `"*"` with auth. |
>
> ## Lesson Learned
>
> CORS defaults should bias toward safety; wildcard origins plus auth is a red flag.
>
> ## What did you do to address this feedback?
>
> Switched the config example to an empty allow list, added deployment guidance about per-environment origin lists, and highlighted the danger of `"*"` when `require_auth` is true.
>
> ## Regression Avoidance Strategy
>
> The security best practices list now encodes this warning so future edits keep the guardrail.
>
> ## Notes
>
> None.

### docs/api/event-hooks.md:124

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
docs/api/event-hooks.md around lines 110 to 124: the HMAC signature currently
only covers the body which allows replay attacks because the listed
X-Webhook-Timestamp is not bound to the signature; update the docs to require
that the signature is computed over a canonical string that includes the
timestamp (e.g., timestamp + "." + body) and that receivers verify the timestamp
is within a configurable freshness window (e.g., ±N seconds) before accepting
the signature, and document that the server must reject deliveries with
missing/old timestamps or mismatched signatures.
```

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Documented the timestamp-bound HMAC string and freshness window. |
>
> ## Lesson Learned
>
> Signing the timestamp alongside the body is an easy replay defense that we should surface explicitly.
>
> ## What did you do to address this feedback?
>
> Clarified the signature header description, updated the verification example to hash `"timestamp.body"`, enforced a freshness check, and reminded clients to reject stale timestamps.
>
> ## Regression Avoidance Strategy
>
> The canonical string is now spelled out in both prose and code, making future doc edits less likely to omit the timestamp requirement.
>
> ## Notes
>
> None.

### docs/api/event-hooks.md:264

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/event-hooks.md around lines 246 to 264, the docs currently describe
rate limiting and retry policy but do not document idempotency or replay
semantics for DLH replays; add a short subsection stating that replays may
duplicate deliveries and receivers must treat deliveries as potentially
repeated, require an idempotency header (e.g., X-Webhook-Delivery) containing a
unique delivery ID, and optionally include a replay indicator (e.g.,
X-Webhook-Replay: true). Explain receiver behavior: persist the delivery ID with
a configurable TTL, deduplicate by returning a successful 2xx response for
already-processed IDs, use the idempotency key to make non-idempotent operations
safe (skip or noop on duplicate IDs), and document recommended TTL and retention
guidance for the idempotency cache.
```

{response}

### EVENT_HOOKS_TEST_DOCUMENTATION.md:44

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In EVENT_HOOKS_TEST_DOCUMENTATION.md around lines 41-44, the docs show an
incorrect go test command using a filename; replace it with package-aware
commands using -run filters: from repo root use "go test -v ./... -run
'^TestHMACSigner_'" or, when inside the package directory, "cd path/to/package
&& go test -v -run '^TestHMACSigner_'", and update the fenced bash block
accordingly so tests run reliably and with dependencies resolved.
```

{response}

### EVENT_HOOKS_TEST_DOCUMENTATION.md:65

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In EVENT_HOOKS_TEST_DOCUMENTATION.md around lines 61 to 65 the example test
command uses a filename which is incorrect when run from the repository root;
update the docs to use package paths or a patterned run flag instead: replace
the current `go test -v ./event_filter_test.go` example with a command that runs
tests by package or name, e.g. `go test -v ./... -run '^TestEventFilter_'`, or
alternatively instruct readers to change into the directory containing the test
and run `go test -v` — ensure the doc shows one clear correct command and
removes the filename-based invocation.
```

{response}

### EVENT_HOOKS_TEST_DOCUMENTATION.md:90

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In EVENT_HOOKS_TEST_DOCUMENTATION.md around lines 86-90, the docs currently
instruct running a single test file; update the example to run the integration
test by pattern instead of a filename. Replace the existing command with one
that runs the package tests using the -run flag to match the TestWebhookHarness_
tests (e.g., cd test/integration && go test -v -run '^TestWebhookHarness_'), and
ensure the fenced code block language remains bash.
```

{response}

### EVENT_HOOKS_TEST_DOCUMENTATION.md:116

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In EVENT_HOOKS_TEST_DOCUMENTATION.md around lines 112 to 116, the example
command and fenced block are incorrect; replace the existing bash block that
runs `go test -v ./nats_transport_test.go` with a bash fenced block that runs
`cd test/integration && go test -v -run '^TestNATSTransport_'` so the docs use
the correct go test invocation to run the NATS transport tests by name pattern
and ensure the code fence language is "bash".
```

{response}

### EVENT_HOOKS_TEST_DOCUMENTATION.md:139

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In EVENT_HOOKS_TEST_DOCUMENTATION.md around lines 136 to 139, the example test
command and fenced block are incorrect for running only DLH tests; replace the
existing bash fenced block (which runs the specific file) with a bash fenced
block that executes the Go test runner with the -run '^TestDLH_' pattern (i.e.,
change the command to: cd test/integration && go test -v -run '^TestDLH_') so
the documentation shows running only DLH tests via the -run flag.
```

{response}

### EVENT_HOOKS_TEST_DOCUMENTATION.md:186

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
`
In EVENT_HOOKS_TEST_DOCUMENTATION.md around lines 182 to 186, the security tests
section currently shows an incorrect go test command that passes a file path;
replace the snippet so it runs the specific test pattern instead (use go test -v
-run '^TestSignatureService_') and ensure the fenced bash block remains intact
and formatted as
```

{response}

### EVENT_HOOKS_TEST_DOCUMENTATION.md:224

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In EVENT_HOOKS_TEST_DOCUMENTATION.md around lines 216-224, the docs incorrectly
suggest using file globbing (go test ./*.go) which fails in multi-package repos
and misuses coverage tools; update the commands to run tests across all packages
(replace ./*.go with ./...), use go test -v ./... and go test -v
-coverprofile=coverage.out ./... and then run go tool cover -func=coverage.out
(or -html=coverage.out -o coverage.html) to generate coverage reports so the
instructions work correctly in multi-package projects.
```

{response}

### EVENT_HOOKS_TEST_DOCUMENTATION.md:235

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In EVENT_HOOKS_TEST_DOCUMENTATION.md around lines 228-235, the docs use a
working-directory change and file glob (cd test/integration + go test -v ./*.go)
which doesn't scope tests by package; update the examples to use explicit
package paths instead — replace that block with a single command using the
package path: "go test -v ./test/integration", and similarly update the
"Security Tests Only" example to use the package path (e.g. "go test -v
./test/security_test.go" or "go test -v ./test/integration -run Security"
depending on intended scope) so tests are run by package path rather than
relying on cd and file globs.
```

{response}

### EVENT_HOOKS_TEST_DOCUMENTATION.md:244

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In EVENT_HOOKS_TEST_DOCUMENTATION.md around lines 239 to 244, the benchmark
examples run tests unintentionally and target specific files; update the
commands to filter out tests with -run '^$' and run across packages with ./...
and use a proper benchmark regex for specific benchmarks (e.g.,
-bench='^BenchmarkName$') so benchmarks run only and across packages instead of
executing tests or limiting to ./*.go.
```

{response}

### create_postmortem_tasks.py:108

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In create_postmortem_tasks.py around lines 107 to 108, the dependencies array is
hard-coded with ten POSTMORTEM IDs which is brittle; replace the static list
with a dynamic generation that builds the dependency list from the source of
truth (e.g., the tasks/workers list or a count) so additions/removals stay in
sync — for example, derive task IDs from the tasks collection or generate using
a formatted range like "POSTMORTEM.{:03d}".format(i) and assign that resulting
list to the "dependencies" key.
```

{response}
