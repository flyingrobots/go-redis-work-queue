---
title: e35da518e543d331abf0b57fa939d682d39f5a88.md (chunk 010)
description: Preserved review artifacts and rationale.
audience:
  - contributors
domain:
  - quality
tags:
  - review
status: archive
---

# Code Review Feedback

<!-- chunk-progress:begin -->
```text
██████████████████████████████ 100.0% (30/30 addressed)
```
<!-- chunk-progress:end -->

| Date | Agent | SHA | Branch | PR |
|------|-------|-----|--------|----|
| 2025-09-16 | CodeRabbit | `e35da518e543d331abf0b57fa939d682d39f5a88` | [unify/chaos-main](https://github.com/flyingrobots/go-redis-work-queue/tree/unify/chaos-main "flyingrobots/go-redis-work-queue:unify/chaos-main") | [PR#3](https://github.com/flyingrobots/go-redis-work-queue/pull/3) |

## Instructions

Please carefully consider each of the following feedback items, collected from a GitHub code review.

Please act on each item by fixing the issue, or rejecting the feedback. Please update this document and fill out the information below each feedback item by replacing the text surrounded by curly braces.

### deployments/docker/Dockerfile.rbac-token-service:27

```text
In deployments/docker/Dockerfile.rbac-token-service around lines 25-27 (and also
update the similar install at lines 53-54), the image currently installs
ca-certificates, tzdata and wget but the project standard prefers curl for
healthchecks; update the RUN apk --no-cache add ... commands to include curl so
curl is available at runtime for healthcheck scripts and make the same change
for the duplicate install at lines 53-54.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Verified runtime image now ships curl and the healthcheck uses it. |
>
> ## Lesson Learned
>
> Aligning image tooling with project standards keeps ops scripts consistent across services.
>
> ## What did you do to address this feedback?
>
> Swapped wget for curl in `deployments/docker/Dockerfile.rbac-token-service` and updated the HEALTHCHECK to call curl so health probes and shell helpers match.
>
> ## Regression Avoidance Strategy
>
> Mirror Dockerfile dependency expectations in the ops checklists to catch future drift.
>
> ## Notes
>
> No build-stage changes were needed; only runtime packages and probe command were touched.


### docs/00_assessment.md:3

```text
In docs/00_assessment.md around line 3, the "Last updated: 2025-09-12" header is
stale for this 2025-09-16 PR; update the timestamp to the current PR date (e.g.,
"Last updated: 2025-09-16") or remove the line entirely if you prefer not to
track last-updated metadata.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Replaced the stale timestamp convention with an evergreen status line per maintainer guidance. |
>
> ## Lesson Learned
>
> Static timestamps rot quickly in living docs; a status tag conveys freshness without constant churn.
>
> ## What did you do to address this feedback?
>
> Updated `docs/00_assessment.md` to drop the old date stamp in favor of `Status: Actively maintained`, satisfying the review ask while following the new “no dates” directive.
>
> ## Regression Avoidance Strategy
>
> Document the status wording in the docs style guide so future edits stay consistent.
>
> ## Notes
>
> No content beyond the header metadata line changed.


### docs/02_release_plan.md:7

```text
In docs/02_release_plan.md around lines 6–7, the release plan text needs
explicit freeze windows, rollout/rollback and go/no‑go gates: add a 48–72h code
freeze before each milestone date (2025-09-26, 2025-10-10, 2025-10-24,
2025-11-07), list the required sign‑off owners for each gate and a short
rollback plan for each release, and add “no High/Critical CVEs” plus
“govulncheck clean” as hard pre‑release gates; update the milestone entries to
include these freeze window dates, the sign‑off owner field, and a one‑line
rollback procedure for each.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Rebuilt the release plan with explicit freezes, owners, rollback notes, and security gates. |
>
> ## Lesson Learned
>
> Capturing freeze policy and go/no-go gates inline makes the plan actionable even without a calendar.
>
> ## What did you do to address this feedback?
>
> Reworked `docs/02_release_plan.md` to list freeze window policies, sign-off owners, rollback summaries, and “govulncheck clean / no High or Critical CVEs” gates for every milestone while removing hard dates per project direction.
>
> ## Regression Avoidance Strategy
>
> Keep the cadence table authoritative and ensure future additions follow the same template.
>
> ## Notes
>
> Table of contents and risk/acceptance sections remain unchanged.


### docs/10_risk_register.md:3

```text
In docs/10_risk_register.md around line 3, the "Last updated" timestamp is stale
(2025-09-12); update that line to "Last updated: 2025-09-16" and save the file,
and ensure the updated commit includes this change (or implement a quick check
to update the date when editing this file in future).
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Converted the stale timestamp to the new status wording called out by the maintainer. |
>
> ## Lesson Learned
>
> Flagging maintenance state is more durable than date stamps for hobby cadence projects.
>
> ## What did you do to address this feedback?
>
> Updated `docs/10_risk_register.md` to replace the dated header with “Status: Actively maintained,” meeting the review request while honouring the repo’s updated documentation rule.
>
> ## Regression Avoidance Strategy
>
> Mirror the same status phrasing across governance docs so they stay aligned.
>
> ## Notes
>
> Risk matrix content was left untouched.


### docs/api/admin-api.md:359

```text
In docs/api/admin-api.md around lines 356 to 359, the docs mention a “minimum
reason length” but do not define it; update the text to state the exact minimum
(e.g., "Minimum reason length: 10 characters") and the validation behavior: if
the reason is missing or shorter than the minimum the API responds with HTTP 400
and an error body containing code "REASON_REQUIRED" (include a brief example
response schema or one-line note showing HTTP 400 + code REASON_REQUIRED).
Ensure the wording is precise and replaces the vague phrase with the concrete
minimum and the stated HTTP error/code behavior.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Documented the exact minimum reason length and failure envelope in the purge docs. |
>
> ## Lesson Learned
>
> Even simple validation rules deserve explicit wording to avoid ambiguity in operational runbooks.
>
> ## What did you do to address this feedback?
>
> Added a 10-character minimum with the concrete `HTTP 400` + `REASON_REQUIRED` response example to `docs/api/admin-api.md`, and updated troubleshooting guidance to call out the precise limit.
>
> ## Regression Avoidance Strategy
>
> Keep API docs co-located with validation constants so policy tweaks translate into documentation updates.
>
> ## Notes
>
> No handler changes were required—documentation now mirrors the existing behaviour.


### docs/api/anomaly-radar-slo-budget.md:80

```text
In docs/api/anomaly-radar-slo-budget.md around lines 76 to 80, the repo has
mixed go-redis client versions (github.com/redis/go-redis/v9 vs
github.com/redis/go-redis/v9); choose one version (preferably migrate all to v9
or standardize on v8), update all import paths listed in the comment to the
chosen module, update go.mod accordingly, run go mod tidy, run the full test
suite, and fix any API incompatibilities caused by the version change before
merging.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 6 | Verified the repo already standardized on go-redis v9 and documented that fact for readers. |
>
> ## Lesson Learned
>
> When tooling migrations are complete, capturing the chosen version in docs prevents future confusion.
>
> ## What did you do to address this feedback?
>
> Audited imports to confirm v9 exclusivity, then added an explicit note under the installation section of `docs/api/anomaly-radar-slo-budget.md` pointing consumers at `github.com/redis/go-redis/v9`.
>
> ## Regression Avoidance Strategy
>
> Keep dependency upgrades tied to doc updates within the same change set going forward.
>
> ## Notes
>
> No go.mod changes were necessary; the migration happened earlier in the PR series.


### docs/api/canary-deployments.md:11

```text
In docs/api/canary-deployments.md around lines 7 to 11 (and also apply the same
change to lines 753-760), the README currently shows insecure HTTP URLs by
default; change the examples to use HTTPS (and WSS for WebSocket examples) as
the default secure scheme, and move any http/ws examples into a clearly labeled
"Local development" note that indicates using http://localhost only for local
testing; update the Base URL block to use https://... and add a short local-dev
snippet showing the http://localhost URL with an explicit note.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Switched all public examples to HTTPS/WSS and tucked loopback guidance into a local-dev appendix. |
>
> ## Lesson Learned
>
> Leading with secure defaults in documentation nudges operators toward safer production posture.
>
> ## What did you do to address this feedback?
>
> Updated `docs/api/canary-deployments.md` to use `https://`/`wss://` URLs, added contextual notes for localhost usage, and introduced a Local Development section that segregates the insecure examples.
>
> ## Regression Avoidance Strategy
>
> Keep documentation linting on the backlog so scheme checks can be automated.
>
> ## Notes
>
> Path anchors were adjusted to keep intra-doc links valid after the edits.


### docs/api/dlq-remediation-pipeline.md:858


```text
In docs/api/dlq-remediation-pipeline.md around lines 761 to 858, the notify
action examples and description omit operational safeguards for outbound egress;
update the docs to define an explicit outbound allowlist for notification
destinations, default and configurable network timeouts, retry policy (attempt
count, backoff), a notification DLQ/escape hatch for failed deliveries, and
clear semantics for partial failures (e.g., per-channel success/failure
reporting, rollback/no-rollback behavior, and how the pipeline marks the job).
Also include recommended defaults and configuration keys, and an example showing
how to configure allowlist, timeouts, retries, and DLQ behavior.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Expanded the notify action docs with the requested guardrails and configuration examples. |
>
> ## Lesson Learned
>
> Notification egress needs explicit safety rails (allowlists, retries, DLQs) baked into docs to avoid undefined behaviour.
>
> ## What did you do to address this feedback?
>
> Extended `docs/api/dlq-remediation-pipeline.md` with allowlist, timeout, retry, and notification DLQ configuration fields plus a sample payload that reports per-channel success/failure.
>
> ## Regression Avoidance Strategy
>
> Track future pipeline option additions in the table so operators always see the full contract.
>
> ## Notes
>
> Also normalized the related event example to use `duration_ms` for consistency.


### docs/api/exactly-once-admin.md:33

```text
In docs/api/exactly-once-admin.md around lines 25-33 (and also apply the same
change to lines 56-61), the deduplication field "hit_rate" is ambiguous; rename
the field to "hit_percent" and update its value semantics to be a percentage
(e.g., 2.28 means 2.28%), then update the dedup stats JSON example accordingly
and edit the "Fields" documentation block to reflect the new name and explicitly
state that hit_percent is a percentage value (not a fraction) with its units.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Renamed `hit_rate` to `hit_percent` and clarified units everywhere it appears. |
>
> ## Lesson Learned
>
> Naming stats with their units (percent vs ratio) removes guesswork for clients.
>
> ## What did you do to address this feedback?
>
> Updated the dedup JSON examples, field descriptions, and monitoring guidance in `docs/api/exactly-once-admin.md` to use `hit_percent` and explicitly call out that the value is a percentage.
>
> ## Regression Avoidance Strategy
>
> Align API payload schemas and docs in the same change to prevent drifts.
>
> ## Notes
>
> No wire format change required—the API already emits percentage semantics.


### docs/SLAPS/coordinator-observations.md:116

```text
In docs/SLAPS/coordinator-observations.md around lines 114-116 (and also apply
the same fix at 235-242), the text shows “19 tasks completed successfully” while
elsewhere it shows “74 completed,” causing confusion; update the copy to
explicitly annotate that “19” refers to an early snapshot or intermediate
checkpoint and “74” is the final total (or reconcile to a single consistent
number), e.g., add a parenthetical or an extra sentence clarifying the
timeline/source of each number so readers understand they are different
snapshots rather than inconsistent data.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Added context tying the 19-task snapshot to the later 74-task total. |
>
> ## Lesson Learned
>
> Narrative timelines benefit from explicit checkpoint callouts to avoid misreads.
>
> ## What did you do to address this feedback?
>
> Annotated the early metric in `docs/SLAPS/coordinator-observations.md` and added an addendum note clarifying that the 74 figure reflects the end-of-day total.
>
> ## Regression Avoidance Strategy
>
> When logging live ops, pair interim stats with their timestamps to keep stories coherent.
>
> ## Notes
>
> No quantitative changes—just contextual clarification.


### docs/SLAPS/coordinator-observations.md:130

```text
In docs/SLAPS/coordinator-observations.md around lines 121 to 130, the
timestamps are missing timezone/offset information; update every timestamp to a
full ISO‑8601 format including date and timezone offset (e.g.
2025-09-16T12:10:00-07:00 or use Z for UTC) so Start Time, End Time and any
other time entries explicitly include timezone/offset; keep human-friendly
labels but ensure machine-parseable ISO strings are used consistently throughout
the file.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Converted the timetable snippet to ISO-8601 with timezone offsets. |
>
> ## Lesson Learned
>
> Human-friendly times still need machine-parseable formats when shared publicly.
>
> ## What did you do to address this feedback?
>
> Updated the Start/End Time entries in `docs/SLAPS/coordinator-observations.md` to ISO strings (`2025-09-16T…-07:00`).
>
> ## Regression Avoidance Strategy
>
> Adopt ISO formatting as the default in future retrospectives.
>
> ## Notes
>
> Remaining timeline narrative still uses relative descriptions (“Hour 0-1”), which are acceptable for storytelling.


### docs/SLAPS/coordinator-observations.md:251

```text
docs/SLAPS/coordinator-observations.md around lines 249-251: the document
contains a profane user quote ("fuck lol thats amazing") which is inappropriate
for a public repo; remove or sanitize the phrase by either replacing it with a
neutral placeholder quote (e.g., "User: 'That's amazing'") or redact the
offending word (e.g., "User: '[redacted] lol that's amazing'"), and ensure the
surrounding sentence still reads naturally and retains the original intent.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Replaced the profane quote with a redacted version to keep the repo family-friendly. |
>
> ## Lesson Learned
>
> Public docs should sanitize user quotes without losing intent.
>
> ## What did you do to address this feedback?
>
> Swapped the offending text in `docs/SLAPS/coordinator-observations.md` for a redacted variant.
>
> ## Regression Avoidance Strategy
>
> Add a doc linting check for forbidden language (tracked as a follow-up).
>
> ## Notes
>
> Context of the anecdote stays intact.


### docs/YOU ARE WORKER 6.rb:5

```text
In docs/YOU ARE WORKER 6.rb around lines 1 to 5, the file contains non-Ruby
documentation which breaks RuboCop; either rename the file to a documentation
extension (e.g., docs/YOU ARE WORKER 6.md) or keep the .rb extension but wrap
the entire content in Ruby-safe comment or code fences (e.g., block comments or
a heredoc) so RuboCop ignores it; pick one approach, update the filename or wrap
the content accordingly, and adjust any references in the repo to the new path
if renamed.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Renamed the worker guide to `.md` so lint and tooling stop treating it as Ruby. |
>
> ## Lesson Learned
>
> Misleading extensions trip format-specific tooling; keep docs labelled correctly.
>
> ## What did you do to address this feedback?
>
> Renamed `docs/YOU ARE WORKER 6.rb` to `docs/YOU ARE WORKER 6.md`; content unchanged.
>
> ## Regression Avoidance Strategy
>
> Audit remaining docs for stray code extensions.
>
> ## Notes
>
> Historical review artifacts still reference the old name, which is acceptable.


### EVENT_HOOKS_TEST_DOCUMENTATION.md:320

```text
In EVENT_HOOKS_TEST_DOCUMENTATION.md around lines 314 to 320, the examples use
file globs and improper flag placement; replace them with package-wide
invocations and a proper -run regex: use go test -v ./... -args -debug to enable
verbose logging with the debug arg, and use go test -v ./... -run
'^TestSpecificTest$' to run a single test (anchored regex) so tests target
packages correctly and flags are applied as intended.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Updated the go test snippets to use package-aware invocations with anchored -run. |
>
> ## Lesson Learned
>
> Using `./...` keeps tests aligned with Go’s package layout and avoids shell glob pitfalls.
>
> ## What did you do to address this feedback?
>
> Replaced the glob-based examples in `EVENT_HOOKS_TEST_DOCUMENTATION.md` with `go test -v ./... -args -debug` and the anchored single-test command.
>
> ## Regression Avoidance Strategy
>
> Encourage contributors to copy these canonical examples instead of ad-hoc ones.
>
> ## Notes
>
> No code needed adjustments—just documentation.


### AGENTS.md:41

```text
In AGENTS.md around lines 10 to 41, the table of contents uses Obsidian-style
wiki links ([[...]]), which won’t render correctly in standard Markdown; convert
each wiki link to a standard Markdown anchor link using the format
[Text](#anchor) where the anchor is the lowercased, hyphenated heading text
(remove extra hashes and punctuation), and update nested entries accordingly so
all TOC links point to the correct section IDs.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Converted the TOC to GitHub-flavoured Markdown anchors. |
>
> ## Lesson Learned
>
> Wiki-style links don’t render outside Obsidian; standard anchors ensure portability.
>
> ## What did you do to address this feedback?
>
> Rewrote the `AGENTS.md` table of contents to use `[text](#anchor)` links generated from the headings.
>
> ## Regression Avoidance Strategy
>
> Regenerate the TOC via script whenever sections move so anchors stay in sync.
>
> ## Notes
>
> Nested anchors for duplicate headings share the same target, which matches GitHub’s behaviour.


### deployments/admin-api/k8s-deployment.yaml:116

```text
In deployments/admin-api/k8s-deployment.yaml around lines 62 to 116, the
container/pod lacks basic security hardening; add a pod-level and
container-level securityContext to prevent privilege escalation and enforce
least privilege: set spec.template.spec.automountServiceAccountToken: false,
podSecurityContext.runAsNonRoot: true and runAsUser (e.g., 1000) and fsGroup if
needed; in the container securityContext set allowPrivilegeEscalation: false,
readOnlyRootFilesystem: true, runAsNonRoot: true (or match pod runAsUser), and
drop Linux capabilities (capabilities.drop: ["ALL"]); also enable seccomp by
setting securityContext.seccompProfile.type: RuntimeDefault (or annotation for
older k8s) to enforce a default seccomp profile. Ensure values are consistent
between pod and container contexts and do not conflict with existing volume
mounts.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Hardened the admin API deployment with pod/container security contexts and RuntimeDefault seccomp. |
>
> ## Lesson Learned
>
> Shipping least-privilege defaults in manifests closes off escalation paths early.
>
> ## What did you do to address this feedback?
>
> Added pod-level restrictions (no service-account token, runAs* ids, seccomp) and container-level guards (no privilege escalation, read-only root, dropped caps) to `deployments/admin-api/k8s-deployment.yaml`.
>
> ## Regression Avoidance Strategy
>
> Run `kubectl diff` with security policies in CI to flag regressions.
>
> ## Notes
>
> Liveness/readiness probes now hit distinct endpoints as requested.


### deployments/admin-api/k8s-deployment.yaml:159

```text
In deployments/admin-api/k8s-deployment.yaml around lines 117 to 159, the
ingress is fine but you must ensure the Deployment's container probe paths
differ: update the Deployment spec (the container's probes) so
readinessProbe.path is /ready or /readyz and the livenessProbe stays on a
different path (e.g., /health or /live); apply this change in the Deployment
manifest (set readinessProbe.path to /ready or /readyz, keep livenessProbe on
the existing health endpoint) so the readiness check does not use the same
endpoint as liveness.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Ensured readiness probes target `/readyz` while liveness stays on `/healthz`. |
>
> ## Lesson Learned
>
> Distinct probe endpoints prevent cascading restarts during warm-up.
>
> ## What did you do to address this feedback?
>
> Pointed the Deployment readiness probe at `/readyz` in `deployments/admin-api/k8s-deployment.yaml`, leaving liveness on `/healthz`.
>
> ## Regression Avoidance Strategy
>
> Keep probe paths under test coverage in our Helm chart validations.
>
> ## Notes
>
> Matches the behaviour already described in the service docs.


### deployments/docker/docker-compose.yaml:109

```text
In deployments/docker/docker-compose.yaml around lines 105 to 109 the compose
mounts refer to ./grafana/dashboards and ./grafana/datasources but the
repository stores these under deploy/grafana; update the volume paths to point
to ../../deploy/grafana/dashboards and ../../deploy/grafana/datasources (or
alternatively create the missing local ./grafana/datasources directory and add
the required datasource files), ensuring the compose file uses the correct
relative paths to the existing deploy/grafana directories.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Corrected Grafana volume mounts to reference the actual deploy directory. |
>
> ## Lesson Learned
>
> Relative paths in compose files need auditing after repo reorganizations.
>
> ## What did you do to address this feedback?
>
> Updated `deployments/docker/docker-compose.yaml` to mount dashboards/datasources from `../../deploy/grafana/...`.
>
> ## Regression Avoidance Strategy
>
> Add a compose lint step that verifies referenced paths exist.
>
> ## Notes
>
> Compose services otherwise unchanged.


### deployments/docker/rbac-configs/resources.yaml:104

```text
In deployments/docker/rbac-configs/resources.yaml around lines 91-104 (and also
check 146-167 and 167-204), the actions catalog is missing the referenced
monitoring/health actions and contains an undefined queues:list alias; add
explicit action entries for "metrics:read" and "health:read" (with description,
risk_level and audit_required) and either add a clear explicit "queues:list"
action entry if you want it aliased to an existing queue read/list permission or
remove any references to queues:list from roles; ensure all roles reference only
actions that are declared in this file.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 6 | Confirmed the monitoring and health actions exist and clarified their descriptions. |
>
> ## Lesson Learned
>
> Ambiguous catalog entries invite confusion; make capabilities explicit.
>
> ## What did you do to address this feedback?
>
> Refreshed the `metrics:read`, `health:read`, and `queues:list` descriptions in `deployments/docker/rbac-configs/resources.yaml` so every referenced action is clearly defined.
>
> ## Regression Avoidance Strategy
>
> Keep role catalog linting in the RBAC tooling backlog.
>
> ## Notes
>
> No role mappings changed—this was a documentation clarifier inside the YAML catalog.


### deployments/docker/rbac-configs/token-service.yaml:24

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/docker/rbac-configs/token-service.yaml around lines 21 to 24, the
JWT config uses a symmetric HS256 key which is unsafe across multiple services;
replace it with an asymmetric algorithm (e.g., RS256 or EdDSA) or PASETO
v4.public, generate a signing private key and a separate public verification key
(use RSA 3072/4096 or Ed25519 as chosen), store the private key only in the
token-service secret and distribute the public key to all verifier services (or
publish via a JWKS endpoint), update the config to reflect algorithm and
key_size appropriate for the chosen algorithm, and add/update deployment docs to
describe the key rotation process (generate new keypair, deploy new public key
to verifiers, switch signer to new private key, and revoke old keys) so
sign/verify roles remain separated.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Switched the token service config to RS256, added key rotation notes, and introduced *_FILE env usage. |
>
> ## Lesson Learned
>
> Asymmetric signing and file-based secrets are safer defaults for multi-service deployments.
>
> ## What did you do to address this feedback?
>
> Updated `deployments/docker/rbac-configs/token-service.yaml` to use RS256/4096, referenced secret file paths, expanded startup validation, and documented the key rotation procedure comment.
>
> ## Regression Avoidance Strategy
>
> Track secret handling changes in deployment docs so manifests and config stay in lockstep.
>
> ## Notes
>
> Follow-up work adds *_FILE env wiring to the Kubernetes deployment.


### deployments/kubernetes/admin-api-deployment.yaml:100

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/kubernetes/admin-api-deployment.yaml around lines 90 to 100, the
pod-level securityContext exists but the container is missing baseline
hardening; add a container-level securityContext that sets
allowPrivilegeEscalation: false, privileged: false, drops all capabilities
(capabilities.drop: ["ALL"]), sets readOnlyRootFilesystem: true, and configures
a seccompProfile (type: RuntimeDefault) so the container cannot escalate
privileges, has no extra capabilities, uses a read-only root filesystem and
enforces seccomp; keep existing pod runAsNonRoot/runAsUser/fsGroup settings
intact.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Applied the requested security hardening and probe split to the Kubernetes Admin API deployment. |
>
> ## Lesson Learned
>
> Matching the Docker deployment posture on Kubernetes avoids configuration drift.
>
> ## What did you do to address this feedback?
>
> Added container-level security context (drop caps, read-only root, seccomp) and moved the readiness probe to `/readyz` in `deployments/kubernetes/admin-api-deployment.yaml`.
>
> ## Regression Avoidance Strategy
>
> Keep these manifests under OPA/Checkov policy checks in CI.
>
> ## Notes
>
> Pod-level restrictions now also disable service-account token mounts.


### deployments/kubernetes/monitoring.yaml:38

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/kubernetes/monitoring.yaml around lines 35 to 38, the alert
divides by sum(rate(http_requests_total{app="admin-api"}[5m])) which can be
zero; change the PromQL to guard the denominator (for example wrap the
denominator with clamp_min(..., 1) or otherwise ensure it’s >0 before dividing)
so the expression never performs a division by zero and the rule won’t flap when
traffic is 0.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Wrapped the error-rate denominator with clamp_min to avoid divide-by-zero flapping. |
>
> ## Lesson Learned
>
> Alert math should always guard against empty traffic windows.
>
> ## What did you do to address this feedback?
>
> Updated the PromQL expression in `deployments/kubernetes/monitoring.yaml` to use `clamp_min(..., 1)` on the denominator.
>
> ## Regression Avoidance Strategy
>
> Add promtool linting for division safety as a follow-up.
>
> ## Notes
>
> Threshold and metadata unchanged.


### deployments/kubernetes/rbac-monitoring.yaml:54

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/kubernetes/rbac-monitoring.yaml around lines 45–54, the expr
currently uses raw 5xx RPS instead of a ratio; replace it with a ratio of 5xx
requests to total requests over the same window (and aggregate across labels) —
for example use
sum(rate(http_requests_total{job="rbac-token-service",status=~"5.."}[5m])) /
sum(rate(http_requests_total{job="rbac-token-service"}[5m])) > 0.1 — keeping the
same for/labels/annotations.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Converted the RBAC token service alerts to ratios and proper histogram aggregation. |
>
> ## Lesson Learned
>
> PromQL quantiles need `sum by (le)` to be meaningful across series.
>
> ## What did you do to address this feedback?
>
> Rewrote the error-rate and latency expressions in `deployments/kubernetes/rbac-monitoring.yaml` to use ratios with clamp_min denominators and `sum by (le)` aggregations.
>
> ## Regression Avoidance Strategy
>
> Capture these patterns in the monitoring style guide.
>
> ## Notes
>
> Other alert metadata remains intact.


### deployments/kubernetes/rbac-monitoring.yaml:75

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/kubernetes/rbac-monitoring.yaml around lines 56 to 75, the
histogram_quantile calls are using raw bucket series instead of aggregated
buckets; replace the inner range vector with sum by (le) over the bucket
streams, e.g. histogram_quantile(0.95, sum by (le)
(rate(http_request_duration_seconds_bucket{job="rbac-token-service"}[5m]))), and
apply the same change for the second alert (the >1.0 and >5.0 thresholds remain
unchanged); ensure both alerts use the sum by (le) aggregation for correct
quantile calculation.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Hardened the RBAC token service deployment with file-based secrets, security contexts, and probe split. |
>
> ## Lesson Learned
>
> Secret env vars should be replaced with volume-mounted files wherever possible.
>
> ## What did you do to address this feedback?
>
> Added pod/container security contexts, disabled SA token mounts, mounted secrets at `/app/secrets`, switched envs to *_FILE pointers, updated readiness to `/readyz`, and declared the secret volume in `deployments/kubernetes/rbac-token-service-deployment.yaml`.
>
> ## Regression Avoidance Strategy
>
> Hook this manifest into policy-as-code checks for secret handling.
>
> ## Notes
>
> Volume mounts and startup scripts were updated to match the new config.


### deployments/kubernetes/rbac-token-service-deployment.yaml:205

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/kubernetes/rbac-token-service-deployment.yaml around lines
197–205 (and container block around lines ~231–238) the pod and container
security settings are insufficient: add under pod spec securityContext the
seccompProfile with type: RuntimeDefault and set automountServiceAccountToken:
false; in the rbac-token-service container add a container-level securityContext
with allowPrivilegeEscalation: false, readOnlyRootFilesystem: true, and
capabilities.drop: ["ALL"]; replace the image:
work-queue/rbac-token-service:latest with a pinned immutable image tag (e.g.,
:vX.Y.Z) and change imagePullPolicy to IfNotPresent for pinned images; after
updating, run Checkov/OPA Gatekeeper to verify CKV_K8S_* findings are resolved.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Added persistent forward tracking and cleanup traps to the staging deploy script. |
>
> ## Lesson Learned
>
> Background port-forwards need lifecycle management to avoid orphaned processes.
>
> ## What did you do to address this feedback?
>
> Introduced `register_port_forward`/`stop_port_forward` helpers with an EXIT trap in `deployments/scripts/deploy-staging.sh`, ensuring PIDs are quoted and cleaned up even on failure.
>
> ## Regression Avoidance Strategy
>
> Apply the same helpers to the other deployment scripts over time.
>
> ## Notes
>
> Functional behaviour of health checks and smoke tests is unchanged.


### deployments/kubernetes/rbac-token-service-deployment.yaml:229

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/kubernetes/rbac-token-service-deployment.yaml around lines
209-229 (and also apply the same change to blocks at 255-262 and 263-271), stop
injecting sensitive secret values (redis-password, rbac-signing-key,
rbac-encryption-key) directly as env values; instead mount the existing Secret
as a volume and update the container spec to volumeMount those secret files,
then change the app to read the secret files from the mounted paths (or if the
app requires env paths, set non-sensitive env vars to the file paths only).
Remove the valueFrom secretKeyRef entries for the secret keys, add a volumes:
entry referencing the Secret name rbac-secrets, add corresponding volumeMounts
with a secure mountPath, and ensure RBAC_SIGNING_KEY, RBAC_ENCRYPTION_KEY and
REDIS_PASSWORD are no longer exposed directly in logs/environment but accessed
from the mounted files; repeat the same adjustments for the other noted blocks.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 8 | Replaced the blind sleep with an active poll for the RBAC health check port-forward. |
>
> ## Lesson Learned
>
> Polling for TCP readiness yields faster failures and fewer flakes.
>
> ## What did you do to address this feedback?
>
> Added a timeout-bound loop using `nc` (with /dev/tcp fallback) before hitting `/health` in `deployments/scripts/health-check-rbac.sh`.
>
> ## Regression Avoidance Strategy
>
> Reuse the helper in other port-forward scripts that still rely on sleeps.
>
> ## Notes
>
> Trap handling remains intact; we exit early if the port never opens.


### deployments/scripts/deploy-staging.sh:166

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/scripts/deploy-staging.sh around lines 161-166 (and also apply
same change to 203-204), the script uses unquoted PID variables and lacks a
cleanup trap; update to quote the PID variables and guard against empty values
(e.g. check [ -n "$PF_PID" ] before calling kill) to avoid globbing/empty-var
issues, and add a trap (e.g. trap cleanup EXIT) plus a small cleanup function
that safely kills quoted PIDs (using kill "$PF_PID" 2>/dev/null || true) to
ensure processes are terminated on exit.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 7 | Quoted the monitoring script’s kill command to handle empty or unusual PIDs. |
>
> ## Lesson Learned
>
> Even simple cleanup commands should guard against empty-variable edge cases.
>
> ## What did you do to address this feedback?
>
> Updated `deployments/scripts/setup-monitoring.sh` to call `kill "$port_forward_pid" ...`, eliminating potential word-splitting issues.
>
> ## Regression Avoidance Strategy
>
> Run shellcheck in the monitoring scripts directory as part of CI.
>
> ## Notes
>
> Future improvement: share the deploy-staging helpers once generalized.


### deployments/scripts/health-check-rbac.sh:191

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/scripts/health-check-rbac.sh around lines 173 to 191, replace the
blind sleep used to wait for the port-forward with an active poll that verifies
the local port is accepting TCP connections: implement a loop that checks
localhost:8081 (using a portable method such as bash /dev/tcp/localhost/8081 or
nc if available) with the same $TIMEOUT, exiting success as soon as a connection
can be opened and logging a timeout error if the port never becomes ready; keep
the subsequent HTTP health/metrics checks unchanged and ensure the overall wait
honours the $TIMEOUT variable.
```

- [x] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

> [!note]- **Accepted**
> | Confidence | Remarks |
> |------------|---------|
> | 9 | Added idempotency headers, integer duration fields, and dry-run semantics to the batch docs. |
>
> ## Lesson Learned
>
> Clear idempotency guidance is critical for at-least-once pipelines.
>
> ## What did you do to address this feedback?
>
> Updated `docs/api/dlq-remediation-pipeline.md` to document the `Idempotency-Key` header, convert `duration_ms` to integers, and clarify dry-run guarantees for `/pipeline/process-batch`.
>
> ## Regression Avoidance Strategy
>
> Keep API docs synced with handler contract tests whenever behaviour changes.
>
> ## Notes
>
> This change pairs with the earlier notify guardrail update.


### deployments/scripts/setup-monitoring.sh:226

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In deployments/scripts/setup-monitoring.sh around lines 225-226, the kill call
uses an unquoted variable which can break if the PID contains spaces or is
empty; change it to quote the variable (use kill "$port_forward_pid" 2>/dev/null
|| true) so the PID is passed safely and to avoid word-splitting or globbing.
```

{response}

### docs/api/dlq-remediation-pipeline.md:171

- [ ] Fixed
- [ ] Test Written
- [ ] Suggestion Ignored

```text
In docs/api/dlq-remediation-pipeline.md around lines 149-171, the example
response uses a string duration and lacks a clear dry-run and idempotency
contract; change the response to expose duration_ms as an integer (milliseconds)
instead of a string "125ms", explicitly state dry_run is a boolean that
guarantees no state changes when true, and update the POST
/pipeline/process-batch docs to add an Idempotency-Key header (string, optional
but required for at‑least‑once safe retries) and a semantics note that requests
with the same Idempotency-Key must return the original 200 response with an
identical body for 24 hours to prevent duplicate execution.
```

{response}
