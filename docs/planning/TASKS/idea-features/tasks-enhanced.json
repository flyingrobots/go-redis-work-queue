{
  "meta": {
    "min_confidence": 0.7,
    "codebase_analysis": {
      "existing_apis": [
        "redis_client",
        "tui_framework",
        "queue_system",
        "worker_system",
        "producer_system",
        "breaker_system",
        "config_system",
        "obs_system",
        "admin_system",
        "reaper_system"
      ],
      "reused_components": [
        "redis_client",
        "tui_framework",
        "queue_system",
        "worker_system",
        "producer_system",
        "breaker_system",
        "config_system",
        "obs_system",
        "admin_system",
        "reaper_system"
      ],
      "extension_points": [
        "queue.Job",
        "worker.Worker",
        "tui.Model"
      ],
      "shared_resources": {
        "redis_schema": {
          "type": "exclusive",
          "location": "Redis key space",
          "constraint": "sequential_only",
          "reason": "Schema modifications require exclusive access to prevent corruption"
        },
        "tui_main_loop": {
          "type": "exclusive",
          "location": "internal/tui/app.go",
          "constraint": "one_at_a_time",
          "reason": "TUI main loop can only be modified by one task at a time"
        },
        "config_files": {
          "type": "exclusive",
          "location": "config/",
          "constraint": "sequential_only",
          "reason": "Configuration changes must be atomic"
        },
        "test_redis": {
          "type": "shared_limited",
          "capacity": 3,
          "location": "test Redis instances",
          "reason": "Limited test Redis instances available"
        }
      }
    },
    "artifact_hash": "68b21f2fbbc92e618417028a6fd350c426b015c352ada34eb1cac157fd2ae39a"
  },
  "tasks": [
    {
      "id": "P1.T001",
      "feature_id": "F001",
      "title": "Design Admin Api architecture",
      "description": "Create detailed technical design for Admin Api",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/admin-api.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:admin-api:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/admin-api.md",
          "excerpt": "Feature specification for admin-api",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Define a versioned, secure Admin API (HTTP/gRPC) that fronts existing admin functions, enabling TUI/web/automation with RBAC and observability. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Smart foundational move! This is the unsexy work that pays dividends forever. REST + optional gRPC is pragmatic. The auth middleware + audit logging shows you're thinking about real production use. This unlocks so many possibilities - web UI, mobile apps, integrations. The 600-1000 LoC estimate feels right. One suggestion: consider GraphQL for the read-heavy operations (Stats, Peek) to avoid overfetching.",
        "motivation": "Create a stable contract for admin operations, allow remote control, and unlock future UI features while enforcing safety and auditability.",
        "design_deliverables": [
          "Architecture document in docs/design/admin-api.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Choose transport: HTTP+JSON (OpenAPI) with optional gRPC; generate clients where useful.",
          "Implement middleware: auth (bearer), rate limiting, request logging, correlation IDs.",
          "Map handlers to internal/admin functions; add pagination/validation.",
          "Versioning: /api/v1; document compat policy; structured errors.",
          "Observability: metrics (per-endpoint latency/error), audit logs for destructive ops.",
          "Ship minimal clients for TUI/CLI; integration tests with ephemeral Redis."
        ],
        "user_stories_to_address": [
          "I can call Stats/Peek/Purge endpoints with auth tokens.",
          "I consume a stable v1 API regardless of internal changes.",
          "I can scope tokens/roles to admin actions."
        ]
      },
      "acceptance_criteria": [
        "Spec published (OpenAPI and/or proto) for Stats, StatsKeys, Peek, PurgeDLQ, PurgeAll, Bench.",
        "Auth with deny\u2011by\u2011default; tokens verified; audit log persisted for destructive calls.",
        "Rate limits and explicit confirmation flags for destructive actions.",
        "Versioned paths; compat notes; structured error schema.",
        "Handler unit tests and integration tests pass in CI.",
        "Draft OpenAPI/proto; agree on schemas",
        "Auth middleware + config",
        "Implement Stats/StatsKeys",
        "Implement Peek",
        "Implement PurgeDLQ/PurgeAll with confirmations",
        "Implement Bench",
        "Add metrics + audit logs",
        "Write unit/integration tests",
        "Wire TUI Stats to API"
      ]
    },
    {
      "id": "P1.T002",
      "feature_id": "F001",
      "title": "Implement Admin Api core logic",
      "description": "Build the core functionality for Admin Api",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/admin-api/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P1.T002\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P1.T002\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P1.T002\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P1.T002\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [
          "redis_schema"
        ],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:admin-api:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [
        "admin_system"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/admin-api.md",
          "excerpt": "Feature specification for admin-api",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Define a versioned, secure Admin API (HTTP/gRPC) that fronts existing admin functions, enabling TUI/web/automation with RBAC and observability. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Smart foundational move! This is the unsexy work that pays dividends forever. REST + optional gRPC is pragmatic. The auth middleware + audit logging shows you're thinking about real production use. This unlocks so many possibilities - web UI, mobile apps, integrations. The 600-1000 LoC estimate feels right. One suggestion: consider GraphQL for the read-heavy operations (Stats, Peek) to avoid overfetching.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Draft OpenAPI/proto; agree on schemas",
          "Auth middleware + config",
          "Implement Stats/StatsKeys",
          "Implement Peek",
          "Implement PurgeDLQ/PurgeAll with confirmations",
          "Implement Bench",
          "Add metrics + audit logs",
          "Write unit/integration tests",
          "Wire TUI Stats to API"
        ],
        "technical_approach": [
          "Choose transport: HTTP+JSON (OpenAPI) with optional gRPC; generate clients where useful.",
          "Implement middleware: auth (bearer), rate limiting, request logging, correlation IDs.",
          "Map handlers to internal/admin functions; add pagination/validation.",
          "Versioning: /api/v1; document compat policy; structured errors.",
          "Observability: metrics (per-endpoint latency/error), audit logs for destructive ops.",
          "Ship minimal clients for TUI/CLI; integration tests with ephemeral Redis."
        ],
        "code_structure": {
          "module_path": "internal/admin-api/",
          "main_files": [
            "admin-api.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "admin-api_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        },
        "api_endpoints": [
          {
            "method": "GET",
            "path": "/api/v1/stats",
            "description": "Queue statistics"
          },
          {
            "method": "GET",
            "path": "/api/v1/stats/keys",
            "description": "List all queue keys"
          },
          {
            "method": "GET",
            "path": "/api/v1/queues/{queue}/peek",
            "description": "Peek at jobs"
          },
          {
            "method": "DELETE",
            "path": "/api/v1/queues/{queue}/dlq",
            "description": "Purge DLQ"
          },
          {
            "method": "DELETE",
            "path": "/api/v1/queues/all",
            "description": "Purge all queues"
          },
          {
            "method": "POST",
            "path": "/api/v1/bench",
            "description": "Run benchmarks"
          }
        ],
        "middleware_stack": [
          "CORS handler",
          "Request ID generator",
          "Bearer token authentication",
          "Rate limiter (100 req/min)",
          "Request/Response logger",
          "Panic recovery",
          "Timeout handler"
        ]
      },
      "acceptance_criteria": [
        "Spec published (OpenAPI and/or proto) for Stats, StatsKeys, Peek, PurgeDLQ, PurgeAll, Bench.",
        "Auth with deny\u2011by\u2011default; tokens verified; audit log persisted for destructive calls.",
        "Rate limits and explicit confirmation flags for destructive actions.",
        "Versioned paths; compat notes; structured error schema.",
        "Handler unit tests and integration tests pass in CI.",
        "Draft OpenAPI/proto; agree on schemas",
        "Auth middleware + config",
        "Implement Stats/StatsKeys",
        "Implement Peek",
        "Implement PurgeDLQ/PurgeAll with confirmations",
        "Implement Bench",
        "Add metrics + audit logs",
        "Write unit/integration tests",
        "Wire TUI Stats to API"
      ]
    },
    {
      "id": "P1.T003",
      "feature_id": "F001",
      "title": "Test Admin Api thoroughly",
      "description": "Comprehensive testing for Admin Api",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/admin-api/*_test.go",
            "test/e2e/admin-api_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:admin-api:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/admin-api.md",
          "excerpt": "Feature specification for admin-api",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: middleware (auth/rate/log) and handlers; fuzz path/query parsing."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: dockerized Redis; golden responses; auth failure/expiry cases."
            ]
          },
          {
            "type": "security",
            "requirements": [
              "Security: basic token leakage and privilege tests."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/admin-api/*_test.go",
          "integration_tests": "test/integration/admin-api_test.go",
          "e2e_tests": "test/e2e/admin-api_test.go",
          "benchmarks": "internal/admin-api/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ],
        "specific_test_scenarios": [
          "Valid bearer token allows access to all endpoints",
          "Expired token returns 401 Unauthorized",
          "Missing token returns 401 Unauthorized",
          "Invalid token format returns 400 Bad Request",
          "Rate limiting returns 429 after 100 requests/min",
          "Destructive operations require confirmation parameter",
          "Audit logs created for all DELETE operations",
          "Stats endpoint returns accurate queue counts",
          "Peek returns jobs without removing from queue",
          "Concurrent requests handled without race conditions",
          "CORS headers present for browser clients",
          "Request IDs propagated through logs"
        ]
      },
      "acceptance_criteria": [
        "Spec published (OpenAPI and/or proto) for Stats, StatsKeys, Peek, PurgeDLQ, PurgeAll, Bench.",
        "Auth with deny\u2011by\u2011default; tokens verified; audit log persisted for destructive calls.",
        "Rate limits and explicit confirmation flags for destructive actions.",
        "Versioned paths; compat notes; structured error schema.",
        "Handler unit tests and integration tests pass in CI.",
        "Draft OpenAPI/proto; agree on schemas",
        "Auth middleware + config",
        "Implement Stats/StatsKeys",
        "Implement Peek",
        "Implement PurgeDLQ/PurgeAll with confirmations",
        "Implement Bench",
        "Add metrics + audit logs",
        "Write unit/integration tests",
        "Wire TUI Stats to API"
      ]
    },
    {
      "id": "P2.T004",
      "feature_id": "F002",
      "title": "Design Multi Cluster Control architecture",
      "description": "Create detailed technical design for Multi Cluster Control",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/multi-cluster-control.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:multi-cluster-control:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/multi-cluster-control.md",
          "excerpt": "Feature specification for multi-cluster-control",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Manage multiple Redis clusters from a single TUI: quick switch tabs, side\u2011by\u2011side compare, and optionally propagate admin actions across clusters with confirmations. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is kubectl for job queues! The side-by-side compare is gold for catching prod/staging drift. The multi-apply is terrifying and powerful - imagine purging DLQs across 5 regions with one command. Color-code clusters (green=staging, red=prod) for safety. Consider adding a \"sync\" mode that shows real-time divergence between clusters. Maybe even a \"promote\" action that copies jobs from staging to prod?",
        "motivation": "- Reduce context switching between environments. - Detect configuration or behavior drift quickly. - Execute coordinated admin operations (e.g., purge DLQ in both staging regions after a fix).",
        "design_deliverables": [
          "Architecture document in docs/design/multi-cluster-control.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Config: accept multiple Redis endpoints with labels; hot\u2011switch via 1..9 or mouse tabs.",
          "Compare view: render side\u2011by\u2011side key stats; highlight deltas and anomalies.",
          "Propagate actions: opt\u2011in multi\u2011select clusters for actions (PurgeDLQ, Bench) with explicit confirm.",
          "Caching: per\u2011cluster polling with jitter; summarized health in tab bar.",
          "Observability: cluster\u2011qualified metrics and logs."
        ],
        "user_stories_to_address": [
          "I can switch clusters instantly and keep filters/focus.",
          "I can compare queue health across clusters.",
          "I can run an action on selected clusters with clear confirmation."
        ]
      },
      "acceptance_criteria": [
        "Multiple clusters configured with labels and colors.",
        "Side\u2011by\u2011side compare mode exists for Jobs and Workers.",
        "Multi\u2011apply actions require explicit selection and confirmation listing targets.",
        "Multi\u2011endpoint config + tabs",
        "Side\u2011by\u2011side compare views",
        "Multi\u2011apply action flow + confirmations",
        "Active jobs \u2502",
        "DLQ contents \u2502",
        "Pause target during sync \u2502"
      ]
    },
    {
      "id": "P2.T005",
      "feature_id": "F002",
      "title": "Implement Multi Cluster Control core logic",
      "description": "Build the core functionality for Multi Cluster Control",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/multi-cluster-control/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T005\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T005\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T005\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T005\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:multi-cluster-control:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/multi-cluster-control.md",
          "excerpt": "Feature specification for multi-cluster-control",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Manage multiple Redis clusters from a single TUI: quick switch tabs, side\u2011by\u2011side compare, and optionally propagate admin actions across clusters with confirmations. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is kubectl for job queues! The side-by-side compare is gold for catching prod/staging drift. The multi-apply is terrifying and powerful - imagine purging DLQs across 5 regions with one command. Color-code clusters (green=staging, red=prod) for safety. Consider adding a \"sync\" mode that shows real-time divergence between clusters. Maybe even a \"promote\" action that copies jobs from staging to prod?",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Multi\u2011endpoint config + tabs",
          "Side\u2011by\u2011side compare views",
          "Multi\u2011apply action flow + confirmations"
        ],
        "technical_approach": [
          "Config: accept multiple Redis endpoints with labels; hot\u2011switch via 1..9 or mouse tabs.",
          "Compare view: render side\u2011by\u2011side key stats; highlight deltas and anomalies.",
          "Propagate actions: opt\u2011in multi\u2011select clusters for actions (PurgeDLQ, Bench) with explicit confirm.",
          "Caching: per\u2011cluster polling with jitter; summarized health in tab bar.",
          "Observability: cluster\u2011qualified metrics and logs."
        ],
        "code_structure": {
          "module_path": "internal/multi-cluster-control/",
          "main_files": [
            "multi-cluster-control.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "multi-cluster-control_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Multiple clusters configured with labels and colors.",
        "Side\u2011by\u2011side compare mode exists for Jobs and Workers.",
        "Multi\u2011apply actions require explicit selection and confirmation listing targets.",
        "Multi\u2011endpoint config + tabs",
        "Side\u2011by\u2011side compare views",
        "Multi\u2011apply action flow + confirmations",
        "Active jobs \u2502",
        "DLQ contents \u2502",
        "Pause target during sync \u2502"
      ]
    },
    {
      "id": "P2.T006",
      "feature_id": "F002",
      "title": "Test Multi Cluster Control thoroughly",
      "description": "Comprehensive testing for Multi Cluster Control",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/multi-cluster-control/*_test.go",
            "test/e2e/multi-cluster-control_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:multi-cluster-control:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/multi-cluster-control.md",
          "excerpt": "Feature specification for multi-cluster-control",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: config parsing; selection logic; confirmation prompts."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Manual: latency impact; consistency of polling; safe multi\u2011apply."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/multi-cluster-control/*_test.go",
          "integration_tests": "test/integration/multi-cluster-control_test.go",
          "e2e_tests": "test/e2e/multi-cluster-control_test.go",
          "benchmarks": "internal/multi-cluster-control/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Multiple clusters configured with labels and colors.",
        "Side\u2011by\u2011side compare mode exists for Jobs and Workers.",
        "Multi\u2011apply actions require explicit selection and confirmation listing targets.",
        "Multi\u2011endpoint config + tabs",
        "Side\u2011by\u2011side compare views",
        "Multi\u2011apply action flow + confirmations",
        "Active jobs \u2502",
        "DLQ contents \u2502",
        "Pause target during sync \u2502"
      ]
    },
    {
      "id": "P2.T007",
      "feature_id": "F003",
      "title": "Design Visual Dag Builder architecture",
      "description": "Create detailed technical design for Visual Dag Builder",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/visual-dag-builder.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:visual-dag-builder:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/visual-dag-builder.md",
          "excerpt": "Feature specification for visual-dag-builder",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Design and ship a terminal-native visual builder for multi-step workflows. Users assemble stages with dependencies, retries, and compensations, then submit the DAG as a reusable pipeline. This turns the queue into a workflow engine without losing Redis simplicity. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > HOLY SHIT. A DAG builder in a terminal?! This is either genius or madness. Use box-drawing characters for edges, colored nodes for status. The keyboard navigation is crucial - vim-style hjkl for movement, 'a' to add node, 'c' to connect. Start SIMPLE - just sequential pipelines first, then add branching. The compensation pattern is gold for financial/payment workflows. This could be your \"iPhone moment\" - the feature that redefines what's possible in a terminal.",
        "motivation": "- Express end-to-end business processes as composable pipelines. - Improve reliability with built-in retries/compensations. - Enable reuse: versioned workflows that teams can share and automate.",
        "design_deliverables": [
          "Architecture document in docs/design/visual-dag-builder.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Model: introduce minimal workflow spec (YAML/JSON) with nodes, edges, retry/backoff, compensation, timeout, concurrency caps.",
          "Persistence: store workflow specs under Redis keys or a small metadata store; versioned with hashes.",
          "Execution: add a light orchestrator that enqueues per-node jobs and tracks completion via processing lists/heartbeats.",
          "Canvas: grid layout with keyboard navigation; node palette; inspector panel.",
          "Node types: task, decision (conditional), delay/timer, compensation.",
          "Validation: detect cycles, unreachable nodes, unsatisfied dependencies.",
          "Run: submit workflow + parameters; show live state (colors by status).",
          "API: endpoints to create/list/get/version workflows, start run, inspect run state.",
          "Observability: per-run trace ID; per-node timings; failure stats."
        ],
        "user_stories_to_address": [
          "I can draw a workflow, validate it, and save it with version notes.",
          "I can start a run with parameters and watch node states update in real time.",
          "I can inspect a failed run and see exactly which node failed and why."
        ]
      },
      "acceptance_criteria": [
        "DAG validation prevents cycles and missing deps.",
        "Runs persist state; survive restarts; resumable.",
        "Per-node retry/backoff and optional compensation supported.",
        "TUI shows statuses: queued, running, success, failed, compensated.",
        "Define workflow spec (schema + examples)",
        "Implement orchestrator (enqueue + state tracking)",
        "Admin API CRUD + run endpoints",
        "TUI canvas + inspector + palette",
        "Live status rendering + colors",
        "Docs + demos"
      ]
    },
    {
      "id": "P2.T008",
      "feature_id": "F003",
      "title": "Implement Visual Dag Builder core logic",
      "description": "Build the core functionality for Visual Dag Builder",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/visual-dag-builder/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T008\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T008\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T008\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T008\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:visual-dag-builder:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [
        "tui_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/visual-dag-builder.md",
          "excerpt": "Feature specification for visual-dag-builder",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Design and ship a terminal-native visual builder for multi-step workflows. Users assemble stages with dependencies, retries, and compensations, then submit the DAG as a reusable pipeline. This turns the queue into a workflow engine without losing Redis simplicity. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > HOLY SHIT. A DAG builder in a terminal?! This is either genius or madness. Use box-drawing characters for edges, colored nodes for status. The keyboard navigation is crucial - vim-style hjkl for movement, 'a' to add node, 'c' to connect. Start SIMPLE - just sequential pipelines first, then add branching. The compensation pattern is gold for financial/payment workflows. This could be your \"iPhone moment\" - the feature that redefines what's possible in a terminal.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define workflow spec (schema + examples)",
          "Implement orchestrator (enqueue + state tracking)",
          "Admin API CRUD + run endpoints",
          "TUI canvas + inspector + palette",
          "Live status rendering + colors",
          "Docs + demos"
        ],
        "technical_approach": [
          "Model: introduce minimal workflow spec (YAML/JSON) with nodes, edges, retry/backoff, compensation, timeout, concurrency caps.",
          "Persistence: store workflow specs under Redis keys or a small metadata store; versioned with hashes.",
          "Execution: add a light orchestrator that enqueues per-node jobs and tracks completion via processing lists/heartbeats.",
          "Canvas: grid layout with keyboard navigation; node palette; inspector panel.",
          "Node types: task, decision (conditional), delay/timer, compensation.",
          "Validation: detect cycles, unreachable nodes, unsatisfied dependencies.",
          "Run: submit workflow + parameters; show live state (colors by status).",
          "API: endpoints to create/list/get/version workflows, start run, inspect run state.",
          "Observability: per-run trace ID; per-node timings; failure stats."
        ],
        "code_structure": {
          "module_path": "internal/visual-dag-builder/",
          "main_files": [
            "visual-dag-builder.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "visual-dag-builder_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        },
        "tui_components": [
          "DAG canvas with grid layout",
          "Node palette (task, decision, parallel, loop)",
          "Properties inspector panel",
          "Keyboard navigation (vim-style)",
          "ASCII art rendering engine",
          "DAG validation engine",
          "Workflow execution tracker"
        ],
        "dag_operations": [
          "Add/remove nodes",
          "Connect/disconnect edges",
          "Validate DAG (cycle detection)",
          "Topological sort",
          "Save/load workflows",
          "Execute workflow",
          "Monitor execution state"
        ]
      },
      "acceptance_criteria": [
        "DAG validation prevents cycles and missing deps.",
        "Runs persist state; survive restarts; resumable.",
        "Per-node retry/backoff and optional compensation supported.",
        "TUI shows statuses: queued, running, success, failed, compensated.",
        "Define workflow spec (schema + examples)",
        "Implement orchestrator (enqueue + state tracking)",
        "Admin API CRUD + run endpoints",
        "TUI canvas + inspector + palette",
        "Live status rendering + colors",
        "Docs + demos"
      ]
    },
    {
      "id": "P2.T009",
      "feature_id": "F003",
      "title": "Test Visual Dag Builder thoroughly",
      "description": "Comprehensive testing for Visual Dag Builder",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/visual-dag-builder/*_test.go",
            "test/e2e/visual-dag-builder_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:visual-dag-builder:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/visual-dag-builder.md",
          "excerpt": "Feature specification for visual-dag-builder",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: DAG validation, topo sort, retry/backoff math, compensation trigger."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: end-to-end run with induced failures and resumptions.",
              "TUI: golden snapshots for small DAGs; navigation tests."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/visual-dag-builder/*_test.go",
          "integration_tests": "test/integration/visual-dag-builder_test.go",
          "e2e_tests": "test/e2e/visual-dag-builder_test.go",
          "benchmarks": "internal/visual-dag-builder/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ],
        "specific_test_scenarios": [
          "Can add nodes with keyboard shortcuts",
          "Can connect nodes to form edges",
          "Cycle detection prevents invalid DAGs",
          "Topological sort produces correct execution order",
          "Canvas renders correctly at different zoom levels",
          "Keyboard navigation works with vim bindings",
          "Properties panel updates on node selection",
          "Workflow saves and loads correctly",
          "Execution state updates in real-time",
          "Validation highlights errors visually"
        ]
      },
      "acceptance_criteria": [
        "DAG validation prevents cycles and missing deps.",
        "Runs persist state; survive restarts; resumable.",
        "Per-node retry/backoff and optional compensation supported.",
        "TUI shows statuses: queued, running, success, failed, compensated.",
        "Define workflow spec (schema + examples)",
        "Implement orchestrator (enqueue + state tracking)",
        "Admin API CRUD + run endpoints",
        "TUI canvas + inspector + palette",
        "Live status rendering + colors",
        "Docs + demos"
      ]
    },
    {
      "id": "P1.T010",
      "feature_id": "F004",
      "title": "Design Distributed Tracing Integration architecture",
      "description": "Create detailed technical design for Distributed Tracing Integration",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/distributed-tracing-integration.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:distributed-tracing-integration:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/distributed-tracing-integration.md",
          "excerpt": "Feature specification for distributed-tracing-integration",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Make tracing first\u2011class with OpenTelemetry: automatically create spans for enqueue, dequeue, and job processing, propagate context through job payloads/metadata, and link to external tracing backends. Add trace exemplars to metrics and expose trace actions in the TUI. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is exactly what distributed systems need but rarely get right! The context propagation through job metadata is brilliant - it preserves the trace lineage without polluting payloads. Trace exemplars linking metrics spikes to actual traces is pure gold for debugging. The TUI integration with \"Open Trace\" actions turns this from yet another observability tax into a developer superpower. This could be the feature that makes your queue the obvious choice for microservices architectures.",
        "motivation": "- Correlate queue activity with upstream/downstream services for faster RCA. - Provide visibility into per\u2011job timing (queueing, processing) and failures. - Standardize on OTel to interoperate with existing org observability stacks.",
        "design_deliverables": [
          "Architecture document in docs/design/distributed-tracing-integration.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "SDK & instrumentation:",
          "Use go.opentelemetry.io/otel across producer, worker, and admin.",
          "Enqueue: start span queue.enqueue with attributes (queue, size, priority, tenant, idempotency_id).",
          "Dequeue: span queue.dequeue with wait time and queue depth at dequeue.",
          "Process: span job.process around user handler; record retries, outcome, error class.",
          "Link parent context if traceparent/tracestate present in payload metadata; otherwise start a new root and inject on enqueue.",
          "Propagation:",
          "Embed W3C trace headers in job metadata (not payload) to avoid accidental redaction.",
          "Ensure workers extract before processing and reinject on any outbound calls.",
          "Exporters & sampling:",
          "Default OTLP exporter to local Collector; config for endpoints/auth.",
          "Head sampling with per\u2011route/queue rates; tail sampling via Collector for high\u2011value spans (errors, long latency).",
          "Metrics + exemplars:",
          "Attach trace IDs to latency/error metrics as exemplars when sampled.",
          "TUI integration:",
          "Show trace ID in Peek/Info; provide an \u201cOpen Trace\u201d action and copyable link; enable quick filter by trace ID.",
          "Security & privacy:",
          "Redact sensitive attributes; configurable allowlist for span attributes.",
          "Disable/limit tracing in prod via config and sampling controls."
        ],
        "user_stories_to_address": [
          "I can open the trace for a failed job directly from the TUI.",
          "I can see queueing time vs processing time for a class of jobs."
        ]
      },
      "acceptance_criteria": [
        "Spans emitted for enqueue/dequeue/process with consistent attributes.",
        "Context propagates via metadata; upstream trace linkage verified.",
        "TUI shows trace IDs and open/copy actions.",
        "Add otel setup in internal/obs/tracing.go with config",
        "Instrument producer/worker/admin critical paths",
        "Inject/extract trace headers in metadata",
        "Add TUI trace actions in Peek/Info",
        "Docs with backend examples and sampling guidance",
        "regexp.Regexp",
        "attribute.KeyValue) attribute.KeyValue {",
        "attribute.KeyValue, len(attrs))"
      ]
    },
    {
      "id": "P1.T011",
      "feature_id": "F004",
      "title": "Implement Distributed Tracing Integration core logic",
      "description": "Build the core functionality for Distributed Tracing Integration",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/distributed-tracing-integration/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P1.T011\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P1.T011\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P1.T011\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P1.T011\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [
          "redis_schema"
        ],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:distributed-tracing-integration:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/distributed-tracing-integration.md",
          "excerpt": "Feature specification for distributed-tracing-integration",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Make tracing first\u2011class with OpenTelemetry: automatically create spans for enqueue, dequeue, and job processing, propagate context through job payloads/metadata, and link to external tracing backends. Add trace exemplars to metrics and expose trace actions in the TUI. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is exactly what distributed systems need but rarely get right! The context propagation through job metadata is brilliant - it preserves the trace lineage without polluting payloads. Trace exemplars linking metrics spikes to actual traces is pure gold for debugging. The TUI integration with \"Open Trace\" actions turns this from yet another observability tax into a developer superpower. This could be the feature that makes your queue the obvious choice for microservices architectures.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Add otel setup in internal/obs/tracing.go with config",
          "Instrument producer/worker/admin critical paths",
          "Inject/extract trace headers in metadata",
          "Add TUI trace actions in Peek/Info",
          "Docs with backend examples and sampling guidance"
        ],
        "technical_approach": [
          "SDK & instrumentation:",
          "Use go.opentelemetry.io/otel across producer, worker, and admin.",
          "Enqueue: start span queue.enqueue with attributes (queue, size, priority, tenant, idempotency_id).",
          "Dequeue: span queue.dequeue with wait time and queue depth at dequeue.",
          "Process: span job.process around user handler; record retries, outcome, error class.",
          "Link parent context if traceparent/tracestate present in payload metadata; otherwise start a new root and inject on enqueue.",
          "Propagation:",
          "Embed W3C trace headers in job metadata (not payload) to avoid accidental redaction.",
          "Ensure workers extract before processing and reinject on any outbound calls.",
          "Exporters & sampling:",
          "Default OTLP exporter to local Collector; config for endpoints/auth.",
          "Head sampling with per\u2011route/queue rates; tail sampling via Collector for high\u2011value spans (errors, long latency).",
          "Metrics + exemplars:",
          "Attach trace IDs to latency/error metrics as exemplars when sampled.",
          "TUI integration:",
          "Show trace ID in Peek/Info; provide an \u201cOpen Trace\u201d action and copyable link; enable quick filter by trace ID.",
          "Security & privacy:",
          "Redact sensitive attributes; configurable allowlist for span attributes.",
          "Disable/limit tracing in prod via config and sampling controls."
        ],
        "code_structure": {
          "module_path": "internal/distributed-tracing-integration/",
          "main_files": [
            "distributed-tracing-integration.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "distributed-tracing-integration_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        },
        "tracing_components": [
          "OpenTelemetry SDK integration",
          "Trace context propagation",
          "Span creation and management",
          "Trace ID generation",
          "Baggage propagation",
          "Sampling strategies",
          "Exporter configuration (Jaeger, Zipkin, etc.)"
        ]
      },
      "acceptance_criteria": [
        "Spans emitted for enqueue/dequeue/process with consistent attributes.",
        "Context propagates via metadata; upstream trace linkage verified.",
        "TUI shows trace IDs and open/copy actions.",
        "Add otel setup in internal/obs/tracing.go with config",
        "Instrument producer/worker/admin critical paths",
        "Inject/extract trace headers in metadata",
        "Add TUI trace actions in Peek/Info",
        "Docs with backend examples and sampling guidance",
        "regexp.Regexp",
        "attribute.KeyValue) attribute.KeyValue {",
        "attribute.KeyValue, len(attrs))"
      ]
    },
    {
      "id": "P1.T012",
      "feature_id": "F004",
      "title": "Test Distributed Tracing Integration thoroughly",
      "description": "Comprehensive testing for Distributed Tracing Integration",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/distributed-tracing-integration/*_test.go",
            "test/e2e/distributed-tracing-integration_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:distributed-tracing-integration:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/distributed-tracing-integration.md",
          "excerpt": "Feature specification for distributed-tracing-integration",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: propagation helpers; attribute sets; error recording."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: OTLP to Collector; verify parent/child linkage across enqueue\u2192process.",
              "Manual: TUI action opens correct trace; sampled exemplar matches metric spike."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/distributed-tracing-integration/*_test.go",
          "integration_tests": "test/integration/distributed-tracing-integration_test.go",
          "e2e_tests": "test/e2e/distributed-tracing-integration_test.go",
          "benchmarks": "internal/distributed-tracing-integration/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Spans emitted for enqueue/dequeue/process with consistent attributes.",
        "Context propagates via metadata; upstream trace linkage verified.",
        "TUI shows trace IDs and open/copy actions.",
        "Add otel setup in internal/obs/tracing.go with config",
        "Instrument producer/worker/admin critical paths",
        "Inject/extract trace headers in metadata",
        "Add TUI trace actions in Peek/Info",
        "Docs with backend examples and sampling guidance",
        "regexp.Regexp",
        "attribute.KeyValue) attribute.KeyValue {",
        "attribute.KeyValue, len(attrs))"
      ]
    },
    {
      "id": "P3.T013",
      "feature_id": "F005",
      "title": "Design Plugin Panel System architecture",
      "description": "Create detailed technical design for Plugin Panel System",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/plugin-panel-system.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:plugin-panel-system:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/plugin-panel-system.md",
          "excerpt": "Feature specification for plugin-panel-system",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A terminal\u2011native plugin framework that lets teams drop in custom panels for org\u2011specific metrics, transforms, and actions. Plugins render inside the TUI, receive typed events (stats, selection, timers), and can call a scoped Admin API with explicit capabilities. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is how you build an ecosystem! WASM is the right choice - safe, portable, and trendy. The capability model is crucial - nobody wants a rogue plugin purging prod. Start with read-only plugins to build trust. The hot-reload is chef's kiss for developer experience. Look at Zellij's WASM plugins for inspiration. Consider a plugin marketplace/registry early - that's how VSCode won.",
        "motivation": "- Organizations need bespoke dashboards and actions (tenant stats, SLA widgets, proprietary transforms). - Avoid hard\u2011coding one\u2011off features into core; encourage contributions without risking stability. - Create a marketplace of panels that showcase the TUI.",
        "design_deliverables": [
          "Architecture document in docs/design/plugin-panel-system.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Runtime: start with WASM (TinyGo) or Starlark/Lua for safety and portability; avoid Go plugin due to portability issues.",
          "API Surface (capability\u2011gated):",
          "Read\u2011only: subscribe to stats, keys, selection, timers.",
          "Actions: enqueue, peek, requeue, purge (require explicit user grant per plugin).",
          "UI: render text blocks with styles; receive keyboard/mouse events in plugin\u2019s zone.",
          "Packaging: plugin bundle = manifest.yaml (name, version, permissions, entry), bytecode/script, optional assets.",
          "Lifecycle: discover under plugins/, validate manifest + permissions, load sandbox, render panel region; hot\u2011reload on file change.",
          "Sandboxing: CPU/memory/time limits; deny filesystem/network by default; only brokered API calls allowed.",
          "Versioning: semantic version the host/plugin API; shims for minor changes; e2e contract tests.",
          "Samples: \u201cTenant SLA\u201d read\u2011only panel; \u201cBulk Requeue Helper\u201d action panel (cap\u2011gated)."
        ],
        "user_stories_to_address": [
          "I can enable a plugin and see its panel render without crashing the TUI.",
          "I can inspect requested permissions and approve/deny.",
          "I can build a plugin with typed events, test locally, and hot\u2011reload."
        ]
      },
      "acceptance_criteria": [
        "Host loads/isolates plugins with manifest validation and resource limits.",
        "Capability prompts on first run; persisted decisions per plugin version.",
        "Stable v1 API documented with examples in WASM and Lua/Starlark.",
        "Two sample plugins shipped and tested.",
        "Define host API + capability model",
        "Choose runtime (WASM + Starlark) and embed",
        "Implement loader, sandbox, event bus",
        "Implement renderer adapter (panel zone)",
        "Add permission UI + persistence",
        "Build sample plugins + docs"
      ]
    },
    {
      "id": "P3.T014",
      "feature_id": "F005",
      "title": "Implement Plugin Panel System core logic",
      "description": "Build the core functionality for Plugin Panel System",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/plugin-panel-system/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T014\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T014\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T014\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T014\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:plugin-panel-system:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/plugin-panel-system.md",
          "excerpt": "Feature specification for plugin-panel-system",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A terminal\u2011native plugin framework that lets teams drop in custom panels for org\u2011specific metrics, transforms, and actions. Plugins render inside the TUI, receive typed events (stats, selection, timers), and can call a scoped Admin API with explicit capabilities. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is how you build an ecosystem! WASM is the right choice - safe, portable, and trendy. The capability model is crucial - nobody wants a rogue plugin purging prod. Start with read-only plugins to build trust. The hot-reload is chef's kiss for developer experience. Look at Zellij's WASM plugins for inspiration. Consider a plugin marketplace/registry early - that's how VSCode won.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define host API + capability model",
          "Choose runtime (WASM + Starlark) and embed",
          "Implement loader, sandbox, event bus",
          "Implement renderer adapter (panel zone)",
          "Add permission UI + persistence",
          "Build sample plugins + docs"
        ],
        "technical_approach": [
          "Runtime: start with WASM (TinyGo) or Starlark/Lua for safety and portability; avoid Go plugin due to portability issues.",
          "API Surface (capability\u2011gated):",
          "Read\u2011only: subscribe to stats, keys, selection, timers.",
          "Actions: enqueue, peek, requeue, purge (require explicit user grant per plugin).",
          "UI: render text blocks with styles; receive keyboard/mouse events in plugin\u2019s zone.",
          "Packaging: plugin bundle = manifest.yaml (name, version, permissions, entry), bytecode/script, optional assets.",
          "Lifecycle: discover under plugins/, validate manifest + permissions, load sandbox, render panel region; hot\u2011reload on file change.",
          "Sandboxing: CPU/memory/time limits; deny filesystem/network by default; only brokered API calls allowed.",
          "Versioning: semantic version the host/plugin API; shims for minor changes; e2e contract tests.",
          "Samples: \u201cTenant SLA\u201d read\u2011only panel; \u201cBulk Requeue Helper\u201d action panel (cap\u2011gated)."
        ],
        "code_structure": {
          "module_path": "internal/plugin-panel-system/",
          "main_files": [
            "plugin-panel-system.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "plugin-panel-system_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Host loads/isolates plugins with manifest validation and resource limits.",
        "Capability prompts on first run; persisted decisions per plugin version.",
        "Stable v1 API documented with examples in WASM and Lua/Starlark.",
        "Two sample plugins shipped and tested.",
        "Define host API + capability model",
        "Choose runtime (WASM + Starlark) and embed",
        "Implement loader, sandbox, event bus",
        "Implement renderer adapter (panel zone)",
        "Add permission UI + persistence",
        "Build sample plugins + docs"
      ]
    },
    {
      "id": "P3.T015",
      "feature_id": "F006",
      "title": "Design Time Travel Debugger architecture",
      "description": "Create detailed technical design for Time Travel Debugger",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/time-travel-debugger.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:time-travel-debugger:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/time-travel-debugger.md",
          "excerpt": "Feature specification for time-travel-debugger",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A revolutionary debugging tool that captures job execution history and allows developers to replay, step through, and analyze past job runs in the TUI. Navigate through time to see exact states, variables, and decision points that led to success or failure. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is absolutely game-changing. Event sourcing for job debugging is brilliant - imagine being able to step through EXACTLY what happened during that 3am production incident, seeing every state transition, retry decision, and payload mutation. The VCR-style controls make complex async debugging as simple as watching a video. The comparison mode between successful/failed runs is pure genius - no more guessing why job A worked but job B didn't. This eliminates the debugging dark ages of \"I can't reproduce this locally.\"",
        "motivation": "- Eliminate \"works on my machine\" by replaying exact production scenarios - Debug intermittent failures that can't be reproduced - Understand complex job flows without adding more logging - Train new engineers by replaying interesting historical cases",
        "design_deliverables": [
          "Architecture document in docs/design/time-travel-debugger.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Event Capture:",
          "Record all state transitions with timestamps (enqueue, start, retry, complete, fail)",
          "Capture snapshots of payload, worker state, Redis state at key moments",
          "Use ring buffer with configurable retention (e.g., last 1000 jobs or 24 hours)",
          "Compress and store in Redis Streams or separate time-series store",
          "Replay Engine:",
          "Reconstruct exact execution timeline from events",
          "Support play/pause/step/rewind with variable speed",
          "Show state diffs between steps",
          "Highlight decision points (retry logic, routing, etc.)",
          "TUI Integration:",
          "Timeline scrubber with keyboard (h/l for back/forward, space to play/pause)",
          "Split view: current state vs. historical state",
          "Breadcrumb trail of execution path",
          "\"Jump to failure\" shortcuts",
          "Analysis Tools:",
          "Compare multiple job runs side-by-side",
          "Pattern detection across similar failures",
          "Export replay sessions for sharing/training",
          "Performance:",
          "Async event capture to avoid blocking job processing",
          "Sampling options for high-volume queues",
          "Automatic pruning of old events"
        ],
        "user_stories_to_address": [
          "I can replay a failed production job locally to understand what went wrong",
          "I can step through the exact sequence of events during an incident",
          "I can use historical replays for training and code reviews"
        ]
      },
      "acceptance_criteria": [
        "Complete execution history captured without >5% performance impact",
        "Replay controls work smoothly in TUI with <100ms response time",
        "Can export and share replay sessions",
        "Historical data automatically pruned based on retention policy",
        "Design event schema and storage strategy",
        "Implement event capture middleware",
        "Build timeline reconstruction engine",
        "Create TUI replay interface",
        "Add comparison and analysis tools",
        "Performance optimization pass",
        "Documentation and examples",
        "EventFilter"
      ]
    },
    {
      "id": "P3.T016",
      "feature_id": "F006",
      "title": "Implement Time Travel Debugger core logic",
      "description": "Build the core functionality for Time Travel Debugger",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/time-travel-debugger/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T016\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T016\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T016\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T016\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:time-travel-debugger:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/time-travel-debugger.md",
          "excerpt": "Feature specification for time-travel-debugger",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A revolutionary debugging tool that captures job execution history and allows developers to replay, step through, and analyze past job runs in the TUI. Navigate through time to see exact states, variables, and decision points that led to success or failure. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is absolutely game-changing. Event sourcing for job debugging is brilliant - imagine being able to step through EXACTLY what happened during that 3am production incident, seeing every state transition, retry decision, and payload mutation. The VCR-style controls make complex async debugging as simple as watching a video. The comparison mode between successful/failed runs is pure genius - no more guessing why job A worked but job B didn't. This eliminates the debugging dark ages of \"I can't reproduce this locally.\"",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Design event schema and storage strategy",
          "Implement event capture middleware",
          "Build timeline reconstruction engine",
          "Create TUI replay interface",
          "Add comparison and analysis tools",
          "Performance optimization pass",
          "Documentation and examples"
        ],
        "technical_approach": [
          "Event Capture:",
          "Record all state transitions with timestamps (enqueue, start, retry, complete, fail)",
          "Capture snapshots of payload, worker state, Redis state at key moments",
          "Use ring buffer with configurable retention (e.g., last 1000 jobs or 24 hours)",
          "Compress and store in Redis Streams or separate time-series store",
          "Replay Engine:",
          "Reconstruct exact execution timeline from events",
          "Support play/pause/step/rewind with variable speed",
          "Show state diffs between steps",
          "Highlight decision points (retry logic, routing, etc.)",
          "TUI Integration:",
          "Timeline scrubber with keyboard (h/l for back/forward, space to play/pause)",
          "Split view: current state vs. historical state",
          "Breadcrumb trail of execution path",
          "\"Jump to failure\" shortcuts",
          "Analysis Tools:",
          "Compare multiple job runs side-by-side",
          "Pattern detection across similar failures",
          "Export replay sessions for sharing/training",
          "Performance:",
          "Async event capture to avoid blocking job processing",
          "Sampling options for high-volume queues",
          "Automatic pruning of old events"
        ],
        "code_structure": {
          "module_path": "internal/time-travel-debugger/",
          "main_files": [
            "time-travel-debugger.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "time-travel-debugger_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Complete execution history captured without >5% performance impact",
        "Replay controls work smoothly in TUI with <100ms response time",
        "Can export and share replay sessions",
        "Historical data automatically pruned based on retention policy",
        "Design event schema and storage strategy",
        "Implement event capture middleware",
        "Build timeline reconstruction engine",
        "Create TUI replay interface",
        "Add comparison and analysis tools",
        "Performance optimization pass",
        "Documentation and examples",
        "EventFilter"
      ]
    },
    {
      "id": "P1.T017",
      "feature_id": "F007",
      "title": "Design Exactly Once Patterns architecture",
      "description": "Create detailed technical design for Exactly Once Patterns",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/exactly-once-patterns.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:exactly-once-patterns:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/exactly-once-patterns.md",
          "excerpt": "Feature specification for exactly-once-patterns",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Document and optionally enforce patterns for effectively exactly\u2011once processing: idempotency keys, deduplication sets, and the transactional outbox. Provide helpers and guardrails so teams can adopt robust semantics incrementally. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is THE reliability feature that separates amateur systems from production-grade infrastructure! The combination of idempotency keys with Redis-backed deduplication and transactional outbox patterns is chef's kiss. The TTL-based cleanup is brilliant for practical bounds. This tackles the hardest problem in distributed systems - exactly-once semantics - with battle-tested patterns. The Admin API visibility into dedup effectiveness will save so many debugging hours.",
        "motivation": "- Reduce duplicate side effects and simplify consumer code. - Align with best practices without forcing heavy frameworks. - Improve reliability in distributed workflows.",
        "design_deliverables": [
          "Architecture document in docs/design/exactly-once-patterns.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Idempotency keys:",
          "Producer attaches id per job; worker records id in a dedup set/hash with TTL.",
          "Before side effects, perform SETNX/HSETNX style guard; on success, proceed; else skip.",
          "Dedup store:",
          "Redis SET/HASH keyed by queue/tenant; TTL for practical bounds; cardinality metrics.",
          "Outbox pattern:",
          "Provide library hooks to write DB changes + outbox event in one transaction.",
          "Separate relay publishes from outbox to queue; worker idempotency still checked.",
          "Go utilities for idempotency check/update and outbox relay; Admin API to inspect dedup stats.",
          "Info panel: idempotency hit rate; dedup size; recent duplicates avoided."
        ],
        "user_stories_to_address": [
          "I can mark jobs with an idempotency key and rely on helpers to avoid duplicate processing.",
          "I can see dedup rates and size to tune TTLs."
        ]
      },
      "acceptance_criteria": [
        "Idempotency helpers and dedup storage shipped with metrics.",
        "Optional outbox relay with sample integrations.",
        "Documentation of tradeoffs and failure modes.",
        "Idempotency key helper + storage",
        "Metrics + Admin API for dedup stats",
        "Outbox relay library + example",
        "Docs + tuning guide",
        "string // fields to include in hash",
        "byte(fmt.Sprintf(\"%v\", value)))",
        "string{d.keyName(key)}, jobID, int(ttl.Seconds())).Result()",
        "string{r.keyName(key)},"
      ]
    },
    {
      "id": "P1.T018",
      "feature_id": "F007",
      "title": "Implement Exactly Once Patterns core logic",
      "description": "Build the core functionality for Exactly Once Patterns",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/exactly-once-patterns/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P1.T018\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P1.T018\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P1.T018\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P1.T018\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [
          "redis_schema"
        ],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:exactly-once-patterns:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/exactly-once-patterns.md",
          "excerpt": "Feature specification for exactly-once-patterns",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Document and optionally enforce patterns for effectively exactly\u2011once processing: idempotency keys, deduplication sets, and the transactional outbox. Provide helpers and guardrails so teams can adopt robust semantics incrementally. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is THE reliability feature that separates amateur systems from production-grade infrastructure! The combination of idempotency keys with Redis-backed deduplication and transactional outbox patterns is chef's kiss. The TTL-based cleanup is brilliant for practical bounds. This tackles the hardest problem in distributed systems - exactly-once semantics - with battle-tested patterns. The Admin API visibility into dedup effectiveness will save so many debugging hours.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Idempotency key helper + storage",
          "Metrics + Admin API for dedup stats",
          "Outbox relay library + example",
          "Docs + tuning guide"
        ],
        "technical_approach": [
          "Idempotency keys:",
          "Producer attaches id per job; worker records id in a dedup set/hash with TTL.",
          "Before side effects, perform SETNX/HSETNX style guard; on success, proceed; else skip.",
          "Dedup store:",
          "Redis SET/HASH keyed by queue/tenant; TTL for practical bounds; cardinality metrics.",
          "Outbox pattern:",
          "Provide library hooks to write DB changes + outbox event in one transaction.",
          "Separate relay publishes from outbox to queue; worker idempotency still checked.",
          "Go utilities for idempotency check/update and outbox relay; Admin API to inspect dedup stats.",
          "Info panel: idempotency hit rate; dedup size; recent duplicates avoided."
        ],
        "code_structure": {
          "module_path": "internal/exactly-once-patterns/",
          "main_files": [
            "exactly-once-patterns.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "exactly-once-patterns_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        },
        "deduplication_mechanisms": [
          "Idempotency key generation",
          "Request fingerprinting",
          "State tracking with Redis SETNX",
          "Transactional outbox pattern",
          "Two-phase commit simulation",
          "Compensation logic",
          "Retry with exponential backoff"
        ]
      },
      "acceptance_criteria": [
        "Idempotency helpers and dedup storage shipped with metrics.",
        "Optional outbox relay with sample integrations.",
        "Documentation of tradeoffs and failure modes.",
        "Idempotency key helper + storage",
        "Metrics + Admin API for dedup stats",
        "Outbox relay library + example",
        "Docs + tuning guide",
        "string // fields to include in hash",
        "byte(fmt.Sprintf(\"%v\", value)))",
        "string{d.keyName(key)}, jobID, int(ttl.Seconds())).Result()",
        "string{r.keyName(key)},"
      ]
    },
    {
      "id": "P1.T019",
      "feature_id": "F007",
      "title": "Test Exactly Once Patterns thoroughly",
      "description": "Comprehensive testing for Exactly Once Patterns",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/exactly-once-patterns/*_test.go",
            "test/e2e/exactly-once-patterns_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:exactly-once-patterns:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/exactly-once-patterns.md",
          "excerpt": "Feature specification for exactly-once-patterns",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: dedup guard correctness; TTL expiry edge cases; outbox relay idempotency."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: duplicate injection tests under load; DB+outbox transaction demo."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/exactly-once-patterns/*_test.go",
          "integration_tests": "test/integration/exactly-once-patterns_test.go",
          "e2e_tests": "test/e2e/exactly-once-patterns_test.go",
          "benchmarks": "internal/exactly-once-patterns/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ],
        "specific_test_scenarios": [
          "Duplicate requests with same idempotency key return cached response",
          "Idempotency keys expire after configured TTL",
          "Concurrent duplicate requests handled correctly",
          "Failed requests can be retried with same key",
          "Successful requests cannot be replayed",
          "State transitions are atomic",
          "Compensation triggered on partial failures",
          "Request fingerprints remain stable"
        ]
      },
      "acceptance_criteria": [
        "Idempotency helpers and dedup storage shipped with metrics.",
        "Optional outbox relay with sample integrations.",
        "Documentation of tradeoffs and failure modes.",
        "Idempotency key helper + storage",
        "Metrics + Admin API for dedup stats",
        "Outbox relay library + example",
        "Docs + tuning guide",
        "string // fields to include in hash",
        "byte(fmt.Sprintf(\"%v\", value)))",
        "string{d.keyName(key)}, jobID, int(ttl.Seconds())).Result()",
        "string{r.keyName(key)},"
      ]
    },
    {
      "id": "P1.T020",
      "feature_id": "F008",
      "title": "Design Rbac And Tokens architecture",
      "description": "Create detailed technical design for Rbac And Tokens",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/rbac-and-tokens.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:rbac-and-tokens:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/rbac-and-tokens.md",
          "excerpt": "Feature specification for rbac-and-tokens",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Introduce role\u2011based access control and signed tokens over the Admin API. Scope access by action and resource, enforce deny\u2011by\u2011default, and maintain an auditable trail of destructive operations. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is enterprise-grade security done right! The PASETO/JWT choice shows serious crypto consideration, and the deny-by-default with fine-grained scopes (stats:read, dlq:purge) is textbook least-privilege. The audit trail for destructive ops is chef's kiss - exactly what SOX compliance demands. Resource constraints per-queue/prefix will be killer for multi-tenant scenarios. Only concern: key rotation complexity could trip up ops teams.",
        "motivation": "- Meet production security expectations (least privilege, auditability, rotation). - Safely expose remote control to teams and automation. - Enable future multi\u2011tenant scenarios with fine\u2011grained scopes.",
        "design_deliverables": [
          "Architecture document in docs/design/rbac-and-tokens.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Token format: PASETO (v2 local/public) or JWT with HMAC; include sub, roles, scopes, exp, iat.",
          "Roles and scopes:",
          "Roles: viewer, operator, maintainer, admin.",
          "Scopes map to endpoints/actions (e.g., stats:read, enqueue:write, dlq:purge).",
          "Resource constraints: per\u2011queue or prefix, per\u2011cluster.",
          "Middleware:",
          "Verify token signature; check expiry and NBF.",
          "Authorize by scopes/role; record decision with request hash.",
          "Correlate request ID for audit link.",
          "Key management:",
          "Support multiple keys (kid) with rotation.",
          "Offline issuance tool; short\u2011lived tokens; revocation list cache.",
          "Append structured entries for destructive ops (who/what/why/result).",
          "Expose via Admin API with time range; redact sensitive values.",
          "Token info panel (who am I, roles, expiry); helpful errors for denied actions."
        ],
        "user_stories_to_address": [
          "I can issue a token with stats:read and no destructive scopes.",
          "I can perform allowed actions and see denied ones fail clearly.",
          "I can list purge/requeue actions with actor and timestamp."
        ]
      },
      "acceptance_criteria": [
        "Auth middleware validates tokens with rotation and kid.",
        "Scope checks enforced for all Admin API endpoints.",
        "Audit entries recorded and retrievable with filters.",
        "Define roles/scopes and resource patterns",
        "Implement token library + middleware",
        "Add audit log sink + API",
        "Update handlers to enforce scopes",
        "CLI issuance tool + docs",
        "string json:\"roles\" // Assigned roles",
        "string json:\"scopes\" // Explicit permissions",
        "byte json:\"pub\"",
        "byte json:\"priv,omitempty\" // Only in issuer",
        "string json:\"event_types\"",
        "string json:\"actors\"",
        "string json:\"resources\"",
        "string json:\"results\"",
        "AuditEvent, error) {",
        "interface{}{}",
        "Token, error)",
        "Token, error)",
        "string) error",
        "string) (Permission, error)",
        "string json:\"resources\""
      ]
    },
    {
      "id": "P1.T021",
      "feature_id": "F008",
      "title": "Implement Rbac And Tokens core logic",
      "description": "Build the core functionality for Rbac And Tokens",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/rbac-and-tokens/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P1.T021\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P1.T021\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P1.T021\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P1.T021\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [
          "redis_schema"
        ],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:rbac-and-tokens:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/rbac-and-tokens.md",
          "excerpt": "Feature specification for rbac-and-tokens",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Introduce role\u2011based access control and signed tokens over the Admin API. Scope access by action and resource, enforce deny\u2011by\u2011default, and maintain an auditable trail of destructive operations. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is enterprise-grade security done right! The PASETO/JWT choice shows serious crypto consideration, and the deny-by-default with fine-grained scopes (stats:read, dlq:purge) is textbook least-privilege. The audit trail for destructive ops is chef's kiss - exactly what SOX compliance demands. Resource constraints per-queue/prefix will be killer for multi-tenant scenarios. Only concern: key rotation complexity could trip up ops teams.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define roles/scopes and resource patterns",
          "Implement token library + middleware",
          "Add audit log sink + API",
          "Update handlers to enforce scopes",
          "CLI issuance tool + docs"
        ],
        "technical_approach": [
          "Token format: PASETO (v2 local/public) or JWT with HMAC; include sub, roles, scopes, exp, iat.",
          "Roles and scopes:",
          "Roles: viewer, operator, maintainer, admin.",
          "Scopes map to endpoints/actions (e.g., stats:read, enqueue:write, dlq:purge).",
          "Resource constraints: per\u2011queue or prefix, per\u2011cluster.",
          "Middleware:",
          "Verify token signature; check expiry and NBF.",
          "Authorize by scopes/role; record decision with request hash.",
          "Correlate request ID for audit link.",
          "Key management:",
          "Support multiple keys (kid) with rotation.",
          "Offline issuance tool; short\u2011lived tokens; revocation list cache.",
          "Append structured entries for destructive ops (who/what/why/result).",
          "Expose via Admin API with time range; redact sensitive values.",
          "Token info panel (who am I, roles, expiry); helpful errors for denied actions."
        ],
        "code_structure": {
          "module_path": "internal/rbac-and-tokens/",
          "main_files": [
            "rbac-and-tokens.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "rbac-and-tokens_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        },
        "auth_components": [
          "JWT token generation and validation",
          "Role definitions (admin, operator, viewer)",
          "Permission matrix",
          "Token refresh mechanism",
          "Session management",
          "API key management",
          "Audit logging for auth events"
        ]
      },
      "acceptance_criteria": [
        "Auth middleware validates tokens with rotation and kid.",
        "Scope checks enforced for all Admin API endpoints.",
        "Audit entries recorded and retrievable with filters.",
        "Define roles/scopes and resource patterns",
        "Implement token library + middleware",
        "Add audit log sink + API",
        "Update handlers to enforce scopes",
        "CLI issuance tool + docs",
        "string json:\"roles\" // Assigned roles",
        "string json:\"scopes\" // Explicit permissions",
        "byte json:\"pub\"",
        "byte json:\"priv,omitempty\" // Only in issuer",
        "string json:\"event_types\"",
        "string json:\"actors\"",
        "string json:\"resources\"",
        "string json:\"results\"",
        "AuditEvent, error) {",
        "interface{}{}",
        "Token, error)",
        "Token, error)",
        "string) error",
        "string) (Permission, error)",
        "string json:\"resources\""
      ]
    },
    {
      "id": "P1.T022",
      "feature_id": "F008",
      "title": "Test Rbac And Tokens thoroughly",
      "description": "Comprehensive testing for Rbac And Tokens",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/rbac-and-tokens/*_test.go",
            "test/e2e/rbac-and-tokens_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:rbac-and-tokens:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/rbac-and-tokens.md",
          "excerpt": "Feature specification for rbac-and-tokens",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: scope matcher; token validation; time skew; revocation."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: end\u2011to\u2011end calls with different roles and resource filters."
            ]
          },
          {
            "type": "security",
            "requirements": [
              "Security: fuzz headers; attempt scope escalation; replay detection."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/rbac-and-tokens/*_test.go",
          "integration_tests": "test/integration/rbac-and-tokens_test.go",
          "e2e_tests": "test/e2e/rbac-and-tokens_test.go",
          "benchmarks": "internal/rbac-and-tokens/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Auth middleware validates tokens with rotation and kid.",
        "Scope checks enforced for all Admin API endpoints.",
        "Audit entries recorded and retrievable with filters.",
        "Define roles/scopes and resource patterns",
        "Implement token library + middleware",
        "Add audit log sink + API",
        "Update handlers to enforce scopes",
        "CLI issuance tool + docs",
        "string json:\"roles\" // Assigned roles",
        "string json:\"scopes\" // Explicit permissions",
        "byte json:\"pub\"",
        "byte json:\"priv,omitempty\" // Only in issuer",
        "string json:\"event_types\"",
        "string json:\"actors\"",
        "string json:\"resources\"",
        "string json:\"results\"",
        "AuditEvent, error) {",
        "interface{}{}",
        "Token, error)",
        "Token, error)",
        "string) error",
        "string) (Permission, error)",
        "string json:\"resources\""
      ]
    },
    {
      "id": "P3.T023",
      "feature_id": "F009",
      "title": "Design Chaos Harness architecture",
      "description": "Create detailed technical design for Chaos Harness",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/chaos-harness.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:chaos-harness:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/chaos-harness.md",
          "excerpt": "Feature specification for chaos-harness",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Inject controlled failures (latency, drops, Redis failovers) to test resilience and visualize recovery in the TUI. Automate soak and chaos scenarios with guardrails. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Netflix's Chaos Monkey for job queues! This builds MASSIVE confidence. The visual markers on charts during chaos events is brilliant - you can literally watch the system recover. The TTL-based injections are smart - prevents forgotten chaos from destroying prod. Consider adding a \"chaos report\" that generates a beautiful PDF showing how the system handled various failure modes. Also, \"Game Day\" mode where teams compete to break each other's configs!",
        "motivation": "- Validate that retries, DLQ, and backpressure behave under stress. - Build confidence in failover paths and SLO budgets. - Catch regressions before they hit production.",
        "design_deliverables": [
          "Architecture document in docs/design/chaos-harness.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Fault injectors:",
          "Worker: delays, random failures by rate, panic/restart, partial processing.",
          "Redis: optional proxy to inject latency/drops; simulate failover (sentinel/cluster).",
          "Admin API: toggles to enable injectors with TTLs and scopes.",
          "Scenario runner:",
          "Define scenarios (duration, patterns) and run/record outcomes.",
          "Integrate with Patterned Load Generator for mixed stress.",
          "Scenario picker; live status; recovery metrics (backlog drain time, error rate).",
          "Visual markers on charts during injections.",
          "Guardrails:",
          "\u201cChaos mode\u201d banner; require typed confirmation; lock out in prod by policy."
        ],
        "user_stories_to_address": [
          "I can run a 5\u2011minute latency+drop scenario in staging and see recovery time and DLQ impact."
        ]
      },
      "acceptance_criteria": [
        "Worker injectors controllable via Admin API with scopes/TTLs.",
        "Scenario runner orchestrates injectors and records metrics.",
        "TUI surfaces status and recovers settings.",
        "Implement worker injectors + API",
        "Add Redis proxy hooks (optional)",
        "Scenario runner + metrics",
        "TUI picker + status",
        "Docs + presets"
      ]
    },
    {
      "id": "P3.T024",
      "feature_id": "F009",
      "title": "Implement Chaos Harness core logic",
      "description": "Build the core functionality for Chaos Harness",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/chaos-harness/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T024\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T024\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T024\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T024\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:chaos-harness:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/chaos-harness.md",
          "excerpt": "Feature specification for chaos-harness",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Inject controlled failures (latency, drops, Redis failovers) to test resilience and visualize recovery in the TUI. Automate soak and chaos scenarios with guardrails. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Netflix's Chaos Monkey for job queues! This builds MASSIVE confidence. The visual markers on charts during chaos events is brilliant - you can literally watch the system recover. The TTL-based injections are smart - prevents forgotten chaos from destroying prod. Consider adding a \"chaos report\" that generates a beautiful PDF showing how the system handled various failure modes. Also, \"Game Day\" mode where teams compete to break each other's configs!",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Implement worker injectors + API",
          "Add Redis proxy hooks (optional)",
          "Scenario runner + metrics",
          "TUI picker + status",
          "Docs + presets"
        ],
        "technical_approach": [
          "Fault injectors:",
          "Worker: delays, random failures by rate, panic/restart, partial processing.",
          "Redis: optional proxy to inject latency/drops; simulate failover (sentinel/cluster).",
          "Admin API: toggles to enable injectors with TTLs and scopes.",
          "Scenario runner:",
          "Define scenarios (duration, patterns) and run/record outcomes.",
          "Integrate with Patterned Load Generator for mixed stress.",
          "Scenario picker; live status; recovery metrics (backlog drain time, error rate).",
          "Visual markers on charts during injections.",
          "Guardrails:",
          "\u201cChaos mode\u201d banner; require typed confirmation; lock out in prod by policy."
        ],
        "code_structure": {
          "module_path": "internal/chaos-harness/",
          "main_files": [
            "chaos-harness.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "chaos-harness_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Worker injectors controllable via Admin API with scopes/TTLs.",
        "Scenario runner orchestrates injectors and records metrics.",
        "TUI surfaces status and recovers settings.",
        "Implement worker injectors + API",
        "Add Redis proxy hooks (optional)",
        "Scenario runner + metrics",
        "TUI picker + status",
        "Docs + presets"
      ]
    },
    {
      "id": "P3.T025",
      "feature_id": "F010",
      "title": "Design Anomaly Radar Slo Budget architecture",
      "description": "Create detailed technical design for Anomaly Radar Slo Budget",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/anomaly-radar-slo-budget.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:anomaly-radar-slo-budget:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/anomaly-radar-slo-budget.md",
          "excerpt": "Feature specification for anomaly-radar-slo-budget",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A compact widget showing backlog growth, error rate, and p95 with SLO budget and burn alerts. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > SRE candy! This is Google SRE book meets terminal aesthetics. Error budgets are how mature teams think about reliability. The burn rate calculation is key - alert on acceleration, not just threshold breaches. Consider adding a \"time until budget exhausted\" countdown for extra drama. Maybe integrate with PagerDuty when budget burns too fast?",
        "motivation": "Provide immediate health signals and guide operational action.",
        "design_deliverables": [
          "Architecture document in docs/design/anomaly-radar-slo-budget.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Compute rolling rates and percentiles with light sampling; thresholds for colorization.",
          "Configurable SLO target and window; simple burn rate calculation."
        ],
        "user_stories_to_address": [
          "I can see whether we\u2019re inside SLO and how fast we\u2019re burning budget."
        ]
      },
      "acceptance_criteria": [
        "Backlog growth, failure rate, and p95 displayed with thresholds.",
        "SLO config and budget burn shown; alert when burning too fast.",
        "Lightweight CPU/memory footprint.",
        "Implement rolling metrics",
        "Add SLO config + budget calc",
        "Integrate widget + thresholds",
        "Document usage and tuning"
      ]
    },
    {
      "id": "P3.T026",
      "feature_id": "F010",
      "title": "Implement Anomaly Radar Slo Budget core logic",
      "description": "Build the core functionality for Anomaly Radar Slo Budget",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/anomaly-radar-slo-budget/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T026\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T026\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T026\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T026\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:anomaly-radar-slo-budget:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/anomaly-radar-slo-budget.md",
          "excerpt": "Feature specification for anomaly-radar-slo-budget",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A compact widget showing backlog growth, error rate, and p95 with SLO budget and burn alerts. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > SRE candy! This is Google SRE book meets terminal aesthetics. Error budgets are how mature teams think about reliability. The burn rate calculation is key - alert on acceleration, not just threshold breaches. Consider adding a \"time until budget exhausted\" countdown for extra drama. Maybe integrate with PagerDuty when budget burns too fast?",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Implement rolling metrics",
          "Add SLO config + budget calc",
          "Integrate widget + thresholds",
          "Document usage and tuning"
        ],
        "technical_approach": [
          "Compute rolling rates and percentiles with light sampling; thresholds for colorization.",
          "Configurable SLO target and window; simple burn rate calculation."
        ],
        "code_structure": {
          "module_path": "internal/anomaly-radar-slo-budget/",
          "main_files": [
            "anomaly-radar-slo-budget.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "anomaly-radar-slo-budget_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Backlog growth, failure rate, and p95 displayed with thresholds.",
        "SLO config and budget burn shown; alert when burning too fast.",
        "Lightweight CPU/memory footprint.",
        "Implement rolling metrics",
        "Add SLO config + budget calc",
        "Integrate widget + thresholds",
        "Document usage and tuning"
      ]
    },
    {
      "id": "P3.T027",
      "feature_id": "F011",
      "title": "Design Automatic Capacity Planning architecture",
      "description": "Create detailed technical design for Automatic Capacity Planning",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/automatic-capacity-planning.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:automatic-capacity-planning:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/automatic-capacity-planning.md",
          "excerpt": "Feature specification for automatic-capacity-planning",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Predict and set recommended worker counts to meet SLOs based on historical arrival/service rates. Generate actionable plans (or auto\u2011apply via Operator) with safety bands, cool\u2011downs, and what\u2011if simulation. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is the holy grail of autoscaling - using queueing theory instead of basic CPU thresholds! The M/M/c and M/G/c models show real sophistication. The what-if simulator is genius for building operator trust. The cooldown periods will save you from the dreaded \"flapping\" that plagued early Kubernetes HPAs. One concern: forecast accuracy degrades quickly in bursty workloads. Consider adding anomaly detection to pause auto-scaling during unusual events. Also, the cost analysis integration is brilliant - showing dollar impact makes this a CFO's dream.",
        "motivation": "- Eliminate manual guesswork in scaling worker fleets. - Maintain SLOs efficiently by matching capacity to demand. - Reduce cost while avoiding backlog spikes.",
        "design_deliverables": [
          "Architecture document in docs/design/automatic-capacity-planning.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Arrival rate \u03bb(t) from enqueue metrics; service time distribution for jobs (mean, p95); current concurrency and utilization.",
          "SLO targets (p95 latency, max backlog, drain time after burst).",
          "Use queueing approximations (M/M/c or M/G/c) to estimate needed concurrency c for target SLO.",
          "Blend with recent forecasts (EWMA/Holt\u2011Winters) to project \u03bb over next 30\u2013120 minutes.",
          "Add safety margin (e.g., 10\u201320%) and apply min/max caps.",
          "Produce a time\u2011segmented plan: desired replicas per WorkerPool per window.",
          "Add cool\u2011downs and step size limits to prevent thrash; roll forward only when confidence > threshold.",
          "Application:",
          "Manual mode: show plan in TUI with \u201cApply\u201d button.",
          "Auto mode: write desired replicas via Kubernetes Operator or local scaler.",
          "Planner panel with \u201cwhat\u2011if\u201d sliders (SLO, margin) and preview overlays on charts.",
          "Show expected backlog/latency under the proposed plan."
        ],
        "user_stories_to_address": [
          "I can see and apply a capacity plan that meets a p95 latency SLO.",
          "I can enable auto\u2011apply with caps and cooldowns."
        ]
      },
      "acceptance_criteria": [
        "Planner computes recommended replicas from \u03bb and service times under a chosen SLO.",
        "Cool\u2011down and cap logic avoids oscillation.",
        "TUI previews impact and supports manual apply.",
        "Compute \u03bb and service stats by queue",
        "Implement planner + safety logic",
        "TUI planner UI + overlays",
        "Operator integration for apply",
        "Docs + runbooks",
        "ScalingStep"
      ]
    },
    {
      "id": "P3.T028",
      "feature_id": "F011",
      "title": "Implement Automatic Capacity Planning core logic",
      "description": "Build the core functionality for Automatic Capacity Planning",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/automatic-capacity-planning/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T028\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T028\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T028\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T028\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:automatic-capacity-planning:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/automatic-capacity-planning.md",
          "excerpt": "Feature specification for automatic-capacity-planning",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Predict and set recommended worker counts to meet SLOs based on historical arrival/service rates. Generate actionable plans (or auto\u2011apply via Operator) with safety bands, cool\u2011downs, and what\u2011if simulation. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is the holy grail of autoscaling - using queueing theory instead of basic CPU thresholds! The M/M/c and M/G/c models show real sophistication. The what-if simulator is genius for building operator trust. The cooldown periods will save you from the dreaded \"flapping\" that plagued early Kubernetes HPAs. One concern: forecast accuracy degrades quickly in bursty workloads. Consider adding anomaly detection to pause auto-scaling during unusual events. Also, the cost analysis integration is brilliant - showing dollar impact makes this a CFO's dream.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Compute \u03bb and service stats by queue",
          "Implement planner + safety logic",
          "TUI planner UI + overlays",
          "Operator integration for apply",
          "Docs + runbooks"
        ],
        "technical_approach": [
          "Arrival rate \u03bb(t) from enqueue metrics; service time distribution for jobs (mean, p95); current concurrency and utilization.",
          "SLO targets (p95 latency, max backlog, drain time after burst).",
          "Use queueing approximations (M/M/c or M/G/c) to estimate needed concurrency c for target SLO.",
          "Blend with recent forecasts (EWMA/Holt\u2011Winters) to project \u03bb over next 30\u2013120 minutes.",
          "Add safety margin (e.g., 10\u201320%) and apply min/max caps.",
          "Produce a time\u2011segmented plan: desired replicas per WorkerPool per window.",
          "Add cool\u2011downs and step size limits to prevent thrash; roll forward only when confidence > threshold.",
          "Application:",
          "Manual mode: show plan in TUI with \u201cApply\u201d button.",
          "Auto mode: write desired replicas via Kubernetes Operator or local scaler.",
          "Planner panel with \u201cwhat\u2011if\u201d sliders (SLO, margin) and preview overlays on charts.",
          "Show expected backlog/latency under the proposed plan."
        ],
        "code_structure": {
          "module_path": "internal/automatic-capacity-planning/",
          "main_files": [
            "automatic-capacity-planning.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "automatic-capacity-planning_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Planner computes recommended replicas from \u03bb and service times under a chosen SLO.",
        "Cool\u2011down and cap logic avoids oscillation.",
        "TUI previews impact and supports manual apply.",
        "Compute \u03bb and service stats by queue",
        "Implement planner + safety logic",
        "TUI planner UI + overlays",
        "Operator integration for apply",
        "Docs + runbooks",
        "ScalingStep"
      ]
    },
    {
      "id": "P3.T029",
      "feature_id": "F012",
      "title": "Design Kubernetes Operator architecture",
      "description": "Create detailed technical design for Kubernetes Operator",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/kubernetes-operator.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:kubernetes-operator:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/kubernetes-operator.md",
          "excerpt": "Feature specification for kubernetes-operator",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Ship a Kubernetes Operator with CRDs to declaratively manage queues and workers. Reconcile desired state (workers, rate limits, DLQ policies) from YAML, autoscale by backlog/SLA targets, and support safe rolling deploys and preemption policies. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > THIS is how you become Kubernetes-native! CRDs are the new API. Autoscaling based on queue depth is what KEDA does, but built-in? Game changer. The GitOps story writes itself. Be careful with the reconciliation loops - they can spiral. The drain hooks during rolling updates show you understand production. This unlocks the entire K8s ecosystem - ArgoCD, Flux, Helm. Consider adding a Grafana dashboard that ships with the operator!",
        "motivation": "- GitOps\u2011friendly operations for the queue stack. - Autoscale workers based on backlog growth and latency SLOs. - Consistent, reviewed changes across environments.",
        "design_deliverables": [
          "Architecture document in docs/design/kubernetes-operator.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Queue: name, priorities, rate limits, DLQ config, retention.",
          "WorkerPool: image, version, env, resources, concurrency, max in\u2011flight, drain policy, min/max replicas.",
          "Policy: global knobs (circuit breaker thresholds, retry/backoff defaults).",
          "Reconciliation:",
          "Manage Deployments/StatefulSets for workers; inject config/secret mounts.",
          "Observe metrics (backlog length, p95 latency) and scale WorkerPool via HPA\u2011like logic.",
          "Orchestrate rolling updates with drain/ready hooks via Admin API.",
          "Safety & RBAC:",
          "Namespace\u2011scoped by default; cluster\u2011scoped optional.",
          "Webhooks: CRD validation (limits, reserved names), defaulting, and drift detection.",
          "Finalizers to drain on delete; prevent orphaned DLQs.",
          "Observability:",
          "Conditions per resource; events; Prometheus metrics (reconcile durations, errors).",
          "Kustomize bases for common setups; examples repo."
        ],
        "user_stories_to_address": [
          "I can declare a WorkerPool and see it reconcile with autoscaling.",
          "I can update a Queue rate limit and see changes propagate safely."
        ]
      },
      "acceptance_criteria": [
        "CRDs with schemas and validation webhooks.",
        "Reconciler manages Deployments and scales by backlog/SLO.",
        "Rolling updates drain before restart.",
        "Define CRDs + validation webhooks",
        "Implement reconcilers (Queue, WorkerPool)",
        "Autoscaling logic (backlog/SLO)",
        "Rolling update hooks (drain/ready)",
        "Examples + CI e2e"
      ]
    },
    {
      "id": "P3.T030",
      "feature_id": "F012",
      "title": "Implement Kubernetes Operator core logic",
      "description": "Build the core functionality for Kubernetes Operator",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/kubernetes-operator/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T030\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T030\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T030\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T030\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:kubernetes-operator:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/kubernetes-operator.md",
          "excerpt": "Feature specification for kubernetes-operator",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Ship a Kubernetes Operator with CRDs to declaratively manage queues and workers. Reconcile desired state (workers, rate limits, DLQ policies) from YAML, autoscale by backlog/SLA targets, and support safe rolling deploys and preemption policies. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > THIS is how you become Kubernetes-native! CRDs are the new API. Autoscaling based on queue depth is what KEDA does, but built-in? Game changer. The GitOps story writes itself. Be careful with the reconciliation loops - they can spiral. The drain hooks during rolling updates show you understand production. This unlocks the entire K8s ecosystem - ArgoCD, Flux, Helm. Consider adding a Grafana dashboard that ships with the operator!",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define CRDs + validation webhooks",
          "Implement reconcilers (Queue, WorkerPool)",
          "Autoscaling logic (backlog/SLO)",
          "Rolling update hooks (drain/ready)",
          "Examples + CI e2e"
        ],
        "technical_approach": [
          "Queue: name, priorities, rate limits, DLQ config, retention.",
          "WorkerPool: image, version, env, resources, concurrency, max in\u2011flight, drain policy, min/max replicas.",
          "Policy: global knobs (circuit breaker thresholds, retry/backoff defaults).",
          "Reconciliation:",
          "Manage Deployments/StatefulSets for workers; inject config/secret mounts.",
          "Observe metrics (backlog length, p95 latency) and scale WorkerPool via HPA\u2011like logic.",
          "Orchestrate rolling updates with drain/ready hooks via Admin API.",
          "Safety & RBAC:",
          "Namespace\u2011scoped by default; cluster\u2011scoped optional.",
          "Webhooks: CRD validation (limits, reserved names), defaulting, and drift detection.",
          "Finalizers to drain on delete; prevent orphaned DLQs.",
          "Observability:",
          "Conditions per resource; events; Prometheus metrics (reconcile durations, errors).",
          "Kustomize bases for common setups; examples repo."
        ],
        "code_structure": {
          "module_path": "internal/kubernetes-operator/",
          "main_files": [
            "kubernetes-operator.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "kubernetes-operator_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "CRDs with schemas and validation webhooks.",
        "Reconciler manages Deployments and scales by backlog/SLO.",
        "Rolling updates drain before restart.",
        "Define CRDs + validation webhooks",
        "Implement reconcilers (Queue, WorkerPool)",
        "Autoscaling logic (backlog/SLO)",
        "Rolling update hooks (drain/ready)",
        "Examples + CI e2e"
      ]
    },
    {
      "id": "P3.T031",
      "feature_id": "F013",
      "title": "Design Canary Deployments architecture",
      "description": "Create detailed technical design for Canary Deployments",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/canary-deployments.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:canary-deployments:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/canary-deployments.md",
          "excerpt": "Feature specification for canary-deployments",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Safely roll out new worker versions by routing a configurable percentage of jobs to canaries, compare key SLOs, and promote or roll back quickly. Support sticky routing and per\u2011queue/tenant canaries. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is deployment confidence in a box! The sticky routing via job ID hash is brilliant - ensures consistent processing for multi-step workflows. The admin API controls make this feel like Kubernetes deployments but for job queues. The TUI integration with sparkline deltas and one-click promote/rollback is chef's kiss. Consider adding automatic traffic ramping (5% \u2192 15% \u2192 50% \u2192 100%) with configurable hold periods.",
        "motivation": "- Reduce risk of deploys by validating in production with guardrails. - Catch regressions in latency/failures early and roll back rapidly. - Enable progressive delivery culture around workers.",
        "design_deliverables": [
          "Architecture document in docs/design/canary-deployments.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Versioning & labels:",
          "Workers advertise version and lane (stable/canary) via heartbeat/registration.",
          "Routing strategies:",
          "Split queues: enqueue sampler routes N% to queue@canary (separate key); canary workers only consume @canary.",
          "Stream groups (if using Streams): distinct consumer group per version with weighted claim.",
          "Hash\u2011based sticky: route by job ID hash for consistency during a canary window.",
          "Control plane:",
          "Admin API to configure percentages per queue/tenant and window duration.",
          "Automatic promotion on green metrics or manual confirmation.",
          "Fast rollback sets canary to 0% and drains remaining.",
          "Observability:",
          "Compare p50/p95 latency, error/DLQ rates, resource usage per version.",
          "TUI panel with sparkline deltas and a one\u2011click promote/rollback (with confirm).",
          "Guard critical queues with max canary %; block incompatible schema changes without a feature flag."
        ],
        "user_stories_to_address": [
          "I can set a 10% canary for payments and watch metrics before promoting.",
          "I can roll back immediately with a single action when errors spike."
        ]
      },
      "acceptance_criteria": [
        "Version\u2011aware routing with configurable percentages.",
        "Side\u2011by\u2011side SLO metrics with alerts on regression.",
        "Promote/rollback flows with confirmations and drain behavior.",
        "Worker version/label plumbing",
        "Routing implementation (split queues or groups)",
        "Admin API for percentages + windows",
        "TUI canary panel + controls",
        "Docs + runbooks",
        "byte(job.ID))",
        "PromotionRule",
        "PromotionRule",
        "WorkerInfo {",
        "WorkerInfo",
        "string{\"queue\", \"status\"},",
        "string{\"queue\", \"deployment_id\"},",
        "string{\"queue\", \"outcome\"},",
        "string{\"queue\", \"reason\"},"
      ]
    },
    {
      "id": "P3.T032",
      "feature_id": "F013",
      "title": "Implement Canary Deployments core logic",
      "description": "Build the core functionality for Canary Deployments",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/canary-deployments/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T032\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T032\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T032\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T032\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:canary-deployments:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/canary-deployments.md",
          "excerpt": "Feature specification for canary-deployments",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Safely roll out new worker versions by routing a configurable percentage of jobs to canaries, compare key SLOs, and promote or roll back quickly. Support sticky routing and per\u2011queue/tenant canaries. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is deployment confidence in a box! The sticky routing via job ID hash is brilliant - ensures consistent processing for multi-step workflows. The admin API controls make this feel like Kubernetes deployments but for job queues. The TUI integration with sparkline deltas and one-click promote/rollback is chef's kiss. Consider adding automatic traffic ramping (5% \u2192 15% \u2192 50% \u2192 100%) with configurable hold periods.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Worker version/label plumbing",
          "Routing implementation (split queues or groups)",
          "Admin API for percentages + windows",
          "TUI canary panel + controls",
          "Docs + runbooks"
        ],
        "technical_approach": [
          "Versioning & labels:",
          "Workers advertise version and lane (stable/canary) via heartbeat/registration.",
          "Routing strategies:",
          "Split queues: enqueue sampler routes N% to queue@canary (separate key); canary workers only consume @canary.",
          "Stream groups (if using Streams): distinct consumer group per version with weighted claim.",
          "Hash\u2011based sticky: route by job ID hash for consistency during a canary window.",
          "Control plane:",
          "Admin API to configure percentages per queue/tenant and window duration.",
          "Automatic promotion on green metrics or manual confirmation.",
          "Fast rollback sets canary to 0% and drains remaining.",
          "Observability:",
          "Compare p50/p95 latency, error/DLQ rates, resource usage per version.",
          "TUI panel with sparkline deltas and a one\u2011click promote/rollback (with confirm).",
          "Guard critical queues with max canary %; block incompatible schema changes without a feature flag."
        ],
        "code_structure": {
          "module_path": "internal/canary-deployments/",
          "main_files": [
            "canary-deployments.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "canary-deployments_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Version\u2011aware routing with configurable percentages.",
        "Side\u2011by\u2011side SLO metrics with alerts on regression.",
        "Promote/rollback flows with confirmations and drain behavior.",
        "Worker version/label plumbing",
        "Routing implementation (split queues or groups)",
        "Admin API for percentages + windows",
        "TUI canary panel + controls",
        "Docs + runbooks",
        "byte(job.ID))",
        "PromotionRule",
        "PromotionRule",
        "WorkerInfo {",
        "WorkerInfo",
        "string{\"queue\", \"status\"},",
        "string{\"queue\", \"deployment_id\"},",
        "string{\"queue\", \"outcome\"},",
        "string{\"queue\", \"reason\"},"
      ]
    },
    {
      "id": "P1.T033",
      "feature_id": "F014",
      "title": "Design Event Hooks architecture",
      "description": "Create detailed technical design for Event Hooks",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/event-hooks.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:event-hooks:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/event-hooks.md",
          "excerpt": "Feature specification for event-hooks",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Emit events for job lifecycle changes (enqueued, started, succeeded, failed, DLQ) to external systems via webhooks or NATS. Signed payloads, backoff retries, and per\u2011queue filters provide safe, flexible integrations. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is the missing link between your queue and the rest of your infrastructure! Webhooks with HMAC signing and exponential backoff? Chef's kiss. The DLH (Dead Letter Hooks) concept for failed deliveries is brilliant - no more lost notifications. Event filtering by queue and priority means teams can subscribe to exactly what they care about. This turns your queue from an isolated component into the nervous system of your entire platform.",
        "motivation": "- Automate workflows: notify services, trigger pipelines, or update dashboards on state changes. - Reduce polling and bespoke glue code. - Provide auditability and a supported escape hatch for extensions.",
        "design_deliverables": [
          "Architecture document in docs/design/event-hooks.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Internal dispatcher with typed events and subscriber backends.",
          "Events: job_enqueued, job_started, job_succeeded, job_failed, job_dlq, job_retried.",
          "Transports:",
          "Webhooks: POST JSON with HMAC signature, per\u2011subscription secret.",
          "NATS (optional): publish to subjects by event type and queue.",
          "Delivery semantics:",
          "At\u2011least\u2011once with exponential backoff; max attempts; dead\u2011letter for hooks.",
          "Idempotency key in headers; timestamp; correlation/trace IDs when available.",
          "Subscriptions: matchers (queue, priority, result), endpoint/subject, headers, secret, rate cap.",
          "Health: last delivery status, retry counts, moving window success rate.",
          "CRUD subscriptions, test delivery, list DLH (dead\u2011letter hooks) with replay.",
          "Minimal management panel: list subscriptions and status; trigger test; replay failed.",
          "Security: HMAC signing; optional mTLS; redact sensitive payload fields by pattern."
        ],
        "user_stories_to_address": [
          "I can subscribe a webhook to job_failed for queue payments with a secret and receive signed payloads.",
          "I can see delivery success rate and replay failures."
        ]
      },
      "acceptance_criteria": [
        "Subscriptions with filters and secrets.",
        "Delivery with retries, backoff, and DLH.",
        "Test and replay supported via Admin API and TUI.",
        "Define event schema + bus",
        "Implement webhook + NATS transports",
        "Retry/backoff + DLH storage",
        "Admin API CRUD + test/replay",
        "TUI management panel",
        "Docs + samples",
        "string json:\"events\" // Event type filter",
        "string json:\"queues\" // Queue name filter ( for all)",
        "string json:\"payload_fields,omitempty\" // Whitelist fields",
        "string json:\"redact_fields,omitempty\" // PII protection",
        "byte, secret string) string {",
        "byte(secret))",
        "byte, signature, secret string) bool {",
        "byte(signature), byte(expected))"
      ]
    },
    {
      "id": "P1.T034",
      "feature_id": "F014",
      "title": "Implement Event Hooks core logic",
      "description": "Build the core functionality for Event Hooks",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/event-hooks/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P1.T034\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P1.T034\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P1.T034\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P1.T034\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [
          "redis_schema"
        ],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:event-hooks:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/event-hooks.md",
          "excerpt": "Feature specification for event-hooks",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Emit events for job lifecycle changes (enqueued, started, succeeded, failed, DLQ) to external systems via webhooks or NATS. Signed payloads, backoff retries, and per\u2011queue filters provide safe, flexible integrations. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is the missing link between your queue and the rest of your infrastructure! Webhooks with HMAC signing and exponential backoff? Chef's kiss. The DLH (Dead Letter Hooks) concept for failed deliveries is brilliant - no more lost notifications. Event filtering by queue and priority means teams can subscribe to exactly what they care about. This turns your queue from an isolated component into the nervous system of your entire platform.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define event schema + bus",
          "Implement webhook + NATS transports",
          "Retry/backoff + DLH storage",
          "Admin API CRUD + test/replay",
          "TUI management panel",
          "Docs + samples"
        ],
        "technical_approach": [
          "Internal dispatcher with typed events and subscriber backends.",
          "Events: job_enqueued, job_started, job_succeeded, job_failed, job_dlq, job_retried.",
          "Transports:",
          "Webhooks: POST JSON with HMAC signature, per\u2011subscription secret.",
          "NATS (optional): publish to subjects by event type and queue.",
          "Delivery semantics:",
          "At\u2011least\u2011once with exponential backoff; max attempts; dead\u2011letter for hooks.",
          "Idempotency key in headers; timestamp; correlation/trace IDs when available.",
          "Subscriptions: matchers (queue, priority, result), endpoint/subject, headers, secret, rate cap.",
          "Health: last delivery status, retry counts, moving window success rate.",
          "CRUD subscriptions, test delivery, list DLH (dead\u2011letter hooks) with replay.",
          "Minimal management panel: list subscriptions and status; trigger test; replay failed.",
          "Security: HMAC signing; optional mTLS; redact sensitive payload fields by pattern."
        ],
        "code_structure": {
          "module_path": "internal/event-hooks/",
          "main_files": [
            "event-hooks.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "event-hooks_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Subscriptions with filters and secrets.",
        "Delivery with retries, backoff, and DLH.",
        "Test and replay supported via Admin API and TUI.",
        "Define event schema + bus",
        "Implement webhook + NATS transports",
        "Retry/backoff + DLH storage",
        "Admin API CRUD + test/replay",
        "TUI management panel",
        "Docs + samples",
        "string json:\"events\" // Event type filter",
        "string json:\"queues\" // Queue name filter ( for all)",
        "string json:\"payload_fields,omitempty\" // Whitelist fields",
        "string json:\"redact_fields,omitempty\" // PII protection",
        "byte, secret string) string {",
        "byte(secret))",
        "byte, signature, secret string) bool {",
        "byte(signature), byte(expected))"
      ]
    },
    {
      "id": "P1.T035",
      "feature_id": "F014",
      "title": "Test Event Hooks thoroughly",
      "description": "Comprehensive testing for Event Hooks",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/event-hooks/*_test.go",
            "test/e2e/event-hooks_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:event-hooks:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/event-hooks.md",
          "excerpt": "Feature specification for event-hooks",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: signature generation/verification; backoff schedule; filter matching."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: webhook endpoint harness; NATS subject contract; DLH replay."
            ]
          },
          {
            "type": "security",
            "requirements": [
              "Security: signature tamper tests; redaction validation."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/event-hooks/*_test.go",
          "integration_tests": "test/integration/event-hooks_test.go",
          "e2e_tests": "test/e2e/event-hooks_test.go",
          "benchmarks": "internal/event-hooks/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Subscriptions with filters and secrets.",
        "Delivery with retries, backoff, and DLH.",
        "Test and replay supported via Admin API and TUI.",
        "Define event schema + bus",
        "Implement webhook + NATS transports",
        "Retry/backoff + DLH storage",
        "Admin API CRUD + test/replay",
        "TUI management panel",
        "Docs + samples",
        "string json:\"events\" // Event type filter",
        "string json:\"queues\" // Queue name filter ( for all)",
        "string json:\"payload_fields,omitempty\" // Whitelist fields",
        "string json:\"redact_fields,omitempty\" // PII protection",
        "byte, secret string) string {",
        "byte(secret))",
        "byte, signature, secret string) bool {",
        "byte(signature), byte(expected))"
      ]
    },
    {
      "id": "P2.T036",
      "feature_id": "F015",
      "title": "Design Smart Payload Deduplication architecture",
      "description": "Create detailed technical design for Smart Payload Deduplication",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/smart-payload-deduplication.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:smart-payload-deduplication:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/smart-payload-deduplication.md",
          "excerpt": "Feature specification for smart-payload-deduplication",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Intelligent deduplication system that detects similar job payloads and stores them once, dramatically reducing Redis memory usage. Uses content-addressable storage with smart chunking to find commonalities even in partially different payloads. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is brilliant infrastructure engineering that tackles one of the most expensive parts of queue systems - payload storage! The rolling hash chunking approach is sophisticated, and the 50-90% memory reduction claims are realistic for repetitive workloads. I love the transparent integration - developers don't need to change anything. The reference counting GC is essential for safety. Consider adding similarity-based grouping for near-duplicates and maybe a bloom filter for fast duplicate detection.",
        "motivation": "- Reduce Redis memory usage by 50-90% for repetitive workloads - Enable longer job history retention within same memory budget - Detect and consolidate duplicate work before processing - Lower infrastructure costs for high-volume queues",
        "design_deliverables": [
          "Architecture document in docs/design/smart-payload-deduplication.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Deduplication Strategy:",
          "Content-based chunking using rolling hash (Rabin fingerprinting)",
          "Store chunks in content-addressable store (CAS)",
          "Job payloads become lists of chunk references",
          "Reference counting for garbage collection",
          "Smart Detection:",
          "Fuzzy matching for near-duplicates (similarity threshold)",
          "Template detection: identify common structures with variable parts",
          "Compression: zstd with dictionary learned from payload corpus",
          "Storage Architecture:",
          "Chunk store: dedup:chunk:{hash} \u2192 compressed data",
          "Reference index: dedup:refs:{job_id} \u2192 chunk list",
          "Stats tracking: dedup ratio, savings, popular chunks",
          "Integration Points:",
          "Producer: dedupe at enqueue time",
          "Worker: reconstruct at dequeue",
          "Admin: dedup stats and tuning",
          "Transparent to application code",
          "Verify chunk integrity with checksums",
          "Atomic reference counting",
          "Gradual rollout with feature flags",
          "Fallback to non-deduped storage"
        ],
        "user_stories_to_address": [
          "I can see 70% memory reduction for repetitive job workloads",
          "deduplication is completely transparent to my job processing code",
          "I can monitor dedup effectiveness and tune parameters"
        ]
      },
      "acceptance_criteria": [
        ">50% memory savings on typical workloads",
        "<10ms overhead for dedup/reconstruction",
        "Zero data loss with checksums and verification",
        "Automatic garbage collection of unreferenced chunks",
        "Implement content-based chunking",
        "Build chunk store with CAS",
        "Add reference counting and GC",
        "Integrate with producer/worker",
        "Create admin stats dashboard",
        "Performance tuning",
        "Migration tooling",
        "byte) Chunk {",
        "byte) (byte, error) {",
        "byte) (byte, error) {",
        "byte) string {",
        "byte) error {"
      ]
    },
    {
      "id": "P2.T037",
      "feature_id": "F015",
      "title": "Implement Smart Payload Deduplication core logic",
      "description": "Build the core functionality for Smart Payload Deduplication",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/smart-payload-deduplication/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T037\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T037\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T037\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T037\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:smart-payload-deduplication:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/smart-payload-deduplication.md",
          "excerpt": "Feature specification for smart-payload-deduplication",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Intelligent deduplication system that detects similar job payloads and stores them once, dramatically reducing Redis memory usage. Uses content-addressable storage with smart chunking to find commonalities even in partially different payloads. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is brilliant infrastructure engineering that tackles one of the most expensive parts of queue systems - payload storage! The rolling hash chunking approach is sophisticated, and the 50-90% memory reduction claims are realistic for repetitive workloads. I love the transparent integration - developers don't need to change anything. The reference counting GC is essential for safety. Consider adding similarity-based grouping for near-duplicates and maybe a bloom filter for fast duplicate detection.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Implement content-based chunking",
          "Build chunk store with CAS",
          "Add reference counting and GC",
          "Integrate with producer/worker",
          "Create admin stats dashboard",
          "Performance tuning",
          "Migration tooling"
        ],
        "technical_approach": [
          "Deduplication Strategy:",
          "Content-based chunking using rolling hash (Rabin fingerprinting)",
          "Store chunks in content-addressable store (CAS)",
          "Job payloads become lists of chunk references",
          "Reference counting for garbage collection",
          "Smart Detection:",
          "Fuzzy matching for near-duplicates (similarity threshold)",
          "Template detection: identify common structures with variable parts",
          "Compression: zstd with dictionary learned from payload corpus",
          "Storage Architecture:",
          "Chunk store: dedup:chunk:{hash} \u2192 compressed data",
          "Reference index: dedup:refs:{job_id} \u2192 chunk list",
          "Stats tracking: dedup ratio, savings, popular chunks",
          "Integration Points:",
          "Producer: dedupe at enqueue time",
          "Worker: reconstruct at dequeue",
          "Admin: dedup stats and tuning",
          "Transparent to application code",
          "Verify chunk integrity with checksums",
          "Atomic reference counting",
          "Gradual rollout with feature flags",
          "Fallback to non-deduped storage"
        ],
        "code_structure": {
          "module_path": "internal/smart-payload-deduplication/",
          "main_files": [
            "smart-payload-deduplication.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "smart-payload-deduplication_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        ">50% memory savings on typical workloads",
        "<10ms overhead for dedup/reconstruction",
        "Zero data loss with checksums and verification",
        "Automatic garbage collection of unreferenced chunks",
        "Implement content-based chunking",
        "Build chunk store with CAS",
        "Add reference counting and GC",
        "Integrate with producer/worker",
        "Create admin stats dashboard",
        "Performance tuning",
        "Migration tooling",
        "byte) Chunk {",
        "byte) (byte, error) {",
        "byte) (byte, error) {",
        "byte) string {",
        "byte) error {"
      ]
    },
    {
      "id": "P2.T038",
      "feature_id": "F015",
      "title": "Test Smart Payload Deduplication thoroughly",
      "description": "Comprehensive testing for Smart Payload Deduplication",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/smart-payload-deduplication/*_test.go",
            "test/e2e/smart-payload-deduplication_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:smart-payload-deduplication:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/smart-payload-deduplication.md",
          "excerpt": "Feature specification for smart-payload-deduplication",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: Chunking algorithms, hash distribution, reference counting"
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: End-to-end dedup with various payload patterns",
              "Stress: High concurrency, chunk store corruption recovery"
            ]
          },
          {
            "type": "performance",
            "requirements": [
              "Performance: Benchmark overhead, measure actual memory savings"
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/smart-payload-deduplication/*_test.go",
          "integration_tests": "test/integration/smart-payload-deduplication_test.go",
          "e2e_tests": "test/e2e/smart-payload-deduplication_test.go",
          "benchmarks": "internal/smart-payload-deduplication/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        ">50% memory savings on typical workloads",
        "<10ms overhead for dedup/reconstruction",
        "Zero data loss with checksums and verification",
        "Automatic garbage collection of unreferenced chunks",
        "Implement content-based chunking",
        "Build chunk store with CAS",
        "Add reference counting and GC",
        "Integrate with producer/worker",
        "Create admin stats dashboard",
        "Performance tuning",
        "Migration tooling",
        "byte) Chunk {",
        "byte) (byte, error) {",
        "byte) (byte, error) {",
        "byte) string {",
        "byte) error {"
      ]
    },
    {
      "id": "P2.T039",
      "feature_id": "F016",
      "title": "Design Job Budgeting architecture",
      "description": "Create detailed technical design for Job Budgeting",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/job-budgeting.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:job-budgeting:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/job-budgeting.md",
          "excerpt": "Feature specification for job-budgeting",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Track and allocate cost per job/tenant using measured resource usage and a simple cost model. Define monthly budgets, show spend vs. budget, forecast burn, and apply optional enforcement (soft warnings \u2192 throttle) with clear governance. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is FinOps for job queues! The enforcement ladder is brilliant - warn before you throttle, throttle before you block. The cost model with CPU time, memory proxy, and payload size gives real insight into job efficiency. Imagine seeing that one tenant's ML jobs are burning through budget while another's lightweight tasks cost pennies. The integration with rate limiting turns budget into a dynamic throttling input - when you're over budget, your jobs slow down automatically!",
        "motivation": "- Bring financial accountability to shared queues. - Prevent runaway costs from poorly behaving producers. - Enable chargeback/showback and informed prioritization.",
        "design_deliverables": [
          "Architecture document in docs/design/job-budgeting.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Cost model:",
          "Components: processing time (CPU), memory footprint proxy, payload size, Redis ops, egress hints.",
          "Assign weights/cost rates; calibrate with benchmarks and environment multipliers.",
          "Data capture:",
          "Worker reports per\u2011job timings and optional resource proxies; producer reports payload size.",
          "Aggregate by tenant/queue/day; store in a compact table (ClickHouse optional).",
          "Budgets & policies:",
          "Define budgets per tenant/queue; thresholds for warning/limit.",
          "Enforcement ladder: warn producers \u2192 suggest backpressure \u2192 throttle low\u2011priority work; never drop without explicit policy.",
          "Integration:",
          "Advanced Rate Limiting uses remaining budget as an input for weights.",
          "Event Hooks for budget threshold notifications.",
          "Budgets panel: current spend, forecast to month\u2011end, top cost drivers, and recent spikes; drill into job samples."
        ],
        "user_stories_to_address": [
          "I can set budgets and see spend by tenant and queue.",
          "I receive early warnings and suggestions if I'm overspending."
        ]
      },
      "acceptance_criteria": [
        "Cost calculation with pluggable weights; daily aggregates.",
        "Budgets with warning/enforcement thresholds and notifications.",
        "TUI shows spend, forecast, and top drivers.",
        "Define cost model + calibration suite",
        "Implement capture and aggregation",
        "Budgets + enforcement hooks",
        "TUI budgets panel + drilldown",
        "Docs + operator guidance",
        "NotificationChannel json:\"notifications\""
      ]
    },
    {
      "id": "P2.T040",
      "feature_id": "F016",
      "title": "Implement Job Budgeting core logic",
      "description": "Build the core functionality for Job Budgeting",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/job-budgeting/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T040\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T040\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T040\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T040\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:job-budgeting:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/job-budgeting.md",
          "excerpt": "Feature specification for job-budgeting",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Track and allocate cost per job/tenant using measured resource usage and a simple cost model. Define monthly budgets, show spend vs. budget, forecast burn, and apply optional enforcement (soft warnings \u2192 throttle) with clear governance. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is FinOps for job queues! The enforcement ladder is brilliant - warn before you throttle, throttle before you block. The cost model with CPU time, memory proxy, and payload size gives real insight into job efficiency. Imagine seeing that one tenant's ML jobs are burning through budget while another's lightweight tasks cost pennies. The integration with rate limiting turns budget into a dynamic throttling input - when you're over budget, your jobs slow down automatically!",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define cost model + calibration suite",
          "Implement capture and aggregation",
          "Budgets + enforcement hooks",
          "TUI budgets panel + drilldown",
          "Docs + operator guidance"
        ],
        "technical_approach": [
          "Cost model:",
          "Components: processing time (CPU), memory footprint proxy, payload size, Redis ops, egress hints.",
          "Assign weights/cost rates; calibrate with benchmarks and environment multipliers.",
          "Data capture:",
          "Worker reports per\u2011job timings and optional resource proxies; producer reports payload size.",
          "Aggregate by tenant/queue/day; store in a compact table (ClickHouse optional).",
          "Budgets & policies:",
          "Define budgets per tenant/queue; thresholds for warning/limit.",
          "Enforcement ladder: warn producers \u2192 suggest backpressure \u2192 throttle low\u2011priority work; never drop without explicit policy.",
          "Integration:",
          "Advanced Rate Limiting uses remaining budget as an input for weights.",
          "Event Hooks for budget threshold notifications.",
          "Budgets panel: current spend, forecast to month\u2011end, top cost drivers, and recent spikes; drill into job samples."
        ],
        "code_structure": {
          "module_path": "internal/job-budgeting/",
          "main_files": [
            "job-budgeting.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "job-budgeting_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Cost calculation with pluggable weights; daily aggregates.",
        "Budgets with warning/enforcement thresholds and notifications.",
        "TUI shows spend, forecast, and top drivers.",
        "Define cost model + calibration suite",
        "Implement capture and aggregation",
        "Budgets + enforcement hooks",
        "TUI budgets panel + drilldown",
        "Docs + operator guidance",
        "NotificationChannel json:\"notifications\""
      ]
    },
    {
      "id": "P2.T041",
      "feature_id": "F016",
      "title": "Test Job Budgeting thoroughly",
      "description": "Comprehensive testing for Job Budgeting",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/job-budgeting/*_test.go",
            "test/e2e/job-budgeting_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:job-budgeting:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/job-budgeting.md",
          "excerpt": "Feature specification for job-budgeting",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: cost calculation; aggregation correctness; forecasting."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: end\u2011to\u2011end with sample tenants; enforcement ladder behavior.",
              "Data quality: reconcile sampled job costs vs. totals within tolerance."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/job-budgeting/*_test.go",
          "integration_tests": "test/integration/job-budgeting_test.go",
          "e2e_tests": "test/e2e/job-budgeting_test.go",
          "benchmarks": "internal/job-budgeting/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Cost calculation with pluggable weights; daily aggregates.",
        "Budgets with warning/enforcement thresholds and notifications.",
        "TUI shows spend, forecast, and top drivers.",
        "Define cost model + calibration suite",
        "Implement capture and aggregation",
        "Budgets + enforcement hooks",
        "TUI budgets panel + drilldown",
        "Docs + operator guidance",
        "NotificationChannel json:\"notifications\""
      ]
    },
    {
      "id": "P3.T042",
      "feature_id": "F017",
      "title": "Design Job Genealogy Navigator architecture",
      "description": "Create detailed technical design for Job Genealogy Navigator",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/job-genealogy-navigator.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:job-genealogy-navigator:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/job-genealogy-navigator.md",
          "excerpt": "Feature specification for job-genealogy-navigator",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Interactive visualization of job family trees showing parent-child relationships, retry chains, spawned subjobs, and failure cascades. Navigate complex job genealogies in the TUI to understand causality and impact. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is brilliant debugging visualization! Job genealogies are exactly what's missing from most queue systems. The ASCII art tree with collapsible nodes is genius - imagine expanding a retry chain to see the exact failure progression. The \"blame mode\" and \"impact mode\" are killer features for root cause analysis. Consider adding edge weighting to show job size or processing time. Also, the time-lapse animation could reveal bottlenecks that aren't obvious in static views.",
        "motivation": "- Understand cascade effects when parent jobs fail - Trace root causes through retry and respawn chains - Visualize workflow dependencies without external tools - Debug complex multi-job transactions",
        "design_deliverables": [
          "Architecture document in docs/design/job-genealogy-navigator.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Relationship Tracking:",
          "Capture parent_id, spawn_reason, relationship_type in job metadata",
          "Relationship types: retry, spawn, fork, callback, compensation",
          "Bidirectional indexes: parent\u2192children, child\u2192ancestors",
          "Store in Redis sorted sets for efficient traversal",
          "Graph Operations:",
          "Build family tree from any job (ancestors + descendants)",
          "Find common ancestors between failed jobs",
          "Calculate impact radius (all affected descendants)",
          "Detect cycles and anomalies",
          "TUI Visualization:",
          "ASCII art tree with box-drawing characters",
          "Collapsible/expandable nodes",
          "Color coding: green (success), red (failed), yellow (processing), gray (pending)",
          "Navigation: arrow keys to traverse, enter to inspect, 'f' to focus subtree",
          "Multiple layout algorithms: top-down tree, left-right timeline, radial",
          "Analysis Features:",
          "\"Blame\" mode: trace failure to root cause",
          "\"Impact\" mode: show all affected descendants",
          "Pattern detection: identify recurring failure genealogies",
          "Time-lapse: animate tree growth over time",
          "Integration:",
          "Auto-capture relationships during enqueue/retry/spawn",
          "Link with Time Travel Debugger for replay",
          "Export as DOT/Mermaid for external visualization"
        ],
        "user_stories_to_address": [
          "I can see the complete family tree of any job",
          "I can trace cascading failures to their root cause",
          "I can identify patterns in job relationships over time"
        ]
      },
      "acceptance_criteria": [
        "Complete genealogy captured automatically",
        "Tree renders smoothly for 100+ node families",
        "Navigation responsive with <50ms updates",
        "Relationship data automatically pruned after TTL",
        "Design relationship schema",
        "Implement relationship capture",
        "Build graph traversal algorithms",
        "Create ASCII tree renderer",
        "Add navigation controls",
        "Implement analysis modes",
        "Performance optimization",
        "string json:\"ancestors\"",
        "string json:\"descendants\"",
        "JobRelationship json:\"relationships\"",
        "string json:\"generation_map\""
      ]
    },
    {
      "id": "P3.T043",
      "feature_id": "F017",
      "title": "Implement Job Genealogy Navigator core logic",
      "description": "Build the core functionality for Job Genealogy Navigator",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/job-genealogy-navigator/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T043\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T043\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T043\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T043\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:job-genealogy-navigator:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/job-genealogy-navigator.md",
          "excerpt": "Feature specification for job-genealogy-navigator",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Interactive visualization of job family trees showing parent-child relationships, retry chains, spawned subjobs, and failure cascades. Navigate complex job genealogies in the TUI to understand causality and impact. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is brilliant debugging visualization! Job genealogies are exactly what's missing from most queue systems. The ASCII art tree with collapsible nodes is genius - imagine expanding a retry chain to see the exact failure progression. The \"blame mode\" and \"impact mode\" are killer features for root cause analysis. Consider adding edge weighting to show job size or processing time. Also, the time-lapse animation could reveal bottlenecks that aren't obvious in static views.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Design relationship schema",
          "Implement relationship capture",
          "Build graph traversal algorithms",
          "Create ASCII tree renderer",
          "Add navigation controls",
          "Implement analysis modes",
          "Performance optimization"
        ],
        "technical_approach": [
          "Relationship Tracking:",
          "Capture parent_id, spawn_reason, relationship_type in job metadata",
          "Relationship types: retry, spawn, fork, callback, compensation",
          "Bidirectional indexes: parent\u2192children, child\u2192ancestors",
          "Store in Redis sorted sets for efficient traversal",
          "Graph Operations:",
          "Build family tree from any job (ancestors + descendants)",
          "Find common ancestors between failed jobs",
          "Calculate impact radius (all affected descendants)",
          "Detect cycles and anomalies",
          "TUI Visualization:",
          "ASCII art tree with box-drawing characters",
          "Collapsible/expandable nodes",
          "Color coding: green (success), red (failed), yellow (processing), gray (pending)",
          "Navigation: arrow keys to traverse, enter to inspect, 'f' to focus subtree",
          "Multiple layout algorithms: top-down tree, left-right timeline, radial",
          "Analysis Features:",
          "\"Blame\" mode: trace failure to root cause",
          "\"Impact\" mode: show all affected descendants",
          "Pattern detection: identify recurring failure genealogies",
          "Time-lapse: animate tree growth over time",
          "Integration:",
          "Auto-capture relationships during enqueue/retry/spawn",
          "Link with Time Travel Debugger for replay",
          "Export as DOT/Mermaid for external visualization"
        ],
        "code_structure": {
          "module_path": "internal/job-genealogy-navigator/",
          "main_files": [
            "job-genealogy-navigator.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "job-genealogy-navigator_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Complete genealogy captured automatically",
        "Tree renders smoothly for 100+ node families",
        "Navigation responsive with <50ms updates",
        "Relationship data automatically pruned after TTL",
        "Design relationship schema",
        "Implement relationship capture",
        "Build graph traversal algorithms",
        "Create ASCII tree renderer",
        "Add navigation controls",
        "Implement analysis modes",
        "Performance optimization",
        "string json:\"ancestors\"",
        "string json:\"descendants\"",
        "JobRelationship json:\"relationships\"",
        "string json:\"generation_map\""
      ]
    },
    {
      "id": "P2.T044",
      "feature_id": "F018",
      "title": "Design Long Term Archives architecture",
      "description": "Create detailed technical design for Long Term Archives",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/long-term-archives.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:long-term-archives:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/long-term-archives.md",
          "excerpt": "Feature specification for long-term-archives",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Stream completed jobs and key metadata to long\u2011term storage (ClickHouse or S3/Parquet) with retention controls. Provide fast querying for forensics and reporting while keeping Redis slim. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is brilliant data engineering! The two-tier storage approach (Redis for hot data, ClickHouse for analytics) is exactly how modern systems handle massive scale. The schema evolution support is critical - you can't just dump JSON and hope for the best. The sampling feature prevents storage explosion while maintaining statistical validity. Plus, GDPR compliance hooks show they've thought through the privacy implications. This transforms a queue system into a proper data platform.",
        "motivation": "- Investigate historical incidents and trends without bloating Redis. - Power analytics and reporting pipelines. - Meet compliance for retention or deletion.",
        "design_deliverables": [
          "Architecture document in docs/design/long-term-archives.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Export path:",
          "Producer: append to a Completed stream (list/stream) on success/failure; optional sampling.",
          "Exporter service: batch reads and writes to ClickHouse (insert) or S3 (Parquet files) with schema evolution.",
          "Core: job_id, queue, priority, timestamps (enqueue/start/end), outcome, retries, worker_id, size, trace_id.",
          "Payload handling: optional redacted payload snapshot or hashes.",
          "TTL in Redis; archive window controls; GDPR delete hooks to remove payload snapshots.",
          "Link to sample queries and recent export status; not a full query UI.",
          "Observability:",
          "Lag metrics, batch sizes, write errors; alerts when exporters fall behind."
        ],
        "user_stories_to_address": [
          "I can query last 90 days of completed jobs by queue and outcome.",
          "I can verify Redis stays within retention while exports keep up."
        ]
      },
      "acceptance_criteria": [
        "Exporter writes to ClickHouse/S3 with retries and idempotency.",
        "Schema versioning with backward compatibility.",
        "Configurable retention in Redis and archive.",
        "Define archive schema + sampling",
        "Build exporter (ClickHouse + S3/Parquet)",
        "Add retention knobs + delete hooks",
        "TUI export status panel",
        "Docs + sample queries",
        "ArchiveDestination",
        "JobRecord, 0, s.config.BatchSize)",
        "Migration json:\"migrations\"",
        "string json:\"deprecated\""
      ]
    },
    {
      "id": "P2.T045",
      "feature_id": "F018",
      "title": "Implement Long Term Archives core logic",
      "description": "Build the core functionality for Long Term Archives",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/long-term-archives/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T045\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T045\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T045\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T045\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:long-term-archives:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/long-term-archives.md",
          "excerpt": "Feature specification for long-term-archives",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Stream completed jobs and key metadata to long\u2011term storage (ClickHouse or S3/Parquet) with retention controls. Provide fast querying for forensics and reporting while keeping Redis slim. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is brilliant data engineering! The two-tier storage approach (Redis for hot data, ClickHouse for analytics) is exactly how modern systems handle massive scale. The schema evolution support is critical - you can't just dump JSON and hope for the best. The sampling feature prevents storage explosion while maintaining statistical validity. Plus, GDPR compliance hooks show they've thought through the privacy implications. This transforms a queue system into a proper data platform.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define archive schema + sampling",
          "Build exporter (ClickHouse + S3/Parquet)",
          "Add retention knobs + delete hooks",
          "TUI export status panel",
          "Docs + sample queries"
        ],
        "technical_approach": [
          "Export path:",
          "Producer: append to a Completed stream (list/stream) on success/failure; optional sampling.",
          "Exporter service: batch reads and writes to ClickHouse (insert) or S3 (Parquet files) with schema evolution.",
          "Core: job_id, queue, priority, timestamps (enqueue/start/end), outcome, retries, worker_id, size, trace_id.",
          "Payload handling: optional redacted payload snapshot or hashes.",
          "TTL in Redis; archive window controls; GDPR delete hooks to remove payload snapshots.",
          "Link to sample queries and recent export status; not a full query UI.",
          "Observability:",
          "Lag metrics, batch sizes, write errors; alerts when exporters fall behind."
        ],
        "code_structure": {
          "module_path": "internal/long-term-archives/",
          "main_files": [
            "long-term-archives.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "long-term-archives_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Exporter writes to ClickHouse/S3 with retries and idempotency.",
        "Schema versioning with backward compatibility.",
        "Configurable retention in Redis and archive.",
        "Define archive schema + sampling",
        "Build exporter (ClickHouse + S3/Parquet)",
        "Add retention knobs + delete hooks",
        "TUI export status panel",
        "Docs + sample queries",
        "ArchiveDestination",
        "JobRecord, 0, s.config.BatchSize)",
        "Migration json:\"migrations\"",
        "string json:\"deprecated\""
      ]
    },
    {
      "id": "P2.T046",
      "feature_id": "F018",
      "title": "Test Long Term Archives thoroughly",
      "description": "Comprehensive testing for Long Term Archives",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/long-term-archives/*_test.go",
            "test/e2e/long-term-archives_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:long-term-archives:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/long-term-archives.md",
          "excerpt": "Feature specification for long-term-archives",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: schema encode/decode; idempotent writes; partitioning."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: backpressure tests under high throughput; recovery after downtime."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/long-term-archives/*_test.go",
          "integration_tests": "test/integration/long-term-archives_test.go",
          "e2e_tests": "test/e2e/long-term-archives_test.go",
          "benchmarks": "internal/long-term-archives/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Exporter writes to ClickHouse/S3 with retries and idempotency.",
        "Schema versioning with backward compatibility.",
        "Configurable retention in Redis and archive.",
        "Define archive schema + sampling",
        "Build exporter (ClickHouse + S3/Parquet)",
        "Add retention knobs + delete hooks",
        "TUI export status panel",
        "Docs + sample queries",
        "ArchiveDestination",
        "JobRecord, 0, s.config.BatchSize)",
        "Migration json:\"migrations\"",
        "string json:\"deprecated\""
      ]
    },
    {
      "id": "P3.T047",
      "feature_id": "F019",
      "title": "Design Forecasting architecture",
      "description": "Create detailed technical design for Forecasting",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/forecasting.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:forecasting:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/forecasting.md",
          "excerpt": "Feature specification for forecasting",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Predict backlog, throughput, and error trends using simple time\u2011series models (moving averages, Holt\u2011Winters, optional ARIMA) to recommend scaling actions and SLO adjustments. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is brilliant infrastructure thinking! Forecasting transforms reactive ops into proactive capacity management. The EWMA/Holt-Winters progression is smart - start simple, add sophistication. The \"translate forecasts into human hints\" approach is key - nobody wants raw statistics, they want actionable recommendations. Consider adding anomaly detection to flag when reality deviates from predictions - that's when you know something fundamentally changed in your system.",
        "motivation": "- Plan capacity and avoid surprise backlogs. - Inform autoscaling targets and maintenance windows. - Provide early warning on budget burn.",
        "design_deliverables": [
          "Architecture document in docs/design/forecasting.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Persist short history in\u2011memory for TUI charts and optionally to disk for longer windows.",
          "Optionally pull from metrics TSDB (Prometheus) for richer history.",
          "Baseline: EWMA / Holt\u2011Winters; later ARIMA/Prophet plugins.",
          "Compute point forecasts and confidence bounds.",
          "Recommendations:",
          "Translate forecasts into human hints (e.g., \"Scale workers +2 for next 30m\").",
          "Budget burn projections for SLO widget.",
          "Overlay forecast bands on charts; show next 30\u2013120m.",
          "Small \"What's Next\" panel with recommendations."
        ],
        "user_stories_to_address": [
          "I can see forecast bands on queue charts and a plain\u2011English recommendation."
        ]
      },
      "acceptance_criteria": [
        "Baseline forecasting on backlog/throughput with confidence bands.",
        "Recommendations generated and displayed succinctly.",
        "Toggleable overlays; no impact on core loop performance.",
        "Implement EWMA/Holt\u2011Winters",
        "Persist short history; optional TSDB reader",
        "Overlay UI + recommendations",
        "Docs + examples",
        "float64, float64, float64) {",
        "float64, horizon)",
        "float64, horizon)",
        "float64, horizon)",
        "float64 // Seasonal components",
        "float64, horizon)",
        "float64 // Model coefficients",
        "float64 // Historical residuals",
        "float64 // Raw historical data",
        "float64) error {",
        "Recommendation {",
        "Recommendation{}",
        "MetricsCollector",
        "QueueMetrics, error)",
        "QueueMetrics) error {",
        "PredictionRecord"
      ]
    },
    {
      "id": "P3.T048",
      "feature_id": "F019",
      "title": "Implement Forecasting core logic",
      "description": "Build the core functionality for Forecasting",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/forecasting/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T048\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T048\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T048\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T048\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:forecasting:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/forecasting.md",
          "excerpt": "Feature specification for forecasting",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Predict backlog, throughput, and error trends using simple time\u2011series models (moving averages, Holt\u2011Winters, optional ARIMA) to recommend scaling actions and SLO adjustments. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is brilliant infrastructure thinking! Forecasting transforms reactive ops into proactive capacity management. The EWMA/Holt-Winters progression is smart - start simple, add sophistication. The \"translate forecasts into human hints\" approach is key - nobody wants raw statistics, they want actionable recommendations. Consider adding anomaly detection to flag when reality deviates from predictions - that's when you know something fundamentally changed in your system.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Implement EWMA/Holt\u2011Winters",
          "Persist short history; optional TSDB reader",
          "Overlay UI + recommendations",
          "Docs + examples"
        ],
        "technical_approach": [
          "Persist short history in\u2011memory for TUI charts and optionally to disk for longer windows.",
          "Optionally pull from metrics TSDB (Prometheus) for richer history.",
          "Baseline: EWMA / Holt\u2011Winters; later ARIMA/Prophet plugins.",
          "Compute point forecasts and confidence bounds.",
          "Recommendations:",
          "Translate forecasts into human hints (e.g., \"Scale workers +2 for next 30m\").",
          "Budget burn projections for SLO widget.",
          "Overlay forecast bands on charts; show next 30\u2013120m.",
          "Small \"What's Next\" panel with recommendations."
        ],
        "code_structure": {
          "module_path": "internal/forecasting/",
          "main_files": [
            "forecasting.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "forecasting_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Baseline forecasting on backlog/throughput with confidence bands.",
        "Recommendations generated and displayed succinctly.",
        "Toggleable overlays; no impact on core loop performance.",
        "Implement EWMA/Holt\u2011Winters",
        "Persist short history; optional TSDB reader",
        "Overlay UI + recommendations",
        "Docs + examples",
        "float64, float64, float64) {",
        "float64, horizon)",
        "float64, horizon)",
        "float64, horizon)",
        "float64 // Seasonal components",
        "float64, horizon)",
        "float64 // Model coefficients",
        "float64 // Historical residuals",
        "float64 // Raw historical data",
        "float64) error {",
        "Recommendation {",
        "Recommendation{}",
        "MetricsCollector",
        "QueueMetrics, error)",
        "QueueMetrics) error {",
        "PredictionRecord"
      ]
    },
    {
      "id": "P2.T049",
      "feature_id": "F020",
      "title": "Design Multi Tenant Isolation architecture",
      "description": "Create detailed technical design for Multi Tenant Isolation",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/multi-tenant-isolation.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:multi-tenant-isolation:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/multi-tenant-isolation.md",
          "excerpt": "Feature specification for multi-tenant-isolation",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Introduce first\u2011class tenant boundaries: per\u2011tenant queues/keys, quotas, rate limits, encryption\u2011at\u2011rest for payloads, and audit trails. Enable safe multi\u2011tenant deployments with predictable isolation and strong governance. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is enterprise-grade queue infrastructure! The t:{tenant}:{queue} namespacing pattern is brilliant - clean, Redis-friendly, and instantly recognizable. Envelope encryption per tenant with KMS integration shows serious security chops. The audit trail with tenant scoping solves the \"who accessed what\" compliance nightmare. The TUI tenant switcher will make DevOps teams fall in love with this system.",
        "motivation": "- Serve multiple teams/customers safely on shared infra. - Enforce fairness and budgets per tenant; prevent noisy neighbors. - Satisfy security/compliance requirements for data handling.",
        "design_deliverables": [
          "Architecture document in docs/design/multi-tenant-isolation.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Namespacing:",
          "Key scheme t:{tenant}:{queue} with strict validation; TUI filters by tenant.",
          "Tenant metadata store: quotas, limits, encryption keys policy.",
          "Quotas & limits:",
          "Integrate with Advanced Rate Limiting per tenant.",
          "Hard caps on backlog size and daily enqueues; soft warnings before enforcement.",
          "Encryption at rest (payload):",
          "Optional envelope encryption per tenant (AES\u2011GCM) with KEK via KMS; rotate data keys.",
          "Transparent decrypt for workers with tenant grants.",
          "Audit & RBAC:",
          "Scopes limited to tenant: resources; audit tagged with tenant ID.",
          "Observability:",
          "Per\u2011tenant metrics and DLQ counts; TUI tenant switcher and summaries."
        ],
        "user_stories_to_address": [
          "I can define a tenant with quotas and limits and see them enforced.",
          "I can verify encryption on payloads and audit access by tenant."
        ]
      },
      "acceptance_criteria": [
        "Namespaced keys and configs per tenant.",
        "Quotas and rate limits enforced; breaches reported.",
        "Optional payload encryption with rotation.",
        "Define tenant model and keying scheme",
        "Implement quotas/limits + integration with RL",
        "Add optional payload encryption",
        "Tenant\u2011aware RBAC + audit",
        "TUI tenant views + docs",
        "byte json:\"encrypted_dek\" // DEK encrypted by KEK",
        "byte json:\"encrypted_payload\" // Actual job data",
        "byte json:\"nonce\" // AES-GCM nonce",
        "byte json:\"auth_tag\" // Authentication tag",
        "byte) (EncryptedPayload, error) {",
        "byte, 32) // 256 bits",
        "byte, gcm.NonceSize())",
        "string json:\"actions\" // \"read\", \"write\", \"admin\"",
        "TenantPermission json:\"permissions\"",
        "string json:\"actions,omitempty\"",
        "AuditEvent, error) {",
        "TenantSummary"
      ]
    },
    {
      "id": "P2.T050",
      "feature_id": "F020",
      "title": "Implement Multi Tenant Isolation core logic",
      "description": "Build the core functionality for Multi Tenant Isolation",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/multi-tenant-isolation/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T050\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T050\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T050\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T050\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:multi-tenant-isolation:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/multi-tenant-isolation.md",
          "excerpt": "Feature specification for multi-tenant-isolation",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Introduce first\u2011class tenant boundaries: per\u2011tenant queues/keys, quotas, rate limits, encryption\u2011at\u2011rest for payloads, and audit trails. Enable safe multi\u2011tenant deployments with predictable isolation and strong governance. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is enterprise-grade queue infrastructure! The t:{tenant}:{queue} namespacing pattern is brilliant - clean, Redis-friendly, and instantly recognizable. Envelope encryption per tenant with KMS integration shows serious security chops. The audit trail with tenant scoping solves the \"who accessed what\" compliance nightmare. The TUI tenant switcher will make DevOps teams fall in love with this system.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define tenant model and keying scheme",
          "Implement quotas/limits + integration with RL",
          "Add optional payload encryption",
          "Tenant\u2011aware RBAC + audit",
          "TUI tenant views + docs"
        ],
        "technical_approach": [
          "Namespacing:",
          "Key scheme t:{tenant}:{queue} with strict validation; TUI filters by tenant.",
          "Tenant metadata store: quotas, limits, encryption keys policy.",
          "Quotas & limits:",
          "Integrate with Advanced Rate Limiting per tenant.",
          "Hard caps on backlog size and daily enqueues; soft warnings before enforcement.",
          "Encryption at rest (payload):",
          "Optional envelope encryption per tenant (AES\u2011GCM) with KEK via KMS; rotate data keys.",
          "Transparent decrypt for workers with tenant grants.",
          "Audit & RBAC:",
          "Scopes limited to tenant: resources; audit tagged with tenant ID.",
          "Observability:",
          "Per\u2011tenant metrics and DLQ counts; TUI tenant switcher and summaries."
        ],
        "code_structure": {
          "module_path": "internal/multi-tenant-isolation/",
          "main_files": [
            "multi-tenant-isolation.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "multi-tenant-isolation_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Namespaced keys and configs per tenant.",
        "Quotas and rate limits enforced; breaches reported.",
        "Optional payload encryption with rotation.",
        "Define tenant model and keying scheme",
        "Implement quotas/limits + integration with RL",
        "Add optional payload encryption",
        "Tenant\u2011aware RBAC + audit",
        "TUI tenant views + docs",
        "byte json:\"encrypted_dek\" // DEK encrypted by KEK",
        "byte json:\"encrypted_payload\" // Actual job data",
        "byte json:\"nonce\" // AES-GCM nonce",
        "byte json:\"auth_tag\" // Authentication tag",
        "byte) (EncryptedPayload, error) {",
        "byte, 32) // 256 bits",
        "byte, gcm.NonceSize())",
        "string json:\"actions\" // \"read\", \"write\", \"admin\"",
        "TenantPermission json:\"permissions\"",
        "string json:\"actions,omitempty\"",
        "AuditEvent, error) {",
        "TenantSummary"
      ]
    },
    {
      "id": "P2.T051",
      "feature_id": "F020",
      "title": "Test Multi Tenant Isolation thoroughly",
      "description": "Comprehensive testing for Multi Tenant Isolation",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/multi-tenant-isolation/*_test.go",
            "test/e2e/multi-tenant-isolation_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:multi-tenant-isolation:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/multi-tenant-isolation.md",
          "excerpt": "Feature specification for multi-tenant-isolation",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: quota accounting; encryption/decryption; key rotation."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: multi\u2011tenant soak with mixed loads and quota breaches."
            ]
          },
          {
            "type": "security",
            "requirements": [
              "Security: attempt cross\u2011tenant access; verify denial and audit."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/multi-tenant-isolation/*_test.go",
          "integration_tests": "test/integration/multi-tenant-isolation_test.go",
          "e2e_tests": "test/e2e/multi-tenant-isolation_test.go",
          "benchmarks": "internal/multi-tenant-isolation/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Namespaced keys and configs per tenant.",
        "Quotas and rate limits enforced; breaches reported.",
        "Optional payload encryption with rotation.",
        "Define tenant model and keying scheme",
        "Implement quotas/limits + integration with RL",
        "Add optional payload encryption",
        "Tenant\u2011aware RBAC + audit",
        "TUI tenant views + docs",
        "byte json:\"encrypted_dek\" // DEK encrypted by KEK",
        "byte json:\"encrypted_payload\" // Actual job data",
        "byte json:\"nonce\" // AES-GCM nonce",
        "byte json:\"auth_tag\" // Authentication tag",
        "byte) (EncryptedPayload, error) {",
        "byte, 32) // 256 bits",
        "byte, gcm.NonceSize())",
        "string json:\"actions\" // \"read\", \"write\", \"admin\"",
        "TenantPermission json:\"permissions\"",
        "string json:\"actions,omitempty\"",
        "AuditEvent, error) {",
        "TenantSummary"
      ]
    },
    {
      "id": "P2.T052",
      "feature_id": "F021",
      "title": "Design Producer Backpressure architecture",
      "description": "Create detailed technical design for Producer Backpressure",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/producer-backpressure.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:producer-backpressure:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/producer-backpressure.md",
          "excerpt": "Feature specification for producer-backpressure",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Provide producers with real\u2011time backpressure signals and helpers to adapt enqueue rate when queues are saturated. Offer SDK shims that expose SuggestThrottle() and circuit breaking by priority. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is brilliant flow control engineering! Backpressure is the difference between resilient systems and cascading failures. The priority-aware shedding is chef's kiss - protecting high-priority payments while throttling bulk emails. The jittered polling prevents thundering herds. Circuit breakers with half-open probing? Chef's kiss. This could prevent so many 3am pages. Consider adding exponential backoff on repeated throttles and maybe webhook notifications for ops teams when circuits trip.",
        "motivation": "- Prevent runaway enqueue that overwhelms workers and downstream systems. - Encourage smooth traffic shaping without bespoke logic in every producer. - Improve overall SLO adherence by coordinating producers and workers.",
        "design_deliverables": [
          "Architecture document in docs/design/producer-backpressure.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Poll Admin API stats (backlog length, age, rate limit budget) with jitter.",
          "Expose per\u2011queue thresholds and target backlog windows.",
          "Optional push channel via Event Hooks for threshold crossings.",
          "SDK helpers (Go first):",
          "BackpressureController with SuggestThrottle() returning sleep/delay hints.",
          "Circuit breaker for enqueue (open on sustained saturation; half\u2011open probes).",
          "Priority\u2011aware hints (high protected, low sheds first).",
          "Protocol hints:",
          "Include rate/budget headers in API responses when using HTTP Admin API.",
          "Producer integration:",
          "Simple wrapper: bp.Run(ctx, func() { enqueue(...) }) schedules work to hints.",
          "Metrics: observed throttle, shed events, breaker state."
        ],
        "user_stories_to_address": [
          "I can adopt a small helper and see my enqueue smooth under saturation.",
          "I can see backpressure metrics and confirm fewer DLQs and timeouts."
        ]
      },
      "acceptance_criteria": [
        "Backpressure controller with thresholds and jittered polling.",
        "Circuit breaker with configurable trip criteria and recovery.",
        "Metrics exported for throttle and shed decisions.",
        "Define thresholds and controller",
        "Implement Go SDK helper",
        "Integrate Admin API hints",
        "Add TUI indicator + metrics",
        "Docs + examples",
        "Job) error {"
      ]
    },
    {
      "id": "P2.T053",
      "feature_id": "F021",
      "title": "Implement Producer Backpressure core logic",
      "description": "Build the core functionality for Producer Backpressure",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/producer-backpressure/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T053\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T053\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T053\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T053\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:producer-backpressure:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [
        "breaker_system"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/producer-backpressure.md",
          "excerpt": "Feature specification for producer-backpressure",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Provide producers with real\u2011time backpressure signals and helpers to adapt enqueue rate when queues are saturated. Offer SDK shims that expose SuggestThrottle() and circuit breaking by priority. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is brilliant flow control engineering! Backpressure is the difference between resilient systems and cascading failures. The priority-aware shedding is chef's kiss - protecting high-priority payments while throttling bulk emails. The jittered polling prevents thundering herds. Circuit breakers with half-open probing? Chef's kiss. This could prevent so many 3am pages. Consider adding exponential backoff on repeated throttles and maybe webhook notifications for ops teams when circuits trip.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define thresholds and controller",
          "Implement Go SDK helper",
          "Integrate Admin API hints",
          "Add TUI indicator + metrics",
          "Docs + examples"
        ],
        "technical_approach": [
          "Poll Admin API stats (backlog length, age, rate limit budget) with jitter.",
          "Expose per\u2011queue thresholds and target backlog windows.",
          "Optional push channel via Event Hooks for threshold crossings.",
          "SDK helpers (Go first):",
          "BackpressureController with SuggestThrottle() returning sleep/delay hints.",
          "Circuit breaker for enqueue (open on sustained saturation; half\u2011open probes).",
          "Priority\u2011aware hints (high protected, low sheds first).",
          "Protocol hints:",
          "Include rate/budget headers in API responses when using HTTP Admin API.",
          "Producer integration:",
          "Simple wrapper: bp.Run(ctx, func() { enqueue(...) }) schedules work to hints.",
          "Metrics: observed throttle, shed events, breaker state."
        ],
        "code_structure": {
          "module_path": "internal/producer-backpressure/",
          "main_files": [
            "producer-backpressure.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "producer-backpressure_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Backpressure controller with thresholds and jittered polling.",
        "Circuit breaker with configurable trip criteria and recovery.",
        "Metrics exported for throttle and shed decisions.",
        "Define thresholds and controller",
        "Implement Go SDK helper",
        "Integrate Admin API hints",
        "Add TUI indicator + metrics",
        "Docs + examples",
        "Job) error {"
      ]
    },
    {
      "id": "P2.T054",
      "feature_id": "F021",
      "title": "Test Producer Backpressure thoroughly",
      "description": "Comprehensive testing for Producer Backpressure",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/producer-backpressure/*_test.go",
            "test/e2e/producer-backpressure_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:producer-backpressure:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/producer-backpressure.md",
          "excerpt": "Feature specification for producer-backpressure",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: controller math, jitter, breaker transitions."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: patterned load showing smoothing vs. baseline."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/producer-backpressure/*_test.go",
          "integration_tests": "test/integration/producer-backpressure_test.go",
          "e2e_tests": "test/e2e/producer-backpressure_test.go",
          "benchmarks": "internal/producer-backpressure/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Backpressure controller with thresholds and jittered polling.",
        "Circuit breaker with configurable trip criteria and recovery.",
        "Metrics exported for throttle and shed decisions.",
        "Define thresholds and controller",
        "Implement Go SDK helper",
        "Integrate Admin API hints",
        "Add TUI indicator + metrics",
        "Docs + examples",
        "Job) error {"
      ]
    },
    {
      "id": "P3.T055",
      "feature_id": "F022",
      "title": "Design Queue Snapshot Testing architecture",
      "description": "Create detailed technical design for Queue Snapshot Testing",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/queue-snapshot-testing.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:queue-snapshot-testing:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/queue-snapshot-testing.md",
          "excerpt": "Feature specification for queue-snapshot-testing",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Capture complete queue states as snapshots for regression testing, allowing teams to save, version, and replay complex queue scenarios. Compare actual vs. expected states with smart diffing that ignores irrelevant changes. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is Jest for queue states! The deterministic serialization with smart diffing is brilliant - imagine catching that subtle bug where payment queues start accumulating differently after a Redis config change. The Git integration makes this a true \"time machine\" for queue debugging. Consider adding snapshot \"contracts\" - assertions about queue behavior that must hold across deployments.",
        "motivation": "- Catch queue behavior regressions before production - Test complex scenarios without manual setup - Document expected behavior through snapshot examples - Enable test-driven development for queue workflows",
        "design_deliverables": [
          "Architecture document in docs/design/queue-snapshot-testing.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Snapshot Capture:",
          "Complete state: all queues, jobs, workers, configs",
          "Deterministic serialization (sorted keys, normalized timestamps)",
          "Compression with zstd for storage efficiency",
          "Metadata: description, tags, creation context",
          "Snapshot Operations:",
          "Save: capture current state with name/description",
          "Load: restore queue to saved state",
          "Compare: diff two snapshots with smart filtering",
          "Assert: verify current state matches snapshot",
          "Smart Diffing:",
          "Ignore: timestamps (unless relative), job IDs, worker IDs",
          "Focus on: queue lengths, job payloads, failure patterns",
          "Configurable ignore patterns",
          "Semantic diff: understand moved vs. changed jobs",
          "Test Integration:",
          "Jest-style snapshot testing for Go",
          "CI/CD integration with snapshot repositories",
          "Automatic snapshot updates with approval flow",
          "Snapshot fixtures for different test scenarios",
          "Version Control:",
          "Store snapshots in git with large file support",
          "Track snapshot changes over time",
          "Branch-specific snapshot sets",
          "Merge conflict resolution tools",
          "TUI Features:",
          "Snapshot browser with search/filter",
          "Visual diff viewer with side-by-side comparison",
          "One-click save/load current state",
          "Snapshot \"albums\" for related scenarios"
        ],
        "user_stories_to_address": [
          "I can save queue state and assert it hasn't changed",
          "I can maintain a library of test scenarios",
          "I can review snapshot changes in pull requests"
        ]
      },
      "acceptance_criteria": [
        "Snapshots capture complete queue state",
        "Smart diff ignores non-semantic changes",
        "Git integration with reasonable file sizes",
        "<1s to save/load typical snapshots",
        "Design snapshot schema",
        "Implement state serialization",
        "Build smart diff engine",
        "Create test framework integration",
        "Add TUI snapshot browser",
        "Git workflow tooling",
        "Example snapshot library",
        "NormalizedJob json:\"jobs\"",
        "WorkerState json:\"workers\"",
        "NormalizedJob{},",
        "WorkerState{},",
        "Job, baseTime time.Time) NormalizedJob {",
        "NormalizedJob, len(jobs))",
        "JobDiff json:\"job_diffs\"",
        "WorkerDiff json:\"worker_diffs\"",
        "IgnoredChange json:\"ignored\"",
        "IgnoredChange{},",
        "byte(diffReport), 0644)"
      ]
    },
    {
      "id": "P3.T056",
      "feature_id": "F022",
      "title": "Implement Queue Snapshot Testing core logic",
      "description": "Build the core functionality for Queue Snapshot Testing",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/queue-snapshot-testing/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T056\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T056\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T056\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T056\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:queue-snapshot-testing:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [
        "redis_client",
        "queue_system"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/queue-snapshot-testing.md",
          "excerpt": "Feature specification for queue-snapshot-testing",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Capture complete queue states as snapshots for regression testing, allowing teams to save, version, and replay complex queue scenarios. Compare actual vs. expected states with smart diffing that ignores irrelevant changes. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is Jest for queue states! The deterministic serialization with smart diffing is brilliant - imagine catching that subtle bug where payment queues start accumulating differently after a Redis config change. The Git integration makes this a true \"time machine\" for queue debugging. Consider adding snapshot \"contracts\" - assertions about queue behavior that must hold across deployments.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Design snapshot schema",
          "Implement state serialization",
          "Build smart diff engine",
          "Create test framework integration",
          "Add TUI snapshot browser",
          "Git workflow tooling",
          "Example snapshot library"
        ],
        "technical_approach": [
          "Snapshot Capture:",
          "Complete state: all queues, jobs, workers, configs",
          "Deterministic serialization (sorted keys, normalized timestamps)",
          "Compression with zstd for storage efficiency",
          "Metadata: description, tags, creation context",
          "Snapshot Operations:",
          "Save: capture current state with name/description",
          "Load: restore queue to saved state",
          "Compare: diff two snapshots with smart filtering",
          "Assert: verify current state matches snapshot",
          "Smart Diffing:",
          "Ignore: timestamps (unless relative), job IDs, worker IDs",
          "Focus on: queue lengths, job payloads, failure patterns",
          "Configurable ignore patterns",
          "Semantic diff: understand moved vs. changed jobs",
          "Test Integration:",
          "Jest-style snapshot testing for Go",
          "CI/CD integration with snapshot repositories",
          "Automatic snapshot updates with approval flow",
          "Snapshot fixtures for different test scenarios",
          "Version Control:",
          "Store snapshots in git with large file support",
          "Track snapshot changes over time",
          "Branch-specific snapshot sets",
          "Merge conflict resolution tools",
          "TUI Features:",
          "Snapshot browser with search/filter",
          "Visual diff viewer with side-by-side comparison",
          "One-click save/load current state",
          "Snapshot \"albums\" for related scenarios"
        ],
        "code_structure": {
          "module_path": "internal/queue-snapshot-testing/",
          "main_files": [
            "queue-snapshot-testing.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "queue-snapshot-testing_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Snapshots capture complete queue state",
        "Smart diff ignores non-semantic changes",
        "Git integration with reasonable file sizes",
        "<1s to save/load typical snapshots",
        "Design snapshot schema",
        "Implement state serialization",
        "Build smart diff engine",
        "Create test framework integration",
        "Add TUI snapshot browser",
        "Git workflow tooling",
        "Example snapshot library",
        "NormalizedJob json:\"jobs\"",
        "WorkerState json:\"workers\"",
        "NormalizedJob{},",
        "WorkerState{},",
        "Job, baseTime time.Time) NormalizedJob {",
        "NormalizedJob, len(jobs))",
        "JobDiff json:\"job_diffs\"",
        "WorkerDiff json:\"worker_diffs\"",
        "IgnoredChange json:\"ignored\"",
        "IgnoredChange{},",
        "byte(diffReport), 0644)"
      ]
    },
    {
      "id": "P2.T057",
      "feature_id": "F023",
      "title": "Design Smart Retry Strategies architecture",
      "description": "Create detailed technical design for Smart Retry Strategies",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/smart-retry-strategies.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:smart-retry-strategies:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/smart-retry-strategies.md",
          "excerpt": "Feature specification for smart-retry-strategies",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Adapt retry timing and policy based on historical success patterns. Start with robust heuristics and a Bayesian layer, optionally evolve to ML\u2011based recommendations. Bound decisions with guardrails and degrade gracefully to static backoffs. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is absolutely brilliant - adaptive retry strategies using Bayesian inference and ML prediction! The incremental approach (heuristics \u2192 Bayesian \u2192 ML) is perfect for reducing risk while building confidence. The feature store integration ensures the system learns from every failure pattern. The explainability requirement (\"why\" for each recommendation) is crucial for debugging mysterious retry behaviors. Consider adding circuit breaker integration and tenant-specific learning to prevent noisy neighbors from poisoning the model.",
        "motivation": "- Reduce wasted retries and downstream load during outages. - Improve success rate by aligning attempts with recovery windows. - Encode organizational knowledge from past incidents into the system.",
        "design_deliverables": [
          "Architecture document in docs/design/smart-retry-strategies.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Data collection:",
          "Log features per attempt: error class/code, status, attempt number, queue/tenant, payload size, time of day, worker version, downstream health signals.",
          "Persist outcomes and delays; aggregate by error class and job type.",
          "Baseline policies:",
          "Rules for common cases: 429/503 \u2192 exponential + jitter with cap; timeouts \u2192 wider backoff; validation errors \u2192 stop early.",
          "Bayesian layer:",
          "For each (job_type, error_class), fit a simple model of success probability over delay since last failure (e.g., Beta\u2011Binomial buckets).",
          "Recommend next delay that crosses a success threshold with confidence, within min/max bounds.",
          "Optional ML:",
          "Train logistic regression or gradient boosting with cross\u2011validation; export small on\u2011disk model; inference library embedded.",
          "Version models; add canary evaluation before rollout.",
          "Policy guardrails:",
          "Hard caps on max attempts/delay; budget integration to avoid overload; per\u2011tenant fairness.",
          "Explainability: record \"why\" for each recommendation.",
          "API to preview recommended schedule for a failed sample; TUI shows suggested next retry window and rationale."
        ],
        "user_stories_to_address": [
          "I can see recommended next retry times and the reason.",
          "I can cap or override strategies per queue/tenant."
        ]
      },
      "acceptance_criteria": [
        "Baseline heuristics with jitter and caps in place.",
        "Bayesian recommendations improve success/time tradeoff in test.",
        "Optional ML can be enabled and rolled back safely.",
        "Define schema for attempt history and features",
        "Implement rules + Bayesian recommender",
        "Add optional ML trainer/inference path",
        "Expose preview API + TUI surfacing",
        "Shadow \u2192 canary \u2192 full rollout",
        "DelayBucket",
        "Alternative json:\"alternatives\"",
        "HistoricalCase json:\"similar_cases\"",
        "RetryFeatures"
      ]
    },
    {
      "id": "P2.T058",
      "feature_id": "F023",
      "title": "Implement Smart Retry Strategies core logic",
      "description": "Build the core functionality for Smart Retry Strategies",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/smart-retry-strategies/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T058\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T058\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T058\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T058\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:smart-retry-strategies:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [
        "breaker_system"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/smart-retry-strategies.md",
          "excerpt": "Feature specification for smart-retry-strategies",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Adapt retry timing and policy based on historical success patterns. Start with robust heuristics and a Bayesian layer, optionally evolve to ML\u2011based recommendations. Bound decisions with guardrails and degrade gracefully to static backoffs. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is absolutely brilliant - adaptive retry strategies using Bayesian inference and ML prediction! The incremental approach (heuristics \u2192 Bayesian \u2192 ML) is perfect for reducing risk while building confidence. The feature store integration ensures the system learns from every failure pattern. The explainability requirement (\"why\" for each recommendation) is crucial for debugging mysterious retry behaviors. Consider adding circuit breaker integration and tenant-specific learning to prevent noisy neighbors from poisoning the model.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define schema for attempt history and features",
          "Implement rules + Bayesian recommender",
          "Add optional ML trainer/inference path",
          "Expose preview API + TUI surfacing",
          "Shadow \u2192 canary \u2192 full rollout"
        ],
        "technical_approach": [
          "Data collection:",
          "Log features per attempt: error class/code, status, attempt number, queue/tenant, payload size, time of day, worker version, downstream health signals.",
          "Persist outcomes and delays; aggregate by error class and job type.",
          "Baseline policies:",
          "Rules for common cases: 429/503 \u2192 exponential + jitter with cap; timeouts \u2192 wider backoff; validation errors \u2192 stop early.",
          "Bayesian layer:",
          "For each (job_type, error_class), fit a simple model of success probability over delay since last failure (e.g., Beta\u2011Binomial buckets).",
          "Recommend next delay that crosses a success threshold with confidence, within min/max bounds.",
          "Optional ML:",
          "Train logistic regression or gradient boosting with cross\u2011validation; export small on\u2011disk model; inference library embedded.",
          "Version models; add canary evaluation before rollout.",
          "Policy guardrails:",
          "Hard caps on max attempts/delay; budget integration to avoid overload; per\u2011tenant fairness.",
          "Explainability: record \"why\" for each recommendation.",
          "API to preview recommended schedule for a failed sample; TUI shows suggested next retry window and rationale."
        ],
        "code_structure": {
          "module_path": "internal/smart-retry-strategies/",
          "main_files": [
            "smart-retry-strategies.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "smart-retry-strategies_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Baseline heuristics with jitter and caps in place.",
        "Bayesian recommendations improve success/time tradeoff in test.",
        "Optional ML can be enabled and rolled back safely.",
        "Define schema for attempt history and features",
        "Implement rules + Bayesian recommender",
        "Add optional ML trainer/inference path",
        "Expose preview API + TUI surfacing",
        "Shadow \u2192 canary \u2192 full rollout",
        "DelayBucket",
        "Alternative json:\"alternatives\"",
        "HistoricalCase json:\"similar_cases\"",
        "RetryFeatures"
      ]
    },
    {
      "id": "P2.T059",
      "feature_id": "F023",
      "title": "Test Smart Retry Strategies thoroughly",
      "description": "Comprehensive testing for Smart Retry Strategies",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/smart-retry-strategies/*_test.go",
            "test/e2e/smart-retry-strategies_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:smart-retry-strategies:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/smart-retry-strategies.md",
          "excerpt": "Feature specification for smart-retry-strategies",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: rules engine; Bayesian bucket math; cap enforcement."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Offline: train/test split evaluation; compare to baselines on historical logs.",
              "Integration: shadow mode where recommendations are logged, then A/B on a subset."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/smart-retry-strategies/*_test.go",
          "integration_tests": "test/integration/smart-retry-strategies_test.go",
          "e2e_tests": "test/e2e/smart-retry-strategies_test.go",
          "benchmarks": "internal/smart-retry-strategies/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Baseline heuristics with jitter and caps in place.",
        "Bayesian recommendations improve success/time tradeoff in test.",
        "Optional ML can be enabled and rolled back safely.",
        "Define schema for attempt history and features",
        "Implement rules + Bayesian recommender",
        "Add optional ML trainer/inference path",
        "Expose preview API + TUI surfacing",
        "Shadow \u2192 canary \u2192 full rollout",
        "DelayBucket",
        "Alternative json:\"alternatives\"",
        "HistoricalCase json:\"similar_cases\"",
        "RetryFeatures"
      ]
    },
    {
      "id": "P2.T060",
      "feature_id": "F024",
      "title": "Design Storage Backends architecture",
      "description": "Create detailed technical design for Storage Backends",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/storage-backends.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:storage-backends:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/storage-backends.md",
          "excerpt": "Feature specification for storage-backends",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Abstract storage to support multiple engines: Redis Lists (current), Redis Streams, and optionally KeyDB/Dragonfly and Redis Cluster. Provide a pluggable interface and a Kafka outbox bridge for interoperability. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is infrastructure gold! A pluggable storage architecture is the foundation for scaling to any environment. Redis Streams unlocks consumer groups and replay - think Kafka but with Redis simplicity. The KeyDB/Dragonfly compatibility opens doors to massive performance gains. The Kafka outbox pattern is brilliant for hybrid architectures - you get Redis speed with Kafka durability. The interface abstraction prevents vendor lock-in while enabling experimentation with emerging storage engines.",
        "motivation": "- Fit diverse environments and operational preferences. - Unlock features like replay (Streams) and better sharding (Cluster). - Enable hybrid designs via a Kafka outbox for cross\u2011system integration.",
        "design_deliverables": [
          "Architecture document in docs/design/storage-backends.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Abstraction:",
          "Define QueueBackend interface: Enqueue, Dequeue, Ack/Nack, Peek, Length, Move (DLQ), Iter.",
          "Capability flags (atomic ack, consumer groups, idempotency support).",
          "Implementations:",
          "Lists (baseline): current Redis lists; keep as default.",
          "Streams: use XADD/XREADGROUP; maintain consumer group per worker pool; handle pending/claim.",
          "Redis Cluster: key tags to route queues to slots; avoid cross\u2011slot ops.",
          "KeyDB/Dragonfly: validate compatibility; perf tuning knobs.",
          "Outbox bridge:",
          "Optional: publish enqueue events to Kafka topics; reconcile on failures.",
          "Config + migration:",
          "Backend selection per queue; migration tools (drain + copy) with safety.",
          "Observability:",
          "Metrics per backend; warnings for unsupported features; tests for ordering/ack semantics."
        ],
        "user_stories_to_address": [
          "I can choose Streams for a specific queue to enable replay and consumer groups.",
          "I can migrate a queue from Lists to Streams with a guided tool."
        ]
      },
      "acceptance_criteria": [
        "Backend interface with Lists + Streams implementations.",
        "Redis Cluster support for key tagging and safe ops.",
        "Migration tool and documentation.",
        "Define interface + capability flags",
        "Implement Streams backend",
        "Add Cluster key tagging support",
        "Migration tooling",
        "Docs + benchmarks"
      ]
    },
    {
      "id": "P2.T061",
      "feature_id": "F024",
      "title": "Implement Storage Backends core logic",
      "description": "Build the core functionality for Storage Backends",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/storage-backends/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T061\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T061\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T061\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T061\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:storage-backends:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/storage-backends.md",
          "excerpt": "Feature specification for storage-backends",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Abstract storage to support multiple engines: Redis Lists (current), Redis Streams, and optionally KeyDB/Dragonfly and Redis Cluster. Provide a pluggable interface and a Kafka outbox bridge for interoperability. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is infrastructure gold! A pluggable storage architecture is the foundation for scaling to any environment. Redis Streams unlocks consumer groups and replay - think Kafka but with Redis simplicity. The KeyDB/Dragonfly compatibility opens doors to massive performance gains. The Kafka outbox pattern is brilliant for hybrid architectures - you get Redis speed with Kafka durability. The interface abstraction prevents vendor lock-in while enabling experimentation with emerging storage engines.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define interface + capability flags",
          "Implement Streams backend",
          "Add Cluster key tagging support",
          "Migration tooling",
          "Docs + benchmarks"
        ],
        "technical_approach": [
          "Abstraction:",
          "Define QueueBackend interface: Enqueue, Dequeue, Ack/Nack, Peek, Length, Move (DLQ), Iter.",
          "Capability flags (atomic ack, consumer groups, idempotency support).",
          "Implementations:",
          "Lists (baseline): current Redis lists; keep as default.",
          "Streams: use XADD/XREADGROUP; maintain consumer group per worker pool; handle pending/claim.",
          "Redis Cluster: key tags to route queues to slots; avoid cross\u2011slot ops.",
          "KeyDB/Dragonfly: validate compatibility; perf tuning knobs.",
          "Outbox bridge:",
          "Optional: publish enqueue events to Kafka topics; reconcile on failures.",
          "Config + migration:",
          "Backend selection per queue; migration tools (drain + copy) with safety.",
          "Observability:",
          "Metrics per backend; warnings for unsupported features; tests for ordering/ack semantics."
        ],
        "code_structure": {
          "module_path": "internal/storage-backends/",
          "main_files": [
            "storage-backends.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "storage-backends_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Backend interface with Lists + Streams implementations.",
        "Redis Cluster support for key tagging and safe ops.",
        "Migration tool and documentation.",
        "Define interface + capability flags",
        "Implement Streams backend",
        "Add Cluster key tagging support",
        "Migration tooling",
        "Docs + benchmarks"
      ]
    },
    {
      "id": "P2.T062",
      "feature_id": "F024",
      "title": "Test Storage Backends thoroughly",
      "description": "Comprehensive testing for Storage Backends",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/storage-backends/*_test.go",
            "test/e2e/storage-backends_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:storage-backends:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/storage-backends.md",
          "excerpt": "Feature specification for storage-backends",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: interface conformance; edge semantics (ack, pending claims)."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: throughput/latency benchmarks; migration rehearsal."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/storage-backends/*_test.go",
          "integration_tests": "test/integration/storage-backends_test.go",
          "e2e_tests": "test/e2e/storage-backends_test.go",
          "benchmarks": "internal/storage-backends/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Backend interface with Lists + Streams implementations.",
        "Redis Cluster support for key tagging and safe ops.",
        "Migration tool and documentation.",
        "Define interface + capability flags",
        "Implement Streams backend",
        "Add Cluster key tagging support",
        "Migration tooling",
        "Docs + benchmarks"
      ]
    },
    {
      "id": "P4.T063",
      "feature_id": "F025",
      "title": "Design Terminal Voice Commands architecture",
      "description": "Create detailed technical design for Terminal Voice Commands",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/terminal-voice-commands.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:terminal-voice-commands:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/terminal-voice-commands.md",
          "excerpt": "Feature specification for terminal-voice-commands",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Hands-free queue management through natural language voice commands. Speak to your terminal to perform operations, get status updates, and navigate the TUI\u2014improving accessibility and enabling multitasking. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is accessibility meets power-user efficiency! The \"Hey Queue\" wake word is perfect branding, and Whisper.cpp for local processing addresses privacy concerns beautifully. The natural language commands (\"drain worker 3\") are so much better than remembering cryptic hotkeys. This could be the first developer tool that makes voice control actually useful instead of gimmicky. Consider adding voice macros for complex operations!",
        "motivation": "- Accessibility for users with mobility impairments - Hands-free operation during incidents - Faster command execution for power users - Natural language queries instead of memorizing syntax",
        "design_deliverables": [
          "Architecture document in docs/design/terminal-voice-commands.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Speech Recognition:",
          "Local: Whisper.cpp for privacy-conscious environments",
          "Cloud: Google Speech-to-Text or Azure Speech for accuracy",
          "Configurable backend with fallback options",
          "Wake word detection: \"Hey Queue\" or custom phrase",
          "Natural Language Processing:",
          "Intent recognition with simple pattern matching initially",
          "Command aliases and variations (\"drain worker 3\", \"stop the third worker\")",
          "Context awareness: remember recent commands",
          "Fuzzy matching for queue/worker names",
          "Command Grammar:",
          "Status queries: \"How many jobs in high priority?\"",
          "Actions: \"Requeue all failed jobs\", \"Drain worker 2\"",
          "Navigation: \"Show me the DLQ\", \"Go to charts\"",
          "Confirmations: \"Yes\", \"Cancel\", \"Confirm\"",
          "Audio Feedback:",
          "Text-to-speech for responses (optional)",
          "Audio cues for success/failure",
          "Adjustable voice personality",
          "Privacy & Security:",
          "Local-first processing option",
          "No audio recording by default",
          "Explicit opt-in for cloud services",
          "Sanitize sensitive data from voice logs",
          "TUI Integration:",
          "Voice indicator when listening",
          "Transcript display of recognized commands",
          "Visual feedback during processing",
          "Keyboard hotkey to activate (e.g., 'v')"
        ],
        "user_stories_to_address": [
          "I can manage queues without using keyboard/mouse",
          "I can query status while typing in another window",
          "I can execute complex commands with natural language"
        ]
      },
      "acceptance_criteria": [
        ">90% accuracy for common commands in quiet environment",
        "<500ms response time for local recognition",
        "Support for 50+ command variations",
        "Graceful degradation when recognition fails",
        "Integrate Whisper.cpp for local recognition",
        "Build intent recognition system",
        "Create command grammar and aliases",
        "Add TUI voice indicators",
        "Implement audio feedback",
        "Cloud backend integration",
        "Accessibility testing",
        "byte) (Recognition, error)",
        "byte) (Recognition, error) {",
        "byte) (bool, string, error) {",
        "CommandPattern",
        "CommandPattern{",
        "EntityType{EntityTarget},",
        "EntityType{EntityWorkerID},",
        "EntityType{EntityDestination},",
        "string) (Entity, error) {",
        "byte) (Command, error) {",
        "SensitivePattern",
        "byte, 100),",
        "byte, 0, 160002) // 1 second buffer at 16kHz",
        "time.Duration",
        "time.Duration"
      ]
    },
    {
      "id": "P4.T064",
      "feature_id": "F025",
      "title": "Implement Terminal Voice Commands core logic",
      "description": "Build the core functionality for Terminal Voice Commands",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/terminal-voice-commands/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P4.T064\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P4.T064\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P4.T064\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P4.T064\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:terminal-voice-commands:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/terminal-voice-commands.md",
          "excerpt": "Feature specification for terminal-voice-commands",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Hands-free queue management through natural language voice commands. Speak to your terminal to perform operations, get status updates, and navigate the TUI\u2014improving accessibility and enabling multitasking. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is accessibility meets power-user efficiency! The \"Hey Queue\" wake word is perfect branding, and Whisper.cpp for local processing addresses privacy concerns beautifully. The natural language commands (\"drain worker 3\") are so much better than remembering cryptic hotkeys. This could be the first developer tool that makes voice control actually useful instead of gimmicky. Consider adding voice macros for complex operations!",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Integrate Whisper.cpp for local recognition",
          "Build intent recognition system",
          "Create command grammar and aliases",
          "Add TUI voice indicators",
          "Implement audio feedback",
          "Cloud backend integration",
          "Accessibility testing"
        ],
        "technical_approach": [
          "Speech Recognition:",
          "Local: Whisper.cpp for privacy-conscious environments",
          "Cloud: Google Speech-to-Text or Azure Speech for accuracy",
          "Configurable backend with fallback options",
          "Wake word detection: \"Hey Queue\" or custom phrase",
          "Natural Language Processing:",
          "Intent recognition with simple pattern matching initially",
          "Command aliases and variations (\"drain worker 3\", \"stop the third worker\")",
          "Context awareness: remember recent commands",
          "Fuzzy matching for queue/worker names",
          "Command Grammar:",
          "Status queries: \"How many jobs in high priority?\"",
          "Actions: \"Requeue all failed jobs\", \"Drain worker 2\"",
          "Navigation: \"Show me the DLQ\", \"Go to charts\"",
          "Confirmations: \"Yes\", \"Cancel\", \"Confirm\"",
          "Audio Feedback:",
          "Text-to-speech for responses (optional)",
          "Audio cues for success/failure",
          "Adjustable voice personality",
          "Privacy & Security:",
          "Local-first processing option",
          "No audio recording by default",
          "Explicit opt-in for cloud services",
          "Sanitize sensitive data from voice logs",
          "TUI Integration:",
          "Voice indicator when listening",
          "Transcript display of recognized commands",
          "Visual feedback during processing",
          "Keyboard hotkey to activate (e.g., 'v')"
        ],
        "code_structure": {
          "module_path": "internal/terminal-voice-commands/",
          "main_files": [
            "terminal-voice-commands.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "terminal-voice-commands_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        ">90% accuracy for common commands in quiet environment",
        "<500ms response time for local recognition",
        "Support for 50+ command variations",
        "Graceful degradation when recognition fails",
        "Integrate Whisper.cpp for local recognition",
        "Build intent recognition system",
        "Create command grammar and aliases",
        "Add TUI voice indicators",
        "Implement audio feedback",
        "Cloud backend integration",
        "Accessibility testing",
        "byte) (Recognition, error)",
        "byte) (Recognition, error) {",
        "byte) (bool, string, error) {",
        "CommandPattern",
        "CommandPattern{",
        "EntityType{EntityTarget},",
        "EntityType{EntityWorkerID},",
        "EntityType{EntityDestination},",
        "string) (Entity, error) {",
        "byte) (Command, error) {",
        "SensitivePattern",
        "byte, 100),",
        "byte, 0, 160002) // 1 second buffer at 16kHz",
        "time.Duration",
        "time.Duration"
      ]
    },
    {
      "id": "P4.T065",
      "feature_id": "F026",
      "title": "Design Theme Playground architecture",
      "description": "Create detailed technical design for Theme Playground",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/theme-playground.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:theme-playground:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/theme-playground.md",
          "excerpt": "Feature specification for theme-playground",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A centralized theme system with dark/light and high\u2011contrast palettes, plus a playground to preview and switch themes live. Persist preferences, expose a few tunables (accent, contrast), and ensure accessible defaults. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is accessibility done right! The combination of live preview + WCAG compliance checking + persistent preferences hits the sweet spot. The real genius is the playground concept - letting users see theme changes across ALL components instantly prevents the \"looks good in isolation, terrible in context\" problem. Consider adding a color blindness simulator mode and maybe export themes as shareable configs.",
        "motivation": "- Improve readability across terminals and environments. - Provide accessible options (high\u2011contrast) without forking styles per component. - Make demos and screenshots pop while staying consistent.",
        "design_deliverables": [
          "Architecture document in docs/design/theme-playground.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Theme core:",
          "Define a Theme struct (palettes, borders, emphasis, status colors) and a registry.",
          "Replace ad\u2011hoc Lip Gloss colors with theme lookups.",
          "Adaptive colors for dark/light terminals; high\u2011contrast variant.",
          "Playground:",
          "Settings tab adds a Theme section with preview tiles and quick toggle keys.",
          "Live apply to the whole app; animation optional to avoid flicker.",
          "Persistence:",
          "Save to a small state file (e.g., $XDG_CONFIG_HOME/go-redis-wq/theme.json).",
          "Respect NO_COLOR and minimal styles mode.",
          "Accessibility:",
          "Contrast checks for text vs background (WCAG-ish heuristic); badges for risky pairs.",
          "Provide a monochrome theme for limited terminals."
        ],
        "user_stories_to_address": [
          "I can switch themes quickly and persist my choice.",
          "I can choose a high\u2011contrast theme that remains readable everywhere."
        ]
      },
      "acceptance_criteria": [
        "Theme registry with dark/light/high\u2011contrast.",
        "Settings UI to preview/apply themes and tweak accent.",
        "Persisted across sessions and machines (path shown in UI).",
        "Define Theme struct + registry",
        "Replace hardcoded colors with theme lookups",
        "Add Settings playground UI",
        "Persist preferences",
        "Docs + screenshots",
        "string json:\"warnings\"",
        "AccessibilityTest, 0),"
      ]
    },
    {
      "id": "P4.T066",
      "feature_id": "F026",
      "title": "Implement Theme Playground core logic",
      "description": "Build the core functionality for Theme Playground",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/theme-playground/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P4.T066\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P4.T066\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P4.T066\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P4.T066\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:theme-playground:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/theme-playground.md",
          "excerpt": "Feature specification for theme-playground",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A centralized theme system with dark/light and high\u2011contrast palettes, plus a playground to preview and switch themes live. Persist preferences, expose a few tunables (accent, contrast), and ensure accessible defaults. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is accessibility done right! The combination of live preview + WCAG compliance checking + persistent preferences hits the sweet spot. The real genius is the playground concept - letting users see theme changes across ALL components instantly prevents the \"looks good in isolation, terrible in context\" problem. Consider adding a color blindness simulator mode and maybe export themes as shareable configs.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Define Theme struct + registry",
          "Replace hardcoded colors with theme lookups",
          "Add Settings playground UI",
          "Persist preferences",
          "Docs + screenshots"
        ],
        "technical_approach": [
          "Theme core:",
          "Define a Theme struct (palettes, borders, emphasis, status colors) and a registry.",
          "Replace ad\u2011hoc Lip Gloss colors with theme lookups.",
          "Adaptive colors for dark/light terminals; high\u2011contrast variant.",
          "Playground:",
          "Settings tab adds a Theme section with preview tiles and quick toggle keys.",
          "Live apply to the whole app; animation optional to avoid flicker.",
          "Persistence:",
          "Save to a small state file (e.g., $XDG_CONFIG_HOME/go-redis-wq/theme.json).",
          "Respect NO_COLOR and minimal styles mode.",
          "Accessibility:",
          "Contrast checks for text vs background (WCAG-ish heuristic); badges for risky pairs.",
          "Provide a monochrome theme for limited terminals."
        ],
        "code_structure": {
          "module_path": "internal/theme-playground/",
          "main_files": [
            "theme-playground.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "theme-playground_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Theme registry with dark/light/high\u2011contrast.",
        "Settings UI to preview/apply themes and tweak accent.",
        "Persisted across sessions and machines (path shown in UI).",
        "Define Theme struct + registry",
        "Replace hardcoded colors with theme lookups",
        "Add Settings playground UI",
        "Persist preferences",
        "Docs + screenshots",
        "string json:\"warnings\"",
        "AccessibilityTest, 0),"
      ]
    },
    {
      "id": "P3.T067",
      "feature_id": "F027",
      "title": "Design Trace Drilldown Log Tail architecture",
      "description": "Create detailed technical design for Trace Drilldown Log Tail",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/trace-drilldown-log-tail.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:trace-drilldown-log-tail:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/trace-drilldown-log-tail.md",
          "excerpt": "Feature specification for trace-drilldown-log-tail",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Surface trace IDs in the TUI and provide a log tail pane with filters to accelerate RCA. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Observability nirvana! Going from \"job failed\" to root cause in seconds, without leaving the terminal. The trace ID propagation is already there in your code. Log tailing is tricky - consider using SSE or WebSockets for real-time streaming. Maybe integrate with Vector or Fluentd for log aggregation? The PII risk is real - add redaction patterns.",
        "motivation": "Tighten the feedback loop from failing jobs to actionable traces/logs.",
        "design_deliverables": [
          "Architecture document in docs/design/trace-drilldown-log-tail.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Ensure trace IDs captured in payload/metadata; configurable tracing base URL.",
          "Add \u201cOpen Trace\u201d action (external link or inline spans summary).",
          "Implement lightweight log tailer with rate cap and filters by job/worker."
        ],
        "user_stories_to_address": [
          "I can open a job\u2019s trace from the TUI.",
          "I can tail logs filtered by job or worker."
        ]
      },
      "acceptance_criteria": [
        "Trace IDs visible in Peek/Info; action to open.",
        "Log tail pane with follow mode, filters, and backpressure protection.",
        "Configurable endpoints for tracing and logs.",
        "Capture/propagate trace IDs",
        "Add Open Trace action",
        "Implement log tail pane with filters",
        "Docs and examples"
      ]
    },
    {
      "id": "P3.T068",
      "feature_id": "F027",
      "title": "Implement Trace Drilldown Log Tail core logic",
      "description": "Build the core functionality for Trace Drilldown Log Tail",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/trace-drilldown-log-tail/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T068\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T068\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T068\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T068\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:trace-drilldown-log-tail:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [
        "obs_system"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/trace-drilldown-log-tail.md",
          "excerpt": "Feature specification for trace-drilldown-log-tail",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Surface trace IDs in the TUI and provide a log tail pane with filters to accelerate RCA. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Observability nirvana! Going from \"job failed\" to root cause in seconds, without leaving the terminal. The trace ID propagation is already there in your code. Log tailing is tricky - consider using SSE or WebSockets for real-time streaming. Maybe integrate with Vector or Fluentd for log aggregation? The PII risk is real - add redaction patterns.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Capture/propagate trace IDs",
          "Add Open Trace action",
          "Implement log tail pane with filters",
          "Docs and examples"
        ],
        "technical_approach": [
          "Ensure trace IDs captured in payload/metadata; configurable tracing base URL.",
          "Add \u201cOpen Trace\u201d action (external link or inline spans summary).",
          "Implement lightweight log tailer with rate cap and filters by job/worker."
        ],
        "code_structure": {
          "module_path": "internal/trace-drilldown-log-tail/",
          "main_files": [
            "trace-drilldown-log-tail.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "trace-drilldown-log-tail_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Trace IDs visible in Peek/Info; action to open.",
        "Log tail pane with follow mode, filters, and backpressure protection.",
        "Configurable endpoints for tracing and logs.",
        "Capture/propagate trace IDs",
        "Add Open Trace action",
        "Implement log tail pane with filters",
        "Docs and examples"
      ]
    },
    {
      "id": "P2.T069",
      "feature_id": "F028",
      "title": "Design Dlq Remediation Ui architecture",
      "description": "Create detailed technical design for Dlq Remediation Ui",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/dlq-remediation-ui.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:dlq-remediation-ui:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/dlq-remediation-ui.md",
          "excerpt": "Feature specification for dlq-remediation-ui",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A focused DLQ tab to list, search, peek, requeue, and purge items safely with confirmations. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > THIS is what separates toy queues from production queues. Every on-call engineer has cursed at a DLQ they can't easily inspect. The fact you're putting this in the TUI instead of a separate tool is chef's kiss. Pagination is critical - I've seen DLQs with millions of items. Consider adding bulk operations (requeue all matching filter) and export-to-CSV for post-mortems.",
        "motivation": "Reduce incident toil; provide a fast remediation loop within the TUI.",
        "design_deliverables": [
          "Architecture document in docs/design/dlq-remediation-ui.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "API: add DLQ list with pagination, peek by index/ID, requeue selected, purge endpoints.",
          "TUI: DLQ tab with pager, filter, selection; action bar; confirmations.",
          "Performance: server-side pagination; cap payload sizes; streaming where feasible."
        ],
        "user_stories_to_address": [
          "I can list and filter DLQ items and peek payloads.",
          "I can requeue or purge selected items with confirmation."
        ]
      },
      "acceptance_criteria": [
        "DLQ list is paginated with total count and filter.",
        "Peek shows pretty JSON and metadata.",
        "Requeue/Purge actions exist for selected items; purge all gated by confirm.",
        "Handles large DLQs without freezing the UI.",
        "API: list + count with filters",
        "API: peek item by ID/index",
        "API: requeue selected",
        "API: purge selected/all",
        "TUI: DLQ tab UI + pager + actions",
        "Docs + screenshots",
        "Override priority \u2502",
        "Invalid payload \u2502",
        "AttemptRecord"
      ]
    },
    {
      "id": "P2.T070",
      "feature_id": "F028",
      "title": "Implement Dlq Remediation Ui core logic",
      "description": "Build the core functionality for Dlq Remediation Ui",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/dlq-remediation-ui/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T070\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T070\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T070\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T070\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:dlq-remediation-ui:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [
        "tui_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/dlq-remediation-ui.md",
          "excerpt": "Feature specification for dlq-remediation-ui",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A focused DLQ tab to list, search, peek, requeue, and purge items safely with confirmations. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > THIS is what separates toy queues from production queues. Every on-call engineer has cursed at a DLQ they can't easily inspect. The fact you're putting this in the TUI instead of a separate tool is chef's kiss. Pagination is critical - I've seen DLQs with millions of items. Consider adding bulk operations (requeue all matching filter) and export-to-CSV for post-mortems.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "API: list + count with filters",
          "API: peek item by ID/index",
          "API: requeue selected",
          "API: purge selected/all",
          "TUI: DLQ tab UI + pager + actions",
          "Docs + screenshots"
        ],
        "technical_approach": [
          "API: add DLQ list with pagination, peek by index/ID, requeue selected, purge endpoints.",
          "TUI: DLQ tab with pager, filter, selection; action bar; confirmations.",
          "Performance: server-side pagination; cap payload sizes; streaming where feasible."
        ],
        "code_structure": {
          "module_path": "internal/dlq-remediation-ui/",
          "main_files": [
            "dlq-remediation-ui.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "dlq-remediation-ui_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "DLQ list is paginated with total count and filter.",
        "Peek shows pretty JSON and metadata.",
        "Requeue/Purge actions exist for selected items; purge all gated by confirm.",
        "Handles large DLQs without freezing the UI.",
        "API: list + count with filters",
        "API: peek item by ID/index",
        "API: requeue selected",
        "API: purge selected/all",
        "TUI: DLQ tab UI + pager + actions",
        "Docs + screenshots",
        "Override priority \u2502",
        "Invalid payload \u2502",
        "AttemptRecord"
      ]
    },
    {
      "id": "P2.T071",
      "feature_id": "F028",
      "title": "Test Dlq Remediation Ui thoroughly",
      "description": "Comprehensive testing for Dlq Remediation Ui",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/dlq-remediation-ui/*_test.go",
            "test/e2e/dlq-remediation-ui_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:dlq-remediation-ui:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/dlq-remediation-ui.md",
          "excerpt": "Feature specification for dlq-remediation-ui",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "integration",
            "requirements": [
              "API: pagination correctness; requeue idempotency; purge limits.",
              "TUI: interaction tests (manual + scripted); large list navigation."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/dlq-remediation-ui/*_test.go",
          "integration_tests": "test/integration/dlq-remediation-ui_test.go",
          "e2e_tests": "test/e2e/dlq-remediation-ui_test.go",
          "benchmarks": "internal/dlq-remediation-ui/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "DLQ list is paginated with total count and filter.",
        "Peek shows pretty JSON and metadata.",
        "Requeue/Purge actions exist for selected items; purge all gated by confirm.",
        "Handles large DLQs without freezing the UI.",
        "API: list + count with filters",
        "API: peek item by ID/index",
        "API: requeue selected",
        "API: purge selected/all",
        "TUI: DLQ tab UI + pager + actions",
        "Docs + screenshots",
        "Override priority \u2502",
        "Invalid payload \u2502",
        "AttemptRecord"
      ]
    },
    {
      "id": "P2.T072",
      "feature_id": "F029",
      "title": "Design Dlq Remediation Pipeline architecture",
      "description": "Create detailed technical design for Dlq Remediation Pipeline",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/dlq-remediation-pipeline.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:dlq-remediation-pipeline:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/dlq-remediation-pipeline.md",
          "excerpt": "Feature specification for dlq-remediation-pipeline",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Automate DLQ cleanup with a configurable pipeline: classify failures, apply rules (transform, redact, reroute), and requeue with safe limits. Provide dry\u2011run, audit, and guardrails to reduce incident toil. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is the feature that turns your DLQ from a graveyard into a resurrection machine! The combination of pluggable classifiers with an external hook option is genius - perfect for ML-powered classification. The safety caps are absolutely critical to prevent runaway retry loops that could bring down production. Consider adding a \"circuit breaker\" pattern that automatically disables rules with high failure rates.",
        "motivation": "- Shrink time from failure to recovery by automating common remediation patterns. - Reduce manual, error\u2011prone DLQ archaeology. - Capture organizational knowledge as reusable rules.",
        "design_deliverables": [
          "Architecture document in docs/design/dlq-remediation-pipeline.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Classification:",
          "Pluggable matchers: error substrings/regex, fields in payload, origin queue, retry count.",
          "Optional external classifier hook (HTTP/NATS) for complex decisions.",
          "Remediation actions:",
          "Requeue to original or alternate queue; transform payload; redact fields; drop.",
          "Caps: max per minute, max total, stop on error rate spike.",
          "Pipeline engine:",
          "Poll DLQ or subscribe to events; batch evaluate; apply actions with idempotency keys.",
          "Track outcome metrics and write audit entries.",
          "TUI integration:",
          "Rule editor (basic); simulation mode on sample DLQ slice; live counters.",
          "Dry\u2011run mode with diff; explicit confirm to apply.",
          "Blacklist of non\u2011retryable errors; TTL for requeued items."
        ],
        "user_stories_to_address": [
          "I can define a rule that requeues validation errors after redacting a field.",
          "I can run a dry\u2011run on 100 DLQ items and review the plan."
        ]
      },
      "acceptance_criteria": [
        "Classifier + actions with rate caps and idempotency.",
        "Dry\u2011run and audit log of changes.",
        "TUI shows rule results and counters.",
        "Rule schema + storage",
        "Classifier engine + optional external hook",
        "Actions (requeue/transform/redact/drop)",
        "Dry\u2011run + audit + metrics",
        "TUI rule editor + counters",
        "string json:\"suggested_actions\"",
        "RemediationRule",
        "Action) error"
      ]
    },
    {
      "id": "P2.T073",
      "feature_id": "F029",
      "title": "Implement Dlq Remediation Pipeline core logic",
      "description": "Build the core functionality for Dlq Remediation Pipeline",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/dlq-remediation-pipeline/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T073\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T073\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T073\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T073\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:dlq-remediation-pipeline:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/dlq-remediation-pipeline.md",
          "excerpt": "Feature specification for dlq-remediation-pipeline",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Automate DLQ cleanup with a configurable pipeline: classify failures, apply rules (transform, redact, reroute), and requeue with safe limits. Provide dry\u2011run, audit, and guardrails to reduce incident toil. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is the feature that turns your DLQ from a graveyard into a resurrection machine! The combination of pluggable classifiers with an external hook option is genius - perfect for ML-powered classification. The safety caps are absolutely critical to prevent runaway retry loops that could bring down production. Consider adding a \"circuit breaker\" pattern that automatically disables rules with high failure rates.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Rule schema + storage",
          "Classifier engine + optional external hook",
          "Actions (requeue/transform/redact/drop)",
          "Dry\u2011run + audit + metrics",
          "TUI rule editor + counters"
        ],
        "technical_approach": [
          "Classification:",
          "Pluggable matchers: error substrings/regex, fields in payload, origin queue, retry count.",
          "Optional external classifier hook (HTTP/NATS) for complex decisions.",
          "Remediation actions:",
          "Requeue to original or alternate queue; transform payload; redact fields; drop.",
          "Caps: max per minute, max total, stop on error rate spike.",
          "Pipeline engine:",
          "Poll DLQ or subscribe to events; batch evaluate; apply actions with idempotency keys.",
          "Track outcome metrics and write audit entries.",
          "TUI integration:",
          "Rule editor (basic); simulation mode on sample DLQ slice; live counters.",
          "Dry\u2011run mode with diff; explicit confirm to apply.",
          "Blacklist of non\u2011retryable errors; TTL for requeued items."
        ],
        "code_structure": {
          "module_path": "internal/dlq-remediation-pipeline/",
          "main_files": [
            "dlq-remediation-pipeline.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "dlq-remediation-pipeline_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Classifier + actions with rate caps and idempotency.",
        "Dry\u2011run and audit log of changes.",
        "TUI shows rule results and counters.",
        "Rule schema + storage",
        "Classifier engine + optional external hook",
        "Actions (requeue/transform/redact/drop)",
        "Dry\u2011run + audit + metrics",
        "TUI rule editor + counters",
        "string json:\"suggested_actions\"",
        "RemediationRule",
        "Action) error"
      ]
    },
    {
      "id": "P2.T074",
      "feature_id": "F029",
      "title": "Test Dlq Remediation Pipeline thoroughly",
      "description": "Comprehensive testing for Dlq Remediation Pipeline",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/dlq-remediation-pipeline/*_test.go",
            "test/e2e/dlq-remediation-pipeline_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:dlq-remediation-pipeline:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/dlq-remediation-pipeline.md",
          "excerpt": "Feature specification for dlq-remediation-pipeline",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: matcher coverage; transform functions; cap enforcement."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: end\u2011to\u2011end on synthetic DLQ with mixed failures."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/dlq-remediation-pipeline/*_test.go",
          "integration_tests": "test/integration/dlq-remediation-pipeline_test.go",
          "e2e_tests": "test/e2e/dlq-remediation-pipeline_test.go",
          "benchmarks": "internal/dlq-remediation-pipeline/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Classifier + actions with rate caps and idempotency.",
        "Dry\u2011run and audit log of changes.",
        "TUI shows rule results and counters.",
        "Rule schema + storage",
        "Classifier engine + optional external hook",
        "Actions (requeue/transform/redact/drop)",
        "Dry\u2011run + audit + metrics",
        "TUI rule editor + counters",
        "string json:\"suggested_actions\"",
        "RemediationRule",
        "Action) error"
      ]
    },
    {
      "id": "P3.T075",
      "feature_id": "F030",
      "title": "Design Patterned Load Generator architecture",
      "description": "Create detailed technical design for Patterned Load Generator",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/patterned-load-generator.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:patterned-load-generator:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/patterned-load-generator.md",
          "excerpt": "Feature specification for patterned-load-generator",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Extend the bench tool to support sine/burst/ramp patterns, with guardrails and live visualization. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Smart evolution of your existing bench tool! Sine waves for daily traffic, bursts for Black Friday, ramps for gradual rollouts. This is how you validate your circuit breaker and rate limiter in realistic scenarios. The profile save/load is clutch for CI/CD integration. Consider adding a \"chaos\" pattern that randomly switches between patterns - great for finding edge cases.",
        "motivation": "Validate behavior under realistic traffic and create great demos.",
        "design_deliverables": [
          "Architecture document in docs/design/patterned-load-generator.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Implement pattern generators; controls for duration/amplitude; guardrails (max rate/total).",
          "Overlay target vs actual enqueue rate on charts; profile persistence optional."
        ],
        "user_stories_to_address": [
          "I can run predefined patterns and see accurate live charts."
        ]
      },
      "acceptance_criteria": [
        "Sine, burst, ramp patterns; cancel/stop supported.",
        "Guardrails prevent runaway load.",
        "Saved profiles can be reloaded.",
        "Implement sine/burst/ramp",
        "Add controls + guardrails",
        "Chart overlay target vs actual",
        "Save/load profiles"
      ]
    },
    {
      "id": "P3.T076",
      "feature_id": "F030",
      "title": "Implement Patterned Load Generator core logic",
      "description": "Build the core functionality for Patterned Load Generator",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/patterned-load-generator/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T076\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T076\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T076\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T076\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:patterned-load-generator:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/patterned-load-generator.md",
          "excerpt": "Feature specification for patterned-load-generator",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Extend the bench tool to support sine/burst/ramp patterns, with guardrails and live visualization. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Smart evolution of your existing bench tool! Sine waves for daily traffic, bursts for Black Friday, ramps for gradual rollouts. This is how you validate your circuit breaker and rate limiter in realistic scenarios. The profile save/load is clutch for CI/CD integration. Consider adding a \"chaos\" pattern that randomly switches between patterns - great for finding edge cases.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Implement sine/burst/ramp",
          "Add controls + guardrails",
          "Chart overlay target vs actual",
          "Save/load profiles"
        ],
        "technical_approach": [
          "Implement pattern generators; controls for duration/amplitude; guardrails (max rate/total).",
          "Overlay target vs actual enqueue rate on charts; profile persistence optional."
        ],
        "code_structure": {
          "module_path": "internal/patterned-load-generator/",
          "main_files": [
            "patterned-load-generator.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "patterned-load-generator_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Sine, burst, ramp patterns; cancel/stop supported.",
        "Guardrails prevent runaway load.",
        "Saved profiles can be reloaded.",
        "Implement sine/burst/ramp",
        "Add controls + guardrails",
        "Chart overlay target vs actual",
        "Save/load profiles"
      ]
    },
    {
      "id": "P3.T077",
      "feature_id": "F031",
      "title": "Design Policy Simulator architecture",
      "description": "Create detailed technical design for Policy Simulator",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/policy-simulator.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:policy-simulator:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/policy-simulator.md",
          "excerpt": "Feature specification for policy-simulator",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A \"what\u2011if\" simulator to preview the impact of policy changes (retry/backoff, rate limits, concurrency) before applying. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Ambitious! This is chaos engineering meets capacity planning. The queueing theory math is non-trivial (Little's Law, M/M/c models). High risk of \"garbage in, garbage out\" if the model oversimplifies. But if you nail this, it's a killer feature. Consider starting with simple scenarios (constant load) before tackling burst/seasonal patterns. Maybe integrate with your existing bench tool for real validation?",
        "motivation": "Prevent outages and tune SLOs by testing changes safely.",
        "design_deliverables": [
          "Architecture document in docs/design/policy-simulator.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Build a first\u2011order queueing model; configurable traffic patterns; show predicted backlog/throughput/latency.",
          "Integrate apply/rollback via Admin API; include audit trail."
        ],
        "user_stories_to_address": [
          "I can simulate and apply policy changes with confidence."
        ]
      },
      "acceptance_criteria": [
        "UI sliders/inputs for core policies; charts update with predictions.",
        "Clear assumptions and limitations documented inline.",
        "Apply/rollback via Admin API with audit log.",
        "Implement core sim model",
        "UI: controls + charts",
        "Admin API: apply/rollback endpoints",
        "Docs + inline assumptions"
      ]
    },
    {
      "id": "P3.T078",
      "feature_id": "F031",
      "title": "Implement Policy Simulator core logic",
      "description": "Build the core functionality for Policy Simulator",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/policy-simulator/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T078\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T078\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T078\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T078\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:policy-simulator:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/policy-simulator.md",
          "excerpt": "Feature specification for policy-simulator",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A \"what\u2011if\" simulator to preview the impact of policy changes (retry/backoff, rate limits, concurrency) before applying. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Ambitious! This is chaos engineering meets capacity planning. The queueing theory math is non-trivial (Little's Law, M/M/c models). High risk of \"garbage in, garbage out\" if the model oversimplifies. But if you nail this, it's a killer feature. Consider starting with simple scenarios (constant load) before tackling burst/seasonal patterns. Maybe integrate with your existing bench tool for real validation?",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Implement core sim model",
          "UI: controls + charts",
          "Admin API: apply/rollback endpoints",
          "Docs + inline assumptions"
        ],
        "technical_approach": [
          "Build a first\u2011order queueing model; configurable traffic patterns; show predicted backlog/throughput/latency.",
          "Integrate apply/rollback via Admin API; include audit trail."
        ],
        "code_structure": {
          "module_path": "internal/policy-simulator/",
          "main_files": [
            "policy-simulator.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "policy-simulator_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "UI sliders/inputs for core policies; charts update with predictions.",
        "Clear assumptions and limitations documented inline.",
        "Apply/rollback via Admin API with audit log.",
        "Implement core sim model",
        "UI: controls + charts",
        "Admin API: apply/rollback endpoints",
        "Docs + inline assumptions"
      ]
    },
    {
      "id": "P2.T079",
      "feature_id": "F032",
      "title": "Design Advanced Rate Limiting architecture",
      "description": "Create detailed technical design for Advanced Rate Limiting",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/advanced-rate-limiting.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:advanced-rate-limiting:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/advanced-rate-limiting.md",
          "excerpt": "Feature specification for advanced-rate-limiting",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Implement token\u2011bucket rate limiting with priority fairness and per\u2011tenant budgets. Support global and per\u2011queue/per\u2011tenant limits, dynamic tuning, and clear visibility into allowance and throttling. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is where you graduate from toy to enterprise. Token buckets are battle-tested (AWS API Gateway, Stripe). The Lua scripting for atomicity is crucial - no race conditions allowed. Priority fairness is the secret sauce - weighted fair queuing prevents starvation. The per-tenant isolation is $$$ for SaaS plays. Consider adding \"burst credits\" that accumulate during quiet periods. Also, rate limit exceptions for VIP customers is a common ask.",
        "motivation": "- Protect downstreams and shared infrastructure with hard, fair caps. - Provide predictable throughput for premium or critical tenants. - Avoid bursty overload while maintaining high utilization.",
        "design_deliverables": [
          "Architecture document in docs/design/advanced-rate-limiting.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Algorithms:",
          "Token bucket per key (tenant:queue) with refill rate and burst capacity.",
          "Priority fairness: weighted sharing between priorities (e.g., high=3, low=1) via separate buckets and proportional scheduling.",
          "Global cap: aggregate bucket above tenants to ensure cluster ceiling.",
          "Storage & atomicity:",
          "Redis Lua script to check/consume tokens atomically; returns allow, wait_ms, and remaining tokens.",
          "Keys: rl:{scope} with TTL; metadata hash for config.",
          "Configuration:",
          "Static config + Admin API to update rates and weights at runtime.",
          "Safe bounds and validation; dry\u2011run mode for tuning.",
          "Integration points:",
          "Producer side (enqueue): soft backpressure suggestions.",
          "Worker pull: throttle consumption when budget depleted.",
          "TUI: status widget showing budget/usage and recent throttles.",
          "Observability:",
          "Metrics: allowed/denied, wait times, bucket fill; per scope and aggregate.",
          "Debug endpoint to inspect current limits for a scope."
        ],
        "user_stories_to_address": [
          "I can set per\u2011tenant caps and verify they\u2019re enforced.",
          "I can see when throttling occurs and why."
        ]
      },
      "acceptance_criteria": [
        "Atomic token check/consume with Lua and proper TTLs.",
        "Priority weights influence throughput under contention.",
        "Admin API updates limits without restart; dry\u2011run previews impact.",
        "Lua script + Go wrapper",
        "Config + Admin API endpoints",
        "Producer hints + worker integration",
        "TUI widget + metrics",
        "Docs + examples"
      ]
    },
    {
      "id": "P2.T080",
      "feature_id": "F032",
      "title": "Implement Advanced Rate Limiting core logic",
      "description": "Build the core functionality for Advanced Rate Limiting",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/advanced-rate-limiting/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T080\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T080\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T080\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T080\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:advanced-rate-limiting:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/advanced-rate-limiting.md",
          "excerpt": "Feature specification for advanced-rate-limiting",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Implement token\u2011bucket rate limiting with priority fairness and per\u2011tenant budgets. Support global and per\u2011queue/per\u2011tenant limits, dynamic tuning, and clear visibility into allowance and throttling. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is where you graduate from toy to enterprise. Token buckets are battle-tested (AWS API Gateway, Stripe). The Lua scripting for atomicity is crucial - no race conditions allowed. Priority fairness is the secret sauce - weighted fair queuing prevents starvation. The per-tenant isolation is $$$ for SaaS plays. Consider adding \"burst credits\" that accumulate during quiet periods. Also, rate limit exceptions for VIP customers is a common ask.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Lua script + Go wrapper",
          "Config + Admin API endpoints",
          "Producer hints + worker integration",
          "TUI widget + metrics",
          "Docs + examples"
        ],
        "technical_approach": [
          "Algorithms:",
          "Token bucket per key (tenant:queue) with refill rate and burst capacity.",
          "Priority fairness: weighted sharing between priorities (e.g., high=3, low=1) via separate buckets and proportional scheduling.",
          "Global cap: aggregate bucket above tenants to ensure cluster ceiling.",
          "Storage & atomicity:",
          "Redis Lua script to check/consume tokens atomically; returns allow, wait_ms, and remaining tokens.",
          "Keys: rl:{scope} with TTL; metadata hash for config.",
          "Configuration:",
          "Static config + Admin API to update rates and weights at runtime.",
          "Safe bounds and validation; dry\u2011run mode for tuning.",
          "Integration points:",
          "Producer side (enqueue): soft backpressure suggestions.",
          "Worker pull: throttle consumption when budget depleted.",
          "TUI: status widget showing budget/usage and recent throttles.",
          "Observability:",
          "Metrics: allowed/denied, wait times, bucket fill; per scope and aggregate.",
          "Debug endpoint to inspect current limits for a scope."
        ],
        "code_structure": {
          "module_path": "internal/advanced-rate-limiting/",
          "main_files": [
            "advanced-rate-limiting.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "advanced-rate-limiting_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Atomic token check/consume with Lua and proper TTLs.",
        "Priority weights influence throughput under contention.",
        "Admin API updates limits without restart; dry\u2011run previews impact.",
        "Lua script + Go wrapper",
        "Config + Admin API endpoints",
        "Producer hints + worker integration",
        "TUI widget + metrics",
        "Docs + examples"
      ]
    },
    {
      "id": "P2.T081",
      "feature_id": "F032",
      "title": "Test Advanced Rate Limiting thoroughly",
      "description": "Comprehensive testing for Advanced Rate Limiting",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/advanced-rate-limiting/*_test.go",
            "test/e2e/advanced-rate-limiting_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:advanced-rate-limiting:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/advanced-rate-limiting.md",
          "excerpt": "Feature specification for advanced-rate-limiting",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: Lua path with table\u2011driven cases for edge rates; fairness under synthetic contention."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: soak tests with patterned load; verify adherence to limits and smoothness.",
              "Regression: persistence/TTL behavior across restarts."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/advanced-rate-limiting/*_test.go",
          "integration_tests": "test/integration/advanced-rate-limiting_test.go",
          "e2e_tests": "test/e2e/advanced-rate-limiting_test.go",
          "benchmarks": "internal/advanced-rate-limiting/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Atomic token check/consume with Lua and proper TTLs.",
        "Priority weights influence throughput under contention.",
        "Admin API updates limits without restart; dry\u2011run previews impact.",
        "Lua script + Go wrapper",
        "Config + Admin API endpoints",
        "Producer hints + worker integration",
        "TUI widget + metrics",
        "Docs + examples"
      ]
    },
    {
      "id": "P4.T082",
      "feature_id": "F033",
      "title": "Design Calendar View architecture",
      "description": "Create detailed technical design for Calendar View",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/calendar-view.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:calendar-view:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/calendar-view.md",
          "excerpt": "Feature specification for calendar-view",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A calendar UI to visualize scheduled and recurring jobs by day/week/month. Users can navigate time ranges, inspect details, reschedule, or pause rules\u2014without leaving the TUI. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Calendar view in a terminal! Use Unicode box characters for the grid, color intensity for job density (heatmap style). The timezone handling is CRITICAL - nothing worse than jobs firing at wrong times. The month view could be gorgeous with gradient colors showing load intensity. Consider adding a \"forecast\" mode that shows predicted future load based on recurring rules. Also, drag-and-drop rescheduling with mouse would be slick!",
        "motivation": "- Make time a first\u2011class dimension: see spikes, gaps, and overlaps at a glance. - Reduce mistakes when backfilling or coordinating large scheduled runs. - Provide confidence that cron rules align with business windows.",
        "design_deliverables": [
          "Architecture document in docs/design/calendar-view.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Data model:",
          "Scheduled jobs: Redis ZSET per queue (score = epoch seconds) or existing schedule store; list endpoints per time window.",
          "Recurring rules: store cron spec + metadata (timezone, jitter, max in\u2011flight).",
          "List scheduled window: /schedules?from&till&queue returns items and counts.",
          "CRUD recurring rules: create, pause/resume, delete.",
          "Reschedule: move item to a new timestamp (idempotent re\u2011enqueue at time T).",
          "Views: Month (heatmap), Week (grid by day), Day (timeline).",
          "Navigation: h/j/k/l or arrow keys, / to shift range, g/G to jump today/end.",
          "Actions: enter to inspect, r to reschedule, p to pause rule, / filter by queue/tag.",
          "Rendering: compact cells with count badges; tooltip/overlay for dense cells using bubblezone hitboxes.",
          "Safety: timezone explicit in header; confirmation on reschedules; audit entries for changes.",
          "Observability: counts per bucket, peak hour visualization; errors surfaced inline."
        ],
        "user_stories_to_address": [
          "I can see scheduled volume per day and drill into a day to inspect items.",
          "I can reschedule an item and confirm the exact new time and queue.",
          "I can pause/resume a recurring rule and see status reflected immediately."
        ]
      },
      "acceptance_criteria": [
        "Month/Week/Day views with keyboard navigation and counts.",
        "Reschedule flow updates ZSET timestamp and reflects in view.",
        "CRUD for recurring rules with pause/resume.",
        "Admin API: schedules window + rules CRUD",
        "TUI: month/week/day views + navigation",
        "Reschedule flow + confirmations",
        "Pause/resume rules + indicators",
        "Docs + screenshots"
      ]
    },
    {
      "id": "P4.T083",
      "feature_id": "F033",
      "title": "Implement Calendar View core logic",
      "description": "Build the core functionality for Calendar View",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/calendar-view/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P4.T083\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P4.T083\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P4.T083\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P4.T083\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:calendar-view:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [
        "tui_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/calendar-view.md",
          "excerpt": "Feature specification for calendar-view",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "A calendar UI to visualize scheduled and recurring jobs by day/week/month. Users can navigate time ranges, inspect details, reschedule, or pause rules\u2014without leaving the TUI. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > Calendar view in a terminal! Use Unicode box characters for the grid, color intensity for job density (heatmap style). The timezone handling is CRITICAL - nothing worse than jobs firing at wrong times. The month view could be gorgeous with gradient colors showing load intensity. Consider adding a \"forecast\" mode that shows predicted future load based on recurring rules. Also, drag-and-drop rescheduling with mouse would be slick!",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Admin API: schedules window + rules CRUD",
          "TUI: month/week/day views + navigation",
          "Reschedule flow + confirmations",
          "Pause/resume rules + indicators",
          "Docs + screenshots"
        ],
        "technical_approach": [
          "Data model:",
          "Scheduled jobs: Redis ZSET per queue (score = epoch seconds) or existing schedule store; list endpoints per time window.",
          "Recurring rules: store cron spec + metadata (timezone, jitter, max in\u2011flight).",
          "List scheduled window: /schedules?from&till&queue returns items and counts.",
          "CRUD recurring rules: create, pause/resume, delete.",
          "Reschedule: move item to a new timestamp (idempotent re\u2011enqueue at time T).",
          "Views: Month (heatmap), Week (grid by day), Day (timeline).",
          "Navigation: h/j/k/l or arrow keys, / to shift range, g/G to jump today/end.",
          "Actions: enter to inspect, r to reschedule, p to pause rule, / filter by queue/tag.",
          "Rendering: compact cells with count badges; tooltip/overlay for dense cells using bubblezone hitboxes.",
          "Safety: timezone explicit in header; confirmation on reschedules; audit entries for changes.",
          "Observability: counts per bucket, peak hour visualization; errors surfaced inline."
        ],
        "code_structure": {
          "module_path": "internal/calendar-view/",
          "main_files": [
            "calendar-view.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "calendar-view_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Month/Week/Day views with keyboard navigation and counts.",
        "Reschedule flow updates ZSET timestamp and reflects in view.",
        "CRUD for recurring rules with pause/resume.",
        "Admin API: schedules window + rules CRUD",
        "TUI: month/week/day views + navigation",
        "Reschedule flow + confirmations",
        "Pause/resume rules + indicators",
        "Docs + screenshots"
      ]
    },
    {
      "id": "P4.T084",
      "feature_id": "F034",
      "title": "Design Collaborative Session architecture",
      "description": "Create detailed technical design for Collaborative Session",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/collaborative-session.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:collaborative-session:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/collaborative-session.md",
          "excerpt": "Feature specification for collaborative-session",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Share a read\u2011only live TUI session over the network. A presenter can invite observers to follow along; optionally hand control with a key press. Perfect for incident reviews, pairing, and demos\u2014without screen\u2011sharing. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > tmux for job queues! This is BRILLIANT for incident response - everyone sees the same thing, no \"can you share your screen?\" delays. The control handoff is gold for training juniors. The redaction feature is critical - you don't want passwords in your collaborative sessions. Consider adding session recording for post-mortems. Also, WebRTC for lower latency than WebSockets!",
        "motivation": "- Reduce friction in incident response and walkthroughs. - Preserve crisp terminal rendering and low bandwidth vs. video calls. - Build trust: observers can see exactly what operators see.",
        "design_deliverables": [
          "Architecture document in docs/design/collaborative-session.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Architecture:",
          "Session host multiplexes TUI output and input events over a secure transport.",
          "Observers receive frame deltas; optional control token to accept input.",
          "Transport & security:",
          "Default: SSH port forwarding or WebSocket over TLS to a small embedded server.",
          "Auth: one\u2011time session token with expiry; optional mTLS.",
          "Permissions: read\u2011only by default; control handoff requires explicit approval.",
          "TUI integration:",
          "Presenter mode indicator; participant list; handoff prompt; control timeout.",
          "Frame capture with minimal diff encoding (lines changed per tick) to keep CPU low.",
          "Observability:",
          "Session logs: joins/leaves, handoffs, duration.",
          "Metrics: active sessions, participants, bandwidth.",
          "Safety: redact sensitive values in frames (configurable patterns) before broadcast."
        ],
        "user_stories_to_address": [
          "I can start a collaborative session and share a link/token for read\u2011only viewing.",
          "I can hand control to a participant temporarily and revoke it easily."
        ]
      },
      "acceptance_criteria": [
        "Read\u2011only viewing with smooth updates under typical terminal sizes.",
        "Secure token\u2011based auth with expiry; control handoff flow.",
        "Redaction of configured secrets in frames.",
        "Session server + transport",
        "Presenter/observer UI + handoff",
        "Token auth + expiry",
        "Redaction + metrics",
        "Docs + demo scripts"
      ]
    },
    {
      "id": "P4.T085",
      "feature_id": "F034",
      "title": "Implement Collaborative Session core logic",
      "description": "Build the core functionality for Collaborative Session",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/collaborative-session/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P4.T085\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P4.T085\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P4.T085\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P4.T085\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:collaborative-session:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/collaborative-session.md",
          "excerpt": "Feature specification for collaborative-session",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Share a read\u2011only live TUI session over the network. A presenter can invite observers to follow along; optionally hand control with a key press. Perfect for incident reviews, pairing, and demos\u2014without screen\u2011sharing. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > tmux for job queues! This is BRILLIANT for incident response - everyone sees the same thing, no \"can you share your screen?\" delays. The control handoff is gold for training juniors. The redaction feature is critical - you don't want passwords in your collaborative sessions. Consider adding session recording for post-mortems. Also, WebRTC for lower latency than WebSockets!",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Session server + transport",
          "Presenter/observer UI + handoff",
          "Token auth + expiry",
          "Redaction + metrics",
          "Docs + demo scripts"
        ],
        "technical_approach": [
          "Architecture:",
          "Session host multiplexes TUI output and input events over a secure transport.",
          "Observers receive frame deltas; optional control token to accept input.",
          "Transport & security:",
          "Default: SSH port forwarding or WebSocket over TLS to a small embedded server.",
          "Auth: one\u2011time session token with expiry; optional mTLS.",
          "Permissions: read\u2011only by default; control handoff requires explicit approval.",
          "TUI integration:",
          "Presenter mode indicator; participant list; handoff prompt; control timeout.",
          "Frame capture with minimal diff encoding (lines changed per tick) to keep CPU low.",
          "Observability:",
          "Session logs: joins/leaves, handoffs, duration.",
          "Metrics: active sessions, participants, bandwidth.",
          "Safety: redact sensitive values in frames (configurable patterns) before broadcast."
        ],
        "code_structure": {
          "module_path": "internal/collaborative-session/",
          "main_files": [
            "collaborative-session.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "collaborative-session_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Read\u2011only viewing with smooth updates under typical terminal sizes.",
        "Secure token\u2011based auth with expiry; control handoff flow.",
        "Redaction of configured secrets in frames.",
        "Session server + transport",
        "Presenter/observer UI + handoff",
        "Token auth + expiry",
        "Redaction + metrics",
        "Docs + demo scripts"
      ]
    },
    {
      "id": "P3.T086",
      "feature_id": "F035",
      "title": "Design Json Payload Studio architecture",
      "description": "Create detailed technical design for Json Payload Studio",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/json-payload-studio.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:json-payload-studio:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/json-payload-studio.md",
          "excerpt": "Feature specification for json-payload-studio",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "An in\u2011TUI JSON editor to author, validate, and enqueue job payloads. Includes templates, snippets, schema validation, scheduling (run\u2011at/cron), and pretty diff/peek integration for rapid iteration. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > YES! This is Postman for job queues. The template system with snippets is brilliant - imagine {{now()}}, {{uuid()}}, {{faker.name()}}. Schema validation prevents those 3am \"wrong payload format\" incidents. The cron scheduling is killer for recurring jobs. Consider adding a \"test run\" mode that enqueues to a sandbox queue. Also, payload history would be clutch - \"use payload from 3 runs ago\".",
        "motivation": "- Speed up development and debugging by removing context switches to external editors and scripts. - Promote consistent payloads with reusable templates and schema hints. - Enable safe scheduled submissions for backfills and recurring work.",
        "design_deliverables": [
          "Architecture document in docs/design/json-payload-studio.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Editor: textinput/viewport hybrid with syntax highlight (lightweight), bracket matching, and JSON linting.",
          "Templates: load from config/templates/.json or inline; snippet expansion (e.g., now(), uuid()).",
          "Validation: optional JSON Schema (draft 7) validation per queue/template; show errors inline.",
          "Enqueue: e submits to selected queue; E opens count + schedule options (run\u2011at ISO8601, optional cron spec).",
          "Safety: size/field caps; strip secrets from UI logs; confirm in non\u2011test envs.",
          "Integration: peek last enqueued sample; diff between current editor buffer and last run."
        ],
        "user_stories_to_address": [
          "I can open a template, edit, validate, and enqueue with one keystroke.",
          "I can schedule a safe backfill at off\u2011peak with guardrails.",
          "I can see a redacted preview and confirm before enqueuing to prod."
        ]
      },
      "acceptance_criteria": [
        "JSON lint and schema errors highlighted with line/col.",
        "Template list with fuzzy search and quick preview.",
        "Enqueue supports count, priority, delay/run\u2011at, and optional cron.",
        "Diff/peek integration shows exactly what was sent.",
        "Implement editor widget + linting",
        "Template loader + snippets",
        "Schema validation + error UI",
        "Enqueue + schedule flow",
        "Diff/peek integration",
        "Docs + screenshots"
      ]
    },
    {
      "id": "P3.T087",
      "feature_id": "F035",
      "title": "Implement Json Payload Studio core logic",
      "description": "Build the core functionality for Json Payload Studio",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/json-payload-studio/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P3.T087\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P3.T087\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P3.T087\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P3.T087\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:json-payload-studio:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/json-payload-studio.md",
          "excerpt": "Feature specification for json-payload-studio",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "An in\u2011TUI JSON editor to author, validate, and enqueue job payloads. Includes templates, snippets, schema validation, scheduling (run\u2011at/cron), and pretty diff/peek integration for rapid iteration. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > YES! This is Postman for job queues. The template system with snippets is brilliant - imagine {{now()}}, {{uuid()}}, {{faker.name()}}. Schema validation prevents those 3am \"wrong payload format\" incidents. The cron scheduling is killer for recurring jobs. Consider adding a \"test run\" mode that enqueues to a sandbox queue. Also, payload history would be clutch - \"use payload from 3 runs ago\".",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Implement editor widget + linting",
          "Template loader + snippets",
          "Schema validation + error UI",
          "Enqueue + schedule flow",
          "Diff/peek integration",
          "Docs + screenshots"
        ],
        "technical_approach": [
          "Editor: textinput/viewport hybrid with syntax highlight (lightweight), bracket matching, and JSON linting.",
          "Templates: load from config/templates/.json or inline; snippet expansion (e.g., now(), uuid()).",
          "Validation: optional JSON Schema (draft 7) validation per queue/template; show errors inline.",
          "Enqueue: e submits to selected queue; E opens count + schedule options (run\u2011at ISO8601, optional cron spec).",
          "Safety: size/field caps; strip secrets from UI logs; confirm in non\u2011test envs.",
          "Integration: peek last enqueued sample; diff between current editor buffer and last run."
        ],
        "code_structure": {
          "module_path": "internal/json-payload-studio/",
          "main_files": [
            "json-payload-studio.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "json-payload-studio_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "JSON lint and schema errors highlighted with line/col.",
        "Template list with fuzzy search and quick preview.",
        "Enqueue supports count, priority, delay/run\u2011at, and optional cron.",
        "Diff/peek integration shows exactly what was sent.",
        "Implement editor widget + linting",
        "Template loader + snippets",
        "Schema validation + error UI",
        "Enqueue + schedule flow",
        "Diff/peek integration",
        "Docs + screenshots"
      ]
    },
    {
      "id": "P2.T088",
      "feature_id": "F036",
      "title": "Design Worker Fleet Controls architecture",
      "description": "Create detailed technical design for Worker Fleet Controls",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/worker-fleet-controls.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:worker-fleet-controls:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/worker-fleet-controls.md",
          "excerpt": "Feature specification for worker-fleet-controls",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Operate the worker fleet from the TUI: pause/resume/drain nodes, rolling restarts, and live worker summaries (ID, last heartbeat, active job). Provide precise controls with confirmations and safeguards. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is kubectl for workers! The drain pattern is battle-tested from Kubernetes. Rolling restarts during deploys? Chef's kiss. The pause feature is clutch for debugging - \"pause that worker, it's acting weird.\" Multi-select with confirmations prevents disasters. Consider adding CPU/memory graphs per worker if you can get the metrics. Also, a \"quarantine\" mode that routes suspicious jobs to a debug worker would be gold.",
        "motivation": "- Reduce toil during deploys and incidents; replace ad\u2011hoc scripts. - Provide visibility into active work and worker health. - Enable safe, reversible controls with auditability.",
        "design_deliverables": [
          "Architecture document in docs/design/worker-fleet-controls.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Registry: persist worker IDs, heartbeats, and current job (keyed by worker). Already partially present\u2014extend admin for list/read.",
          "Pause: worker stops pulling new jobs; finishes current if any.",
          "Resume: worker resumes pulls.",
          "Drain: like pause, then exit after current job; pool\u2011wide drain option.",
          "Rolling restart: N\u2011at\u2011a\u2011time drain+exit with max unavailable.",
          "TUI: Workers tab lists workers with statuses; select one/many; action bar with confirmations showing impact.",
          "Safety: prevent draining all by mistake; require typed confirmation in prod; show ETA estimates from running jobs.",
          "Audit: log who/when/what with success/failure; expose via Admin API."
        ],
        "user_stories_to_address": [
          "I can see a list of workers with last heartbeat and active job.",
          "I can pause or drain a worker safely with a confirmation and see state transition.",
          "I can perform a rolling restart with a concurrency cap."
        ]
      },
      "acceptance_criteria": [
        "Admin API supplies workers list and control endpoints (pause/resume/drain).",
        "TUI lists workers, supports multi\u2011select, and shows action progress.",
        "Safety checks prevent accidental global stoppage.",
        "Admin: workers list + control endpoints",
        "Worker: honor pause/drain signals; expose state in heartbeat",
        "TUI: list, select, actions, confirmations",
        "Audit logs + docs"
      ]
    },
    {
      "id": "P2.T089",
      "feature_id": "F036",
      "title": "Implement Worker Fleet Controls core logic",
      "description": "Build the core functionality for Worker Fleet Controls",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/worker-fleet-controls/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P2.T089\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P2.T089\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P2.T089\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P2.T089\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:worker-fleet-controls:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [
        "worker_system"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/worker-fleet-controls.md",
          "excerpt": "Feature specification for worker-fleet-controls",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Operate the worker fleet from the TUI: pause/resume/drain nodes, rolling restarts, and live worker summaries (ID, last heartbeat, active job). Provide precise controls with confirmations and safeguards. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > This is kubectl for workers! The drain pattern is battle-tested from Kubernetes. Rolling restarts during deploys? Chef's kiss. The pause feature is clutch for debugging - \"pause that worker, it's acting weird.\" Multi-select with confirmations prevents disasters. Consider adding CPU/memory graphs per worker if you can get the metrics. Also, a \"quarantine\" mode that routes suspicious jobs to a debug worker would be gold.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Admin: workers list + control endpoints",
          "Worker: honor pause/drain signals; expose state in heartbeat",
          "TUI: list, select, actions, confirmations",
          "Audit logs + docs"
        ],
        "technical_approach": [
          "Registry: persist worker IDs, heartbeats, and current job (keyed by worker). Already partially present\u2014extend admin for list/read.",
          "Pause: worker stops pulling new jobs; finishes current if any.",
          "Resume: worker resumes pulls.",
          "Drain: like pause, then exit after current job; pool\u2011wide drain option.",
          "Rolling restart: N\u2011at\u2011a\u2011time drain+exit with max unavailable.",
          "TUI: Workers tab lists workers with statuses; select one/many; action bar with confirmations showing impact.",
          "Safety: prevent draining all by mistake; require typed confirmation in prod; show ETA estimates from running jobs.",
          "Audit: log who/when/what with success/failure; expose via Admin API."
        ],
        "code_structure": {
          "module_path": "internal/worker-fleet-controls/",
          "main_files": [
            "worker-fleet-controls.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "worker-fleet-controls_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Admin API supplies workers list and control endpoints (pause/resume/drain).",
        "TUI lists workers, supports multi\u2011select, and shows action progress.",
        "Safety checks prevent accidental global stoppage.",
        "Admin: workers list + control endpoints",
        "Worker: honor pause/drain signals; expose state in heartbeat",
        "TUI: list, select, actions, confirmations",
        "Audit logs + docs"
      ]
    },
    {
      "id": "P2.T090",
      "feature_id": "F036",
      "title": "Test Worker Fleet Controls thoroughly",
      "description": "Comprehensive testing for Worker Fleet Controls",
      "boundaries": {
        "expected_complexity": {
          "value": "~400 LoC tests, 80% coverage",
          "breakdown": "Unit tests (200 LoC), Integration tests (150 LoC), E2E tests (50 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "80% code coverage achieved",
            "All test scenarios passing",
            "Performance benchmarks met",
            "Edge cases covered"
          ],
          "stop_when": "Tests complete and passing; do NOT refactor code"
        },
        "scope": {
          "includes": [
            "internal/worker-fleet-controls/*_test.go",
            "test/e2e/worker-fleet-controls_test.go"
          ],
          "excludes": [
            "production code changes"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with test results and coverage",
          "checkpoints": [
            "unit",
            "integration",
            "e2e",
            "coverage",
            "benchmarks"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "tests:worker-fleet-controls:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 4,
        "likely": 8,
        "pessimistic": 16
      },
      "reuses_existing": [
        "test_framework"
      ],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/worker-fleet-controls.md",
          "excerpt": "Feature specification for worker-fleet-controls",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "test_categories": [
          {
            "type": "unit",
            "requirements": [
              "Unit: state transitions (running\u2192paused\u2192draining\u2192stopped); TTL/heartbeat logic."
            ]
          },
          {
            "type": "integration",
            "requirements": [
              "Integration: end\u2011to\u2011end drain/resume with synthetic workers."
            ]
          },
          {
            "type": "performance",
            "requirements": [
              "Manual: rolling restarts; race conditions under load."
            ]
          }
        ],
        "coverage_requirements": {
          "unit": "80% minimum",
          "integration": "70% minimum",
          "e2e": "Critical paths covered"
        },
        "test_structure": {
          "unit_tests": "internal/worker-fleet-controls/*_test.go",
          "integration_tests": "test/integration/worker-fleet-controls_test.go",
          "e2e_tests": "test/e2e/worker-fleet-controls_test.go",
          "benchmarks": "internal/worker-fleet-controls/benchmark_test.go"
        },
        "test_data_requirements": [
          "Mock Redis instance for unit tests",
          "Dockerized Redis for integration tests",
          "Full system deployment for E2E tests"
        ]
      },
      "acceptance_criteria": [
        "Admin API supplies workers list and control endpoints (pause/resume/drain).",
        "TUI lists workers, supports multi\u2011select, and shows action progress.",
        "Safety checks prevent accidental global stoppage.",
        "Admin: workers list + control endpoints",
        "Worker: honor pause/drain signals; expose state in heartbeat",
        "TUI: list, select, actions, confirmations",
        "Audit logs + docs"
      ]
    },
    {
      "id": "P4.T091",
      "feature_id": "F037",
      "title": "Design Right Click Context Menus architecture",
      "description": "Create detailed technical design for Right Click Context Menus",
      "boundaries": {
        "expected_complexity": {
          "value": "1 design doc, ~500 lines",
          "breakdown": "Architecture (200 lines), API spec (150 lines), Data model (150 lines)"
        },
        "definition_of_done": {
          "criteria": [
            "Architecture documented with diagrams",
            "API endpoints specified",
            "Data models defined",
            "Integration points identified"
          ],
          "stop_when": "Design approved; do NOT start implementation"
        },
        "scope": {
          "includes": [
            "docs/design/right-click-context-menus.md"
          ],
          "excludes": [
            "implementation code",
            "tests"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines format with task_id, step, status, percent",
          "checkpoints": [
            "architecture",
            "api",
            "data_model",
            "review"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          }
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [],
        "creates": [
          "design:right-click-context-menus:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 2,
        "likely": 4,
        "pessimistic": 8
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/right-click-context-menus.md",
          "excerpt": "Feature specification for right-click-context-menus",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Add contextual menus on right\u2011click (and m) across the TUI to expose actions where users are looking: peek, requeue, purge (confirmed), copy payload/key, open trace, export sample. Menus appear adjacent to the cursor with precise hitboxes and keyboard fallbacks. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > THIS is how you make terminal apps feel modern! Right-click is muscle memory for everyone. The bubblezone integration for precise hitboxes is crucial - nothing worse than clicking the wrong row. Consider adding copy-paste actions (copy job ID, copy payload JSON). Also, a \"Send to...\" submenu that integrates with other tools would be killer. Make sure the menus are FAST - any lag kills the magic.",
        "motivation": "- Speed common actions and reduce cognitive load. - Surface advanced operations without cluttering primary views. - Improve mouse UX to match keyboard parity.",
        "design_deliverables": [
          "Architecture document in docs/design/right-click-context-menus.md",
          "API specification (OpenAPI 3.0 or Protocol Buffers)",
          "Data model schemas (JSON Schema or Proto)",
          "System architecture diagram (Mermaid)",
          "Data flow diagram",
          "Error handling strategy",
          "Security threat model"
        ],
        "design_sections_required": [
          "Executive Summary",
          "System Architecture",
          "API Endpoints and Contracts",
          "Data Models and Schemas",
          "Integration Points",
          "Error Handling Strategy",
          "Security Considerations",
          "Performance Considerations",
          "Testing Strategy",
          "Migration Plan (if applicable)"
        ],
        "technical_approach": [
          "Hitboxes: integrate bubblezone to register zones for tabs, table rows, charts, and info regions.",
          "Menu widget: lightweight overlay with items, accelerators, and disabled states; auto\u2011positions within viewport.",
          "Action registry: map context (e.g., queue row, DLQ item) to available actions with capability checks.",
          "Safety: destructive items require confirm modal; show exact scope (e.g., \u201cPurge DLQ dead_letter (123 items)\u201d).",
          "Keyboard parity: m opens context menu for the focused item; arrow/enter to select.",
          "Extensibility: plug new actions via registry to avoid scattering logic."
        ],
        "user_stories_to_address": [
          "I can right\u2011click a queue row and choose Peek or Enqueue.",
          "I can right\u2011click DLQ items and Requeue/Purge with confirmation."
        ]
      },
      "acceptance_criteria": [
        "Precise hitboxes with bubblezone on rows/tabs.",
        "Context menu overlays with keyboard + mouse interaction.",
        "Destructive actions gated with explicit confirmation.",
        "Integrate bubblezone and register zones",
        "Build menu overlay component",
        "Implement action registry + confirm hooks",
        "Wire menus in Queues + DLQ",
        "Update help/docs"
      ]
    },
    {
      "id": "P4.T092",
      "feature_id": "F037",
      "title": "Implement Right Click Context Menus core logic",
      "description": "Build the core functionality for Right Click Context Menus",
      "boundaries": {
        "expected_complexity": {
          "value": "~800-1200 LoC",
          "breakdown": "Core logic (600 LoC), Utilities (200 LoC), Integration (200-400 LoC)"
        },
        "definition_of_done": {
          "criteria": [
            "Core functionality implemented",
            "Unit tests passing",
            "Integration with existing systems",
            "Error handling complete"
          ],
          "stop_when": "Core feature works; do NOT add UI or advanced features"
        },
        "scope": {
          "includes": [
            "internal/right-click-context-menus/"
          ],
          "excludes": [
            "UI components",
            "advanced features"
          ]
        },
        "execution_guidance": {
          "logging": "JSON Lines with progress updates every 10%",
          "checkpoints": [
            "setup",
            "core",
            "integration",
            "tests",
            "validation"
          ],
          "logging_format": {
            "description": "JSON Lines format (one JSON object per line)",
            "required_fields": [
              "timestamp",
              "task_id",
              "step",
              "status",
              "message"
            ],
            "optional_fields": [
              "percent",
              "data"
            ],
            "status_values": [
              "start",
              "progress",
              "done",
              "error"
            ]
          },
          "example_logs": [
            "{\"timestamp\":\"2025-01-01T10:00:00Z\",\"task_id\":\"P4.T092\",\"step\":\"setup\",\"status\":\"start\",\"message\":\"Creating module structure\"}",
            "{\"timestamp\":\"2025-01-01T10:05:00Z\",\"task_id\":\"P4.T092\",\"step\":\"setup\",\"status\":\"done\",\"message\":\"Module structure created\"}",
            "{\"timestamp\":\"2025-01-01T10:10:00Z\",\"task_id\":\"P4.T092\",\"step\":\"core\",\"status\":\"progress\",\"percent\":50,\"message\":\"Core logic 50% complete\"}",
            "{\"timestamp\":\"2025-01-01T11:00:00Z\",\"task_id\":\"P4.T092\",\"step\":\"tests\",\"status\":\"done\",\"message\":\"All tests passing\",\"data\":{\"coverage\":\"82%\"}}"
          ]
        }
      },
      "shared_resources": {
        "exclusive": [],
        "shared_limited": [
          "test_redis"
        ],
        "creates": [
          "module:right-click-context-menus:v1"
        ],
        "modifies": []
      },
      "duration": {
        "optimistic": 8,
        "likely": 16,
        "pessimistic": 32
      },
      "reuses_existing": [],
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/right-click-context-menus.md",
          "excerpt": "Feature specification for right-click-context-menus",
          "confidence": 1.0,
          "rationale": "Primary feature specification document"
        }
      ],
      "implementation_spec": {
        "overview": "Add contextual menus on right\u2011click (and m) across the TUI to expose actions where users are looking: peek, requeue, purge (confirmed), copy payload/key, open trace, export sample. Menus appear adjacent to the cursor with precise hitboxes and keyboard fallbacks. > !note- \ud83d\udde3\ufe0f CLAUDE'S THOUGHTS \ud83d\udcad > THIS is how you make terminal apps feel modern! Right-click is muscle memory for everyone. The bubblezone integration for precise hitboxes is crucial - nothing worse than clicking the wrong row. Consider adding copy-paste actions (copy job ID, copy payload JSON). Also, a \"Send to...\" submenu that integrates with other tools would be killer. Make sure the menus are FAST - any lag kills the magic.",
        "estimated_size": "~800-1200 LoC",
        "complexity_notes": "Effort",
        "implementation_steps": [
          "Integrate bubblezone and register zones",
          "Build menu overlay component",
          "Implement action registry + confirm hooks",
          "Wire menus in Queues + DLQ",
          "Update help/docs"
        ],
        "technical_approach": [
          "Hitboxes: integrate bubblezone to register zones for tabs, table rows, charts, and info regions.",
          "Menu widget: lightweight overlay with items, accelerators, and disabled states; auto\u2011positions within viewport.",
          "Action registry: map context (e.g., queue row, DLQ item) to available actions with capability checks.",
          "Safety: destructive items require confirm modal; show exact scope (e.g., \u201cPurge DLQ dead_letter (123 items)\u201d).",
          "Keyboard parity: m opens context menu for the focused item; arrow/enter to select.",
          "Extensibility: plug new actions via registry to avoid scattering logic."
        ],
        "code_structure": {
          "module_path": "internal/right-click-context-menus/",
          "main_files": [
            "right-click-context-menus.go - Core implementation",
            "types.go - Data structures and interfaces",
            "handlers.go - Request handlers (if API)",
            "middleware.go - Middleware functions (if applicable)",
            "errors.go - Custom error types",
            "config.go - Configuration structures"
          ],
          "test_files": [
            "right-click-context-menus_test.go - Unit tests",
            "integration_test.go - Integration tests",
            "benchmark_test.go - Performance benchmarks"
          ]
        }
      },
      "acceptance_criteria": [
        "Precise hitboxes with bubblezone on rows/tabs.",
        "Context menu overlays with keyboard + mouse interaction.",
        "Destructive actions gated with explicit confirmation.",
        "Integrate bubblezone and register zones",
        "Build menu overlay component",
        "Implement action registry + confirm hooks",
        "Wire menus in Queues + DLQ",
        "Update help/docs"
      ]
    }
  ],
  "dependencies": [
    {
      "from": "P1.T001",
      "to": "P1.T002",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T002",
      "to": "P1.T003",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T004",
      "to": "P2.T005",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T005",
      "to": "P2.T006",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T007",
      "to": "P2.T008",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T008",
      "to": "P2.T009",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T010",
      "to": "P1.T011",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T011",
      "to": "P1.T012",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T013",
      "to": "P3.T014",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T015",
      "to": "P3.T016",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T017",
      "to": "P1.T018",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T018",
      "to": "P1.T019",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T020",
      "to": "P1.T021",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T021",
      "to": "P1.T022",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T023",
      "to": "P3.T024",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T025",
      "to": "P3.T026",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T027",
      "to": "P3.T028",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T029",
      "to": "P3.T030",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T031",
      "to": "P3.T032",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T033",
      "to": "P1.T034",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T034",
      "to": "P1.T035",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T036",
      "to": "P2.T037",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T037",
      "to": "P2.T038",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T039",
      "to": "P2.T040",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T040",
      "to": "P2.T041",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T042",
      "to": "P3.T043",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T044",
      "to": "P2.T045",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T045",
      "to": "P2.T046",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T047",
      "to": "P3.T048",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T049",
      "to": "P2.T050",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T050",
      "to": "P2.T051",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T052",
      "to": "P2.T053",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T053",
      "to": "P2.T054",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T055",
      "to": "P3.T056",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T057",
      "to": "P2.T058",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T058",
      "to": "P2.T059",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T060",
      "to": "P2.T061",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T061",
      "to": "P2.T062",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P4.T063",
      "to": "P4.T064",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P4.T065",
      "to": "P4.T066",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T067",
      "to": "P3.T068",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T069",
      "to": "P2.T070",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T070",
      "to": "P2.T071",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T072",
      "to": "P2.T073",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T073",
      "to": "P2.T074",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T075",
      "to": "P3.T076",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T077",
      "to": "P3.T078",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T079",
      "to": "P2.T080",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T080",
      "to": "P2.T081",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P4.T082",
      "to": "P4.T083",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P4.T084",
      "to": "P4.T085",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T086",
      "to": "P3.T087",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T088",
      "to": "P2.T089",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P2.T089",
      "to": "P2.T090",
      "type": "sequential",
      "reason": "Tests require completed implementation",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Testing follows implementation",
          "confidence": 1.0,
          "rationale": "Cannot test non-existent code"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P4.T091",
      "to": "P4.T092",
      "type": "sequential",
      "reason": "Implementation requires completed design",
      "evidence": [
        {
          "type": "plan",
          "source": "T.A.S.K.S. methodology",
          "excerpt": "Design must precede implementation",
          "confidence": 1.0,
          "rationale": "Standard software development practice"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P3.T048",
      "to": "P3.T027",
      "type": "technical",
      "reason": "forecasting provides required functionality for automatic-capacity-planning",
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/DAG.json",
          "excerpt": "DAG edge: forecasting -> automatic-capacity-planning",
          "confidence": 0.9,
          "rationale": "Dependency defined in feature DAG"
        }
      ],
      "confidence": 0.9,
      "isHard": true
    },
    {
      "from": "P3.T048",
      "to": "P3.T077",
      "type": "technical",
      "reason": "forecasting provides required functionality for policy-simulator",
      "evidence": [
        {
          "type": "plan",
          "source": "docs/ideas/DAG.json",
          "excerpt": "DAG edge: forecasting -> policy-simulator",
          "confidence": 0.9,
          "rationale": "Dependency defined in feature DAG"
        }
      ],
      "confidence": 0.9,
      "isHard": false
    },
    {
      "from": "P1.T002",
      "to": "P1.T011",
      "type": "mutual_exclusion",
      "reason": "Both require exclusive access to redis_schema",
      "shared_resource": "redis_schema",
      "evidence": [
        {
          "type": "infrastructure",
          "source": "shared_resources",
          "excerpt": "Schema modifications require exclusive access to prevent corruption",
          "confidence": 1.0,
          "rationale": "Infrastructure constraint"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T002",
      "to": "P1.T018",
      "type": "mutual_exclusion",
      "reason": "Both require exclusive access to redis_schema",
      "shared_resource": "redis_schema",
      "evidence": [
        {
          "type": "infrastructure",
          "source": "shared_resources",
          "excerpt": "Schema modifications require exclusive access to prevent corruption",
          "confidence": 1.0,
          "rationale": "Infrastructure constraint"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T002",
      "to": "P1.T021",
      "type": "mutual_exclusion",
      "reason": "Both require exclusive access to redis_schema",
      "shared_resource": "redis_schema",
      "evidence": [
        {
          "type": "infrastructure",
          "source": "shared_resources",
          "excerpt": "Schema modifications require exclusive access to prevent corruption",
          "confidence": 1.0,
          "rationale": "Infrastructure constraint"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T002",
      "to": "P1.T034",
      "type": "mutual_exclusion",
      "reason": "Both require exclusive access to redis_schema",
      "shared_resource": "redis_schema",
      "evidence": [
        {
          "type": "infrastructure",
          "source": "shared_resources",
          "excerpt": "Schema modifications require exclusive access to prevent corruption",
          "confidence": 1.0,
          "rationale": "Infrastructure constraint"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T011",
      "to": "P1.T018",
      "type": "mutual_exclusion",
      "reason": "Both require exclusive access to redis_schema",
      "shared_resource": "redis_schema",
      "evidence": [
        {
          "type": "infrastructure",
          "source": "shared_resources",
          "excerpt": "Schema modifications require exclusive access to prevent corruption",
          "confidence": 1.0,
          "rationale": "Infrastructure constraint"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T011",
      "to": "P1.T021",
      "type": "mutual_exclusion",
      "reason": "Both require exclusive access to redis_schema",
      "shared_resource": "redis_schema",
      "evidence": [
        {
          "type": "infrastructure",
          "source": "shared_resources",
          "excerpt": "Schema modifications require exclusive access to prevent corruption",
          "confidence": 1.0,
          "rationale": "Infrastructure constraint"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T011",
      "to": "P1.T034",
      "type": "mutual_exclusion",
      "reason": "Both require exclusive access to redis_schema",
      "shared_resource": "redis_schema",
      "evidence": [
        {
          "type": "infrastructure",
          "source": "shared_resources",
          "excerpt": "Schema modifications require exclusive access to prevent corruption",
          "confidence": 1.0,
          "rationale": "Infrastructure constraint"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T018",
      "to": "P1.T021",
      "type": "mutual_exclusion",
      "reason": "Both require exclusive access to redis_schema",
      "shared_resource": "redis_schema",
      "evidence": [
        {
          "type": "infrastructure",
          "source": "shared_resources",
          "excerpt": "Schema modifications require exclusive access to prevent corruption",
          "confidence": 1.0,
          "rationale": "Infrastructure constraint"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T018",
      "to": "P1.T034",
      "type": "mutual_exclusion",
      "reason": "Both require exclusive access to redis_schema",
      "shared_resource": "redis_schema",
      "evidence": [
        {
          "type": "infrastructure",
          "source": "shared_resources",
          "excerpt": "Schema modifications require exclusive access to prevent corruption",
          "confidence": 1.0,
          "rationale": "Infrastructure constraint"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    },
    {
      "from": "P1.T021",
      "to": "P1.T034",
      "type": "mutual_exclusion",
      "reason": "Both require exclusive access to redis_schema",
      "shared_resource": "redis_schema",
      "evidence": [
        {
          "type": "infrastructure",
          "source": "shared_resources",
          "excerpt": "Schema modifications require exclusive access to prevent corruption",
          "confidence": 1.0,
          "rationale": "Infrastructure constraint"
        }
      ],
      "confidence": 1.0,
      "isHard": true
    }
  ],
  "resource_conflicts": {
    "redis_schema": {
      "tasks": [
        "P1.T002",
        "P1.T011",
        "P1.T018",
        "P1.T021",
        "P1.T034"
      ],
      "resolution": "sequential_ordering",
      "rationale": "Schema modifications require exclusive access to prevent corruption"
    }
  }
}